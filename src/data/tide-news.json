{
  "date": "2026-02-12",
  "generatedAt": "2026-02-12T23:57:26.087930",
  "introduction": "今日AI领域迎来重磅融资与技术双突破：Anthropic以3800亿美元估值完成300亿美元G轮融资，刷新行业纪录；Google推出Gemini 3 Deep Think，专攻科研与工程复杂推理。论文方面，Control Reinforcement Learning实现对语言模型输出的精准干预，RADAR基准推动具身智能走向真实世界评估，而ENIGMA仅用1%参数即可从脑电图生成图像，为脑机接口带来实用化曙光。同时，多篇研究聚焦AI可靠性——从视频生成重复问题、深度伪造检测到机器人世界模型的泛化能力，彰显行业从“能做”向“可信、可控、可部署”演进的关键转向。",
  "introductionZh": "今日AI领域迎来重磅融资与技术双突破：Anthropic以3800亿美元估值完成300亿美元G轮融资，刷新行业纪录；Google推出Gemini 3 Deep Think，专攻科研与工程复杂推理。论文方面，Control Reinforcement Learning实现对语言模型输出的精准干预，RADAR基准推动具身智能走向真实世界评估，而ENIGMA仅用1%参数即可从脑电图生成图像，为脑机接口带来实用化曙光。同时，多篇研究聚焦AI可靠性——从视频生成重复问题、深度伪造检测到机器人世界模型的泛化能力，彰显行业从“能做”向“可信、可控、可部署”演进的关键转向。",
  "introductionEn": "Today’s AI landscape is defined by a record-breaking $30B funding round for Anthropic at a $380B valuation and Google’s launch of Gemini 3 Deep Think, optimized for scientific reasoning. Key research advances include precise control over language model outputs via SAE feature steering, a new real-world benchmark (RADAR) for embodied AI, and ENIGMA—a brain-to-image model using under 1% of typical parameters. Critical reliability issues are also addressed: hallucination mitigation in VLMs, output repetition in VideoLLMs, and physics-grounded world models for robotics. These developments signal a pivotal shift from raw capability toward trustworthy, deployable, and interpretable AI systems.",
  "longformScript": "今天AI领域发生的事，可以用三个关键词概括：规模、精度和边界。一边是创纪录的融资和算力投入，一边是模型能力向科研、工程等高精尖场景纵深；而与此同时，开发者和研究者正不断试探人机协作的新边界——不是靠堆参数，而是靠更好的工具、更清晰的规约，甚至更聪明的对抗防御。\n\n先看资本与基础设施的动向。Anthropic刚刚完成300亿美元G轮融资，投后估值高达3800亿美元，刷新了整个AI行业的纪录。这背后不只是数字游戏——它的年化收入已达140亿美元，其中Claude Code一项就贡献了25亿，客户包括8家财富10强公司。更关键的是，这些企业不再只是试用AI，而是把Claude深度嵌入到编码、金融分析、销售流程中，并通过AWS、Google Cloud和Azure实现合规部署。这说明，大模型已经从“炫技玩具”变成了企业运转的基础设施。而另一边，NVIDIA的Blackwell平台正推动推理成本断崖式下降。多家服务商利用其硬件-软件协同优化，将每token成本压低最高10倍。医生用它节省数千万分钟文书时间，游戏公司让NPC对话更流畅且便宜四倍，客服响应低于400毫秒——AI服务正在变得既高性能又高性价比，这对中小企业和普通用户都是实实在在的利好。\n\n如果说基础设施在“铺路”，那么模型能力就在“登高”。Google今天正式推出Gemini 3的Deep Think模式，专攻科研与工程中的复杂推理。它不只是回答问题，而是能生成科研假设、验证工程设计，甚至在数学、物理奥赛中达到金牌水平。有研究者用它发现论文逻辑漏洞，还有团队优化了半导体晶体生长工艺。这标志着大模型正从通用聊天助手，转向专业领域的“协作者”。值得注意的是，这种能力并非封闭在实验室里——Google已开放API早期试用，允许研究人员直接调用其高阶推理功能，比如把草图转成可3D打印的模型。与此同时，开源生态也在加速追赶。Unsloth推出的微调工具，能让Llama、Qwen等主流模型在70%更少显存下训练快两倍；而Google新开源的langextract库，则让非结构化文本的信息抽取变得可溯源、可解释，特别适合法律或新闻核查这类对可信度要求极高的场景。工具链的进步，正在让高质量AI能力从巨头专属走向更广泛的开发者群体。\n\n但技术越强大，攻防就越激烈。Google威胁情报团队披露，今年已拦截超10万次针对Gemini的“蒸馏攻击”——攻击者试图通过合法API调用，逆向克隆模型的推理能力。虽然普通用户不受影响，但这敲响了警钟：AI模型的知识产权保护，正成为新的安全前线。企业和开发者需要警惕异常调用模式，优先选择具备主动防御机制的平台。有趣的是，防御不仅来自后台，也来自前端的人机协作设计。一位开发者仅通过改进编程代理的编辑接口——引入基于行内容哈希的工具Hashline——就让15个主流模型的代码修复成功率集体跃升，有的甚至从不到7%飙升至近70%。这说明，当前AI编码的瓶颈，往往不在模型本身，而在我们如何与它交互。同样，另一位工程师用Claude在48小时内构建出具备拜占庭容错的分布式系统，核心秘诀不是模型多强，而是他先写了3000行高精度行为规约，再让AI据此生成代码和测试。过程中系统多次失败，但通过迭代规约和负载测试，最终实现了5000 RPS下的低延迟稳定运行。这些案例共同指向一个趋势：未来的AI开发，将越来越依赖“规约先行、工具协同”的范式，而非单纯依赖更大模型。\n\n那么，作为听众，该如何理解今天的这些变化？首先，AI正在从“能做”转向“可信、可控、可部署”。无论是企业采购Claude看重合规性，还是科研界要求Gemini提供可验证的推理链，都说明市场不再满足于惊艳demo，而是要可审计、可集成、可长期运维的解决方案。其次，成本下降和工具开源正在重塑竞争格局——你不需要拥有千亿参数模型，也能通过高效微调或智能工具链，构建有竞争力的应用。但也要警惕，随着AI能力外溢，模型安全和数据主权将成为新风险点。建议开发者关注API调用监控、本地化部署选项，以及像Matrix这样的去中心化通信方案——后者因Discord强制实名而迎来迁移潮，再次证明，在数字身份日益收紧的时代，技术架构的选择本身就是一种自由。\n\n今天的AI世界，既有3800亿美元估值的巨轮，也有仅改一行代码就撬动性能飞跃的巧思。它不再只是关于谁跑得更快，而是谁更能精准定义问题、设计协作边界，并在开放与安全之间找到平衡。这些变化不会一夜颠覆生活，但它们正悄悄重构我们工作、创造甚至思考的方式。",
  "longformScriptZh": "今天AI领域发生的事，可以用三个关键词概括：规模、精度和边界。一边是创纪录的融资和算力投入，一边是模型能力向科研、工程等高精尖场景纵深；而与此同时，开发者和研究者正不断试探人机协作的新边界——不是靠堆参数，而是靠更好的工具、更清晰的规约，甚至更聪明的对抗防御。\n\n先看资本与基础设施的动向。Anthropic刚刚完成300亿美元G轮融资，投后估值高达3800亿美元，刷新了整个AI行业的纪录。这背后不只是数字游戏——它的年化收入已达140亿美元，其中Claude Code一项就贡献了25亿，客户包括8家财富10强公司。更关键的是，这些企业不再只是试用AI，而是把Claude深度嵌入到编码、金融分析、销售流程中，并通过AWS、Google Cloud和Azure实现合规部署。这说明，大模型已经从“炫技玩具”变成了企业运转的基础设施。而另一边，NVIDIA的Blackwell平台正推动推理成本断崖式下降。多家服务商利用其硬件-软件协同优化，将每token成本压低最高10倍。医生用它节省数千万分钟文书时间，游戏公司让NPC对话更流畅且便宜四倍，客服响应低于400毫秒——AI服务正在变得既高性能又高性价比，这对中小企业和普通用户都是实实在在的利好。\n\n如果说基础设施在“铺路”，那么模型能力就在“登高”。Google今天正式推出Gemini 3的Deep Think模式，专攻科研与工程中的复杂推理。它不只是回答问题，而是能生成科研假设、验证工程设计，甚至在数学、物理奥赛中达到金牌水平。有研究者用它发现论文逻辑漏洞，还有团队优化了半导体晶体生长工艺。这标志着大模型正从通用聊天助手，转向专业领域的“协作者”。值得注意的是，这种能力并非封闭在实验室里——Google已开放API早期试用，允许研究人员直接调用其高阶推理功能，比如把草图转成可3D打印的模型。与此同时，开源生态也在加速追赶。Unsloth推出的微调工具，能让Llama、Qwen等主流模型在70%更少显存下训练快两倍；而Google新开源的langextract库，则让非结构化文本的信息抽取变得可溯源、可解释，特别适合法律或新闻核查这类对可信度要求极高的场景。工具链的进步，正在让高质量AI能力从巨头专属走向更广泛的开发者群体。\n\n但技术越强大，攻防就越激烈。Google威胁情报团队披露，今年已拦截超10万次针对Gemini的“蒸馏攻击”——攻击者试图通过合法API调用，逆向克隆模型的推理能力。虽然普通用户不受影响，但这敲响了警钟：AI模型的知识产权保护，正成为新的安全前线。企业和开发者需要警惕异常调用模式，优先选择具备主动防御机制的平台。有趣的是，防御不仅来自后台，也来自前端的人机协作设计。一位开发者仅通过改进编程代理的编辑接口——引入基于行内容哈希的工具Hashline——就让15个主流模型的代码修复成功率集体跃升，有的甚至从不到7%飙升至近70%。这说明，当前AI编码的瓶颈，往往不在模型本身，而在我们如何与它交互。同样，另一位工程师用Claude在48小时内构建出具备拜占庭容错的分布式系统，核心秘诀不是模型多强，而是他先写了3000行高精度行为规约，再让AI据此生成代码和测试。过程中系统多次失败，但通过迭代规约和负载测试，最终实现了5000 RPS下的低延迟稳定运行。这些案例共同指向一个趋势：未来的AI开发，将越来越依赖“规约先行、工具协同”的范式，而非单纯依赖更大模型。\n\n那么，作为听众，该如何理解今天的这些变化？首先，AI正在从“能做”转向“可信、可控、可部署”。无论是企业采购Claude看重合规性，还是科研界要求Gemini提供可验证的推理链，都说明市场不再满足于惊艳demo，而是要可审计、可集成、可长期运维的解决方案。其次，成本下降和工具开源正在重塑竞争格局——你不需要拥有千亿参数模型，也能通过高效微调或智能工具链，构建有竞争力的应用。但也要警惕，随着AI能力外溢，模型安全和数据主权将成为新风险点。建议开发者关注API调用监控、本地化部署选项，以及像Matrix这样的去中心化通信方案——后者因Discord强制实名而迎来迁移潮，再次证明，在数字身份日益收紧的时代，技术架构的选择本身就是一种自由。\n\n今天的AI世界，既有3800亿美元估值的巨轮，也有仅改一行代码就撬动性能飞跃的巧思。它不再只是关于谁跑得更快，而是谁更能精准定义问题、设计协作边界，并在开放与安全之间找到平衡。这些变化不会一夜颠覆生活，但它们正悄悄重构我们工作、创造甚至思考的方式。",
  "longformScriptEn": "Today’s AI landscape is defined by a pivotal shift: we’re moving beyond raw scale and speed toward systems that are trustworthy, interpretable, and deeply integrated into real-world workflows. This transition is evident in record-breaking enterprise adoption, specialized reasoning models for science and engineering, dramatic cost reductions in inference, and growing attention to both the infrastructure and interfaces that make AI usable—and secure. The era of “just bigger models” is giving way to smarter deployment, better tooling, and more thoughtful alignment with human expertise.\n\nLeading this charge is Anthropic, which just closed a staggering $30 billion Series G funding round at a $380 billion valuation—the largest private financing in tech history. What’s striking isn’t just the number, but what it represents: $14 billion in annualized revenue, growing over tenfold each year for three straight years, with eight of the Fortune 10 now relying on Claude as a core productivity layer. Products like Claude Code alone generate over $2.5 billion annually, and enterprises are deploying these tools securely across AWS, Google Cloud, and Azure for everything from financial modeling to sales automation. This isn’t speculative hype—it’s proof that frontier AI has become operational infrastructure for the world’s most sophisticated organizations.\n\nAt the same time, Google is pushing the boundaries of expert reasoning with its upgraded Gemini 3 Deep Think mode. No longer just a chatbot, Deep Think now delivers gold-medal performance on Olympiad-level math, physics, and coding challenges, and has already helped researchers spot subtle errors in published papers and optimize semiconductor crystal growth. It can turn hand-drawn sketches into 3D-printable models and interpret complex experimental data—all through a chain-of-thought architecture fine-tuned for scientific rigor. This marks a clear evolution: AI is becoming a domain-specialized collaborator, not just a general-purpose assistant. But such capabilities also attract unwanted attention—Google recently thwarted over 100,000 “distillation attacks” aimed at cloning Gemini’s reasoning logic via API prompts, mostly from private companies and researchers. While not a direct user threat, it underscores a new reality: proprietary AI reasoning is valuable intellectual property, and platforms must now defend against algorithmic extraction as fiercely as they guard data.\n\nMeanwhile, the economics of running AI are transforming thanks to hardware-software co-design. NVIDIA’s Blackwell platform, paired with optimizations like NVFP4 precision and TensorRT-LLM, is enabling inference providers like Baseten, Fireworks AI, and Together AI to slash costs by up to 10x. Real-world impacts are already visible: Sully.ai is saving physicians over 30 million minutes in documentation, gaming studio Latitude cut token costs by 4x, and Decagon delivers sub-400ms voice responses at one-sixth the price. This democratization of high-performance inference means startups and even individual developers can now offer responsive, affordable AI services—from healthcare assistants to dynamic game characters—without massive cloud bills. Complementing this, open-source tools like Unsloth are making fine-tuning dramatically more accessible, enabling 2x faster training with 70% less VRAM for models like Llama and Gemma. And Google’s new langextract library lets developers extract structured, source-grounded facts from unstructured text with built-in traceability—critical for legal, scientific, or journalistic applications where verifiability matters.\n\nBut perhaps the most underappreciated insight emerging this week is that model capability isn’t always the bottleneck. Developer Can Bölük demonstrated that simply changing the edit tool in an AI coding agent—from basic diff operations to a novel “Hashline” approach using per-line content hashes—boosted code-editing success rates across 15 different LLMs. For Grok Code Fast 1, performance jumped from 6.7% to 68.3% overnight. This “harness problem” reveals a crucial truth: how we interface with models often matters more than the models themselves. Similarly, Henry Garner and Claude built a full Byzantine fault-tolerant distributed system over a weekend—not by prompting casually, but by writing 3,000 lines of precise behavioral specifications in a custom language called Allium. Claude generated nearly 5,000 lines of Kotlin and 103 passing tests in under an hour, though initial failures at integration boundaries reminded us that human oversight remains essential where components meet. These cases show that AI’s real power emerges not in isolation, but when tightly coupled with rigorous human-defined structure.\n\nLooking ahead, keep an eye on three key vectors. First, the tension between centralized AI platforms and decentralized alternatives—as seen in Discord’s mandatory age verification triggering a migration to federated Matrix servers—will intensify, especially as regulations demand more identity transparency. Second, expect more innovation at the “last mile” of AI: the tooling, harnesses, and specification layers that determine whether a model’s potential is actually realized. Third, while cost and performance improve, security will become a differentiator; organizations should audit their AI vendors for defenses against distillation, prompt injection, and other emerging threats. The opportunity lies not just in adopting AI, but in shaping how it integrates into your workflows—with the right safeguards, interfaces, and human-in-the-loop design.\n\nIn sum, today’s AI story isn’t about who has the biggest model—it’s about who builds the most reliable, interpretable, and well-integrated systems. From enterprise adoption surging past $14 billion in revenue to university labs running petaflop-scale AI on desktop supercomputers like NVIDIA’s DGX Spark, the field is maturing rapidly. The winners won’t just be those with the best algorithms, but those who understand that deployment, trust, and thoughtful design are now the true frontiers of artificial intelligence.",
  "audioUrl": "",
  "papers": [
    {
      "id": "arxiv_2602_10437v1",
      "title": "Control Reinforcement Learning: Token-Level Mechanistic Analysis via Learned SAE Feature Steering",
      "titleZh": "Control Reinforcement Learning: Token-Level Mechanistic Analysis via Learned SAE Feature Steering",
      "titleEn": "Control Reinforcement Learning: Token-Level Mechanistic Analysis via Learned SAE Feature Steering",
      "url": "https://arxiv.org/abs/2602.10437v1",
      "type": "paper",
      "source": "arXiv",
      "summary": "该研究提出Control Reinforcement Learning（CRL）框架，通过强化学习策略在每个token级别动态选择稀疏自编码器（SAE）特征进行干预，从而识别出真正影响语言模型输出的可解释特征；其引入的自适应特征掩码机制促进多样性发现，同时保留单特征可解释性，并支持分支点追踪、评论家轨迹分析和层间特征对比等新分析能力，在Gemma-2 2B模型上于MMLU、BBQ等多个基准测试中实现性能提升并提供逐token干预日志，确立了学习型特征引导作为动态机制可解释性工具的价值。",
      "summaryZh": "该研究提出Control Reinforcement Learning（CRL）框架，通过强化学习策略在每个token级别动态选择稀疏自编码器（SAE）特征进行干预，从而识别出真正影响语言模型输出的可解释特征；其引入的自适应特征掩码机制促进多样性发现，同时保留单特征可解释性，并支持分支点追踪、评论家轨迹分析和层间特征对比等新分析能力，在Gemma-2 2B模型上于MMLU、BBQ等多个基准测试中实现性能提升并提供逐token干预日志，确立了学习型特征引导作为动态机制可解释性工具的价值。",
      "summaryEn": "This work introduces Control Reinforcement Learning (CRL), a framework that trains a policy to dynamically select Sparse Autoencoder (SAE) features for steering at each token to identify which interpretable features actually alter language model outputs. With Adaptive Feature Masking to encourage diverse yet individually interpretable feature discovery, CRL enables new analytical capabilities—including branch point tracking, critic trajectory analysis, and layer-wise comparisons revealing syntactic-to-semantic feature progression. Evaluated on Gemma-2 2B across MMLU, BBQ, GSM8K, HarmBench, and XSTest, CRL improves performance while providing per-token intervention logs, establishing learned feature steering as a dynamic mechanistic interpretability tool that complements static analysis.",
      "fullText": "",
      "imageUrl": "",
      "tags": [
        "LLM",
        "RAG"
      ],
      "paperCategory": "General AI",
      "signalReasons": [
        "来源：arXiv",
        "跨源重复：1 个来源",
        "模型评分：10/10，理由：首次实现基于SAE特征的可控制作强化学习，开启大模型内部机制可解释与可控干预的新纪元。",
        "热度：15 / 评论 0"
      ],
      "score": 10.0,
      "publishedAt": "2026-02-11T02:28:49+00:00",
      "authors": [
        "Seonglae Cho",
        "Zekun Wu",
        "Adriano Koshiyama"
      ]
    },
    {
      "id": "arxiv_2602_10980v1",
      "title": "RADAR: Benchmarking Vision-Language-Action Generalization via Real-World Dynamics, Spatial-Physical Intelligence, and Autonomous Evaluation",
      "titleZh": "RADAR: Benchmarking Vision-Language-Action Generalization via Real-World Dynamics, Spatial-Physical Intelligence, and Autonomous Evaluation",
      "titleEn": "RADAR: Benchmarking Vision-Language-Action Generalization via Real-World Dynamics, Spatial-Physical Intelligence, and Autonomous Evaluation",
      "url": "https://arxiv.org/abs/2602.10980v1",
      "type": "paper",
      "source": "arXiv",
      "summary": "针对当前视觉-语言-动作（VLA）模型评估局限于仿真或高度受限环境的问题，该研究提出RADAR基准，系统性地整合真实世界动态（如物体配置变化、传感器噪声）、空间-物理智能任务（测试几何推理能力）以及基于3D指标的全自动评估流程；对多个前沿VLA模型的审计显示，其在轻微物理扰动下性能急剧下降（3D IoU从0.261降至0.068），且空间推理能力有限，揭示了现有模型在现实泛化中的严重脆弱性，为可靠评估VLA系统提供了必要标准。",
      "summaryZh": "针对当前视觉-语言-动作（VLA）模型评估局限于仿真或高度受限环境的问题，该研究提出RADAR基准，系统性地整合真实世界动态（如物体配置变化、传感器噪声）、空间-物理智能任务（测试几何推理能力）以及基于3D指标的全自动评估流程；对多个前沿VLA模型的审计显示，其在轻微物理扰动下性能急剧下降（3D IoU从0.261降至0.068），且空间推理能力有限，揭示了现有模型在现实泛化中的严重脆弱性，为可靠评估VLA系统提供了必要标准。",
      "summaryEn": "Addressing the reality gap in Vision-Language-Action (VLA) model evaluation—currently confined to simulations or constrained settings—this paper introduces RADAR (Real-world Autonomous Dynamics And Reasoning), a benchmark integrating real-world dynamics (e.g., dynamic object states, sensor noise), spatial–physical reasoning tasks, and a fully autonomous 3D-metric-based evaluation pipeline. Audits of state-of-the-art VLA models reveal severe fragility: 3D IoU drops from 0.261 to 0.068 under modest sensor noise, and spatial reasoning remains limited. RADAR thus establishes a necessary standard for reliable, generalizable real-world VLA assessment.",
      "fullText": "",
      "imageUrl": "",
      "tags": [
        "Vision",
        "Multimodal",
        "Robotics",
        "3D"
      ],
      "paperCategory": "Robotics",
      "signalReasons": [
        "来源：arXiv",
        "跨源重复：1 个来源",
        "模型评分：10/10，理由：创建首个面向真实世界动态的VLA通用评估基准，填补评估空白，将重塑机器人与AI系统评测范式。",
        "热度：13 / 评论 0"
      ],
      "score": 10.0,
      "publishedAt": "2026-02-11T16:08:30+00:00",
      "authors": [
        "Yuhao Chen",
        "Zhihao Zhan",
        "Xiaoxin Lin"
      ]
    },
    {
      "id": "arxiv_2602_11021v1",
      "title": "ContactGaussian-WM: Learning Physics-Grounded World Model from Videos",
      "titleZh": "ContactGaussian-WM: Learning Physics-Grounded World Model from Videos",
      "titleEn": "ContactGaussian-WM: Learning Physics-Grounded World Model from Videos",
      "url": "https://arxiv.org/abs/2602.11021v1",
      "type": "paper",
      "source": "arXiv",
      "summary": "为应对数据稀缺和复杂接触动力学下世界模型建模困难的问题，该研究提出ContactGaussian-WM，一种可微分的物理驱动刚体世界模型，通过统一高斯表示同时建模视觉外观与碰撞几何，并端到端地反向传播通过闭式物理引擎以从稀疏视频中推断物理属性；实验表明该方法在模拟与真实场景中均优于现有技术，具备强泛化能力，并成功应用于数据合成与实时模型预测控制（MPC）等下游任务。",
      "summaryZh": "为应对数据稀缺和复杂接触动力学下世界模型建模困难的问题，该研究提出ContactGaussian-WM，一种可微分的物理驱动刚体世界模型，通过统一高斯表示同时建模视觉外观与碰撞几何，并端到端地反向传播通过闭式物理引擎以从稀疏视频中推断物理属性；实验表明该方法在模拟与真实场景中均优于现有技术，具备强泛化能力，并成功应用于数据合成与实时模型预测控制（MPC）等下游任务。",
      "summaryEn": "To address challenges in learning world models under data scarcity and complex contact-rich dynamics, this work proposes ContactGaussian-WM, a differentiable physics-grounded rigid-body world model that uses a unified Gaussian representation for both visual appearance and collision geometry and performs end-to-end learning by differentiating through a closed-form physics engine to infer physical properties from sparse videos. Extensive evaluations show it outperforms state-of-the-art methods in simulation and real-world settings with strong generalization, and demonstrate practical utility in data synthesis and real-time Model Predictive Control (MPC).",
      "fullText": "",
      "imageUrl": "",
      "tags": [
        "Robotics",
        "3D",
        "Benchmark"
      ],
      "paperCategory": "Robotics",
      "signalReasons": [
        "来源：arXiv",
        "跨源重复：1 个来源",
        "模型评分：9/10，理由：基于物理约束的世界模型从视频中学习接触动力学，对机器人规划与仿真具有变革性意义，解决数据稀缺与复杂交互建模难题，有望成为下一代具身智能核心基础。",
        "热度：10 / 评论 0"
      ],
      "score": 9.0,
      "publishedAt": "2026-02-11T16:48:13+00:00",
      "authors": [
        "Meizhong Wang",
        "Wanxin Jin",
        "Kun Cao"
      ]
    },
    {
      "id": "arxiv_2602_11146v1",
      "title": "Beyond VLM-Based Rewards: Diffusion-Native Latent Reward Modeling",
      "titleZh": "Beyond VLM-Based Rewards: Diffusion-Native Latent Reward Modeling",
      "titleEn": "Beyond VLM-Based Rewards: Diffusion-Native Latent Reward Modeling",
      "url": "https://arxiv.org/abs/2602.11146v1",
      "type": "paper",
      "source": "arXiv",
      "summary": "针对扩散模型偏好优化中依赖高成本视觉-语言模型（VLM）奖励函数及像素空间奖励与潜在生成域不匹配的问题，该研究提出DiNa-LRM——一种原生扩散潜在奖励模型，直接在带噪扩散状态上建模偏好，采用噪声校准的Thurstone似然与时间步条件奖励头，并支持推理时噪声集成；在图像对齐任务中，DiNa-LRM以显著更低计算成本达到与顶尖VLM相当的性能，并改善偏好优化动态，实现更快、更高效的模型对齐。",
      "summaryZh": "针对扩散模型偏好优化中依赖高成本视觉-语言模型（VLM）奖励函数及像素空间奖励与潜在生成域不匹配的问题，该研究提出DiNa-LRM——一种原生扩散潜在奖励模型，直接在带噪扩散状态上建模偏好，采用噪声校准的Thurstone似然与时间步条件奖励头，并支持推理时噪声集成；在图像对齐任务中，DiNa-LRM以显著更低计算成本达到与顶尖VLM相当的性能，并改善偏好优化动态，实现更快、更高效的模型对齐。",
      "summaryEn": "To overcome the high computational cost of Vision-Language Model (VLM)-based rewards and the domain mismatch between pixel-space rewards and latent diffusion generators, this paper proposes DiNa-LRM, a diffusion-native latent reward model that formulates preference learning directly on noisy diffusion states using a noise-calibrated Thurstone likelihood and a timestep-conditioned reward head, with inference-time noise ensembling. DiNa-LRM matches state-of-the-art VLM performance at a fraction of the cost and improves preference optimization dynamics, enabling faster and more efficient alignment.",
      "fullText": "",
      "imageUrl": "",
      "tags": [
        "LLM",
        "Vision",
        "Multimodal",
        "Diffusion"
      ],
      "paperCategory": "Computer Vision",
      "signalReasons": [
        "来源：arXiv",
        "跨源重复：1 个来源",
        "模型评分：9/10，理由：提出扩散模型原生的潜在奖励建模方法，突破依赖VLM的奖励瓶颈，显著提升生成质量与训练效率，可能重塑扩散模型对齐范式，具备全球级技术影响力。",
        "热度：19 / 评论 0"
      ],
      "score": 9.0,
      "publishedAt": "2026-02-11T18:57:29+00:00",
      "authors": [
        "Gongye Liu",
        "Bo Yang",
        "Yida Zhi"
      ]
    },
    {
      "id": "arxiv_2602_10675v1",
      "title": "TwiFF (Think With Future Frames): A Large-Scale Dataset for Dynamic Visual Reasoning",
      "titleZh": "TwiFF (Think With Future Frames): A Large-Scale Dataset for Dynamic Visual Reasoning",
      "titleEn": "TwiFF (Think With Future Frames): A Large-Scale Dataset for Dynamic Visual Reasoning",
      "url": "https://arxiv.org/abs/2602.10675v1",
      "type": "paper",
      "source": "arXiv",
      "summary": "为突破现有视觉思维链（VCoT）方法局限于静态场景的瓶颈，该研究构建了TwiFF-2.7M——首个大规模动态视觉推理数据集，包含270万段视频片段，并配套高质量评估基准TwiFF-Bench（1,078个样本）；同时提出TwiFF模型，融合预训练视频生成与图像理解能力，通过迭代生成未来动作帧与文本推理实现时序一致的视觉推理，在动态问答任务上显著超越现有VCoT与纯文本思维链方法，验证了动态视觉推理的有效性。",
      "summaryZh": "为突破现有视觉思维链（VCoT）方法局限于静态场景的瓶颈，该研究构建了TwiFF-2.7M——首个大规模动态视觉推理数据集，包含270万段视频片段，并配套高质量评估基准TwiFF-Bench（1,078个样本）；同时提出TwiFF模型，融合预训练视频生成与图像理解能力，通过迭代生成未来动作帧与文本推理实现时序一致的视觉推理，在动态问答任务上显著超越现有VCoT与纯文本思维链方法，验证了动态视觉推理的有效性。",
      "summaryEn": "To overcome the static-scenario limitation of existing Visual Chain-of-Thought (VCoT) methods, this work introduces TwiFF-2.7M—the first large-scale temporally grounded VCoT dataset derived from 2.7 million video clips—and TwiFF-Bench, a high-quality evaluation benchmark with 1,078 samples assessing reasoning plausibility and answer correctness in dynamic settings. The proposed TwiFF model synergistically combines pretrained video generation and image comprehension to iteratively produce future action frames and textual reasoning, significantly outperforming prior VCoT and textual CoT baselines on dynamic visual question answering.",
      "fullText": "",
      "imageUrl": "",
      "tags": [
        "Vision",
        "Multimodal",
        "RAG",
        "Reasoning"
      ],
      "paperCategory": "Computer Vision",
      "signalReasons": [
        "来源：arXiv",
        "跨源重复：1 个来源",
        "模型评分：9/10，理由：构建大规模动态视觉推理数据集TwiFF，填补了视觉链式思维在动态场景中的空白，将推动具身智能与视频理解的范式演进，具备全球行业影响力。",
        "热度：13 / 评论 0"
      ],
      "score": 9.0,
      "publishedAt": "2026-02-11T09:20:04+00:00",
      "authors": [
        "Junhua Liu",
        "Zhangcheng Wang",
        "Zhike Han"
      ]
    },
    {
      "id": "arxiv_2602_10715v1",
      "title": "Locomo-Plus: Beyond-Factual Cognitive Memory Evaluation Framework for LLM Agents",
      "titleZh": "Locomo-Plus: Beyond-Factual Cognitive Memory Evaluation Framework for LLM Agents",
      "titleEn": "Locomo-Plus: Beyond-Factual Cognitive Memory Evaluation Framework for LLM Agents",
      "url": "https://arxiv.org/abs/2602.10715v1",
      "type": "paper",
      "source": "arXiv",
      "summary": "针对大语言模型对话系统长期记忆评估仅关注表面事实回忆的不足，该研究提出LoCoMo-Plus基准，专门评估模型在“线索-触发语义脱节”场景下对隐含约束（如用户状态、目标或价值观）的认知记忆能力；研究指出传统字符串匹配指标和显式任务提示在此类场景中失效，并提出基于约束一致性的统一评估框架，实验证明当前各类记忆机制在此任务上仍表现不佳，揭示了现有基准未能捕捉的关键失败模式。",
      "summaryZh": "针对大语言模型对话系统长期记忆评估仅关注表面事实回忆的不足，该研究提出LoCoMo-Plus基准，专门评估模型在“线索-触发语义脱节”场景下对隐含约束（如用户状态、目标或价值观）的认知记忆能力；研究指出传统字符串匹配指标和显式任务提示在此类场景中失效，并提出基于约束一致性的统一评估框架，实验证明当前各类记忆机制在此任务上仍表现不佳，揭示了现有基准未能捕捉的关键失败模式。",
      "summaryEn": "Addressing the limitation of existing benchmarks that evaluate long-term memory in LLM agents only via surface-level factual recall, this paper introduces LoCoMo-Plus—a benchmark for assessing cognitive memory under cue–trigger semantic disconnect, where models must retain and apply latent constraints (e.g., user state, goals, values) across long conversations. It shows conventional string-matching metrics and explicit prompting are misaligned with this setting and proposes a constraint-consistency-based evaluation framework. Experiments reveal that current memory systems struggle significantly, exposing failures invisible to prior benchmarks.",
      "fullText": "",
      "imageUrl": "",
      "tags": [
        "LLM",
        "Agent",
        "RAG",
        "Open Source"
      ],
      "paperCategory": "General AI",
      "signalReasons": [
        "来源：arXiv",
        "跨源重复：1 个来源",
        "模型评分：9/10，理由：提出超越事实记忆的认知记忆评估框架Locomo-Plus，首次系统刻画长时对话中隐含状态与目标依赖，将重塑LLM代理评估标准，具有战略意义。",
        "热度：11 / 评论 0"
      ],
      "score": 9.0,
      "publishedAt": "2026-02-11T10:22:35+00:00",
      "authors": [
        "Yifei Li",
        "Weidong Guo",
        "Lingling Zhang"
      ]
    },
    {
      "id": "arxiv_2602_10687v1",
      "title": "OmniVL-Guard: Towards Unified Vision-Language Forgery Detection and Grounding via Balanced RL",
      "titleZh": "OmniVL-Guard: Towards Unified Vision-Language Forgery Detection and Grounding via Balanced RL",
      "titleEn": "OmniVL-Guard: Towards Unified Vision-Language Forgery Detection and Grounding via Balanced RL",
      "url": "https://arxiv.org/abs/2602.10687v1",
      "type": "paper",
      "source": "arXiv",
      "summary": "为应对现实虚假信息中多模态交织（文本、图像、视频）带来的伪造检测挑战，该研究提出OmniVL-Guard——一个统一的视觉-语言伪造检测与定位框架，通过自进化思维链生成缓解冷启动问题，并设计自适应奖励缩放策略优化（ARSPO）以平衡检测与细粒度定位任务间的梯度竞争；实验表明该方法在多模态伪造检测与零样本跨域泛化上显著优于现有技术，有效解决多任务优化中的“难度偏差”问题。",
      "summaryZh": "为应对现实虚假信息中多模态交织（文本、图像、视频）带来的伪造检测挑战，该研究提出OmniVL-Guard——一个统一的视觉-语言伪造检测与定位框架，通过自进化思维链生成缓解冷启动问题，并设计自适应奖励缩放策略优化（ARSPO）以平衡检测与细粒度定位任务间的梯度竞争；实验表明该方法在多模态伪造检测与零样本跨域泛化上显著优于现有技术，有效解决多任务优化中的“难度偏差”问题。",
      "summaryEn": "To tackle forgery detection in real-world misinformation involving interleaved text, images, and videos, this paper proposes OmniVL-Guard—a unified vision-language framework for joint detection and grounding. It addresses the 'difficulty bias' problem in multi-task optimization (where easier veracity classification dominates gradients) via Self-Evolving CoT Generation to overcome cold-start and Adaptive Reward Scaling Policy Optimization (ARSPO) to balance task gradients. OmniVL-Guard significantly outperforms state-of-the-art methods and demonstrates strong zero-shot generalization across out-of-domain scenarios.",
      "fullText": "",
      "imageUrl": "",
      "tags": [
        "Vision",
        "Multimodal",
        "Reasoning",
        "Research"
      ],
      "paperCategory": "Computer Vision",
      "signalReasons": [
        "来源：arXiv",
        "跨源重复：1 个来源",
        "模型评分：9/10，理由：提出统一的跨模态伪造检测与定位框架OmniVL-Guard，应对真实世界多模态信息污染，对全球数字信任体系构建具有变革性意义。",
        "热度：15 / 评论 0"
      ],
      "score": 9.0,
      "publishedAt": "2026-02-11T09:41:36+00:00",
      "authors": [
        "Jinjie Shen",
        "Jing Wu",
        "Yaxiong Wang"
      ]
    },
    {
      "id": "arxiv_2602_10825v1",
      "title": "Flow caching for autoregressive video generation",
      "titleZh": "Flow caching for autoregressive video generation",
      "titleEn": "Flow caching for autoregressive video generation",
      "url": "https://arxiv.org/abs/2602.10825v1",
      "type": "paper",
      "source": "arXiv",
      "summary": "针对自回归视频生成模型因顺序生成而速度缓慢的问题，该研究提出FlowCache——首个专为自回归视频生成设计的缓存框架，其核心是分块缓存策略：允许每个视频块独立决定是否重计算，并结合重要性-冗余联合优化的KV缓存压缩机制，在固定内存开销下保持生成质量；在MAGI-1和SkyReels-V2上分别实现2.38倍和6.7倍加速，VBench质量指标变化可忽略，为超长视频的实时生成提供了高效解决方案。",
      "summaryZh": "针对自回归视频生成模型因顺序生成而速度缓慢的问题，该研究提出FlowCache——首个专为自回归视频生成设计的缓存框架，其核心是分块缓存策略：允许每个视频块独立决定是否重计算，并结合重要性-冗余联合优化的KV缓存压缩机制，在固定内存开销下保持生成质量；在MAGI-1和SkyReels-V2上分别实现2.38倍和6.7倍加速，VBench质量指标变化可忽略，为超长视频的实时生成提供了高效解决方案。",
      "summaryEn": "To accelerate slow sequential generation in autoregressive video models, this work presents FlowCache—the first caching framework tailored for autoregressive video generation. Its chunkwise caching strategy allows independent recomputation decisions per video chunk, complemented by a joint importance-redundancy optimized KV cache compression that maintains fixed memory bounds without quality loss. FlowCache achieves 2.38× speedup on MAGI-1 and 6.7× on SkyReels-V2 with negligible VBench degradation (±<1), enabling real-time ultra-long video synthesis.",
      "fullText": "",
      "imageUrl": "",
      "tags": [
        "Diffusion",
        "Research",
        "Open Source",
        "Benchmark"
      ],
      "paperCategory": "Computer Vision",
      "signalReasons": [
        "来源：arXiv",
        "跨源重复：1 个来源",
        "模型评分：9/10，理由：提出流缓存技术加速自回归视频生成，直接解决长视频生成瓶颈，可能推动下一代视频生成产品落地。",
        "热度：15 / 评论 0"
      ],
      "score": 9.0,
      "publishedAt": "2026-02-11T13:11:04+00:00",
      "authors": [
        "Yuexiao Ma",
        "Xuzhe Zheng",
        "Jing Xu"
      ]
    },
    {
      "id": "arxiv_2602_10450v1",
      "title": "Constructing Industrial-Scale Optimization Modeling Benchmark",
      "titleZh": "Constructing Industrial-Scale Optimization Modeling Benchmark",
      "titleEn": "Constructing Industrial-Scale Optimization Modeling Benchmark",
      "url": "https://arxiv.org/abs/2602.10450v1",
      "type": "paper",
      "source": "arXiv",
      "summary": "针对工业级优化建模缺乏真实自然语言到求解器代码对齐基准的问题，研究者提出MIPLIB-NL，通过从MIPLIB 2017真实混合整数线性规划实例出发，采用结构感知的逆向构建方法，恢复模型结构、生成与之严格对应的自然语言描述，并经专家与人机交互验证，最终形成223个保真重建样本；实验表明，现有在小型合成数据上表现优异的系统在MIPLIB-NL上性能显著下降，揭示了其在处理含10³–10⁶变量/约束的真实工业问题时的失效模式。",
      "summaryZh": "针对工业级优化建模缺乏真实自然语言到求解器代码对齐基准的问题，研究者提出MIPLIB-NL，通过从MIPLIB 2017真实混合整数线性规划实例出发，采用结构感知的逆向构建方法，恢复模型结构、生成与之严格对应的自然语言描述，并经专家与人机交互验证，最终形成223个保真重建样本；实验表明，现有在小型合成数据上表现优异的系统在MIPLIB-NL上性能显著下降，揭示了其在处理含10³–10⁶变量/约束的真实工业问题时的失效模式。",
      "summaryEn": "To address the lack of realistic benchmarks aligning natural-language specifications with solver-executable code in industrial-scale optimization, researchers introduce MIPLIB-NL—a dataset built via structure-aware reverse construction from real mixed-integer linear programs in MIPLIB 2017. Their pipeline recovers compact model structures, generates tightly coupled natural-language descriptions under a unified model–data separation format, and validates semantics through expert review and human–LLM interaction, yielding 223 one-to-one reconstructions. Experiments reveal significant performance degradation on MIPLIB-NL for systems that excel on toy benchmarks, exposing failure modes invisible at small scale when handling problems with 10³–10⁶+ variables and constraints.",
      "fullText": "",
      "imageUrl": "",
      "tags": [
        "LLM",
        "Benchmark"
      ],
      "paperCategory": "General AI",
      "signalReasons": [
        "来源：arXiv",
        "跨源重复：1 个来源",
        "模型评分：9/10，理由：构建工业级优化建模基准，推动LLM在复杂决策系统中的规模化应用，具有国家级产业意义。",
        "热度：8 / 评论 0"
      ],
      "score": 9.0,
      "publishedAt": "2026-02-11T02:45:31+00:00",
      "authors": [
        "Zhong Li",
        "Hongliang Lu",
        "Tao Wei"
      ]
    },
    {
      "id": "arxiv_2602_10361v1",
      "title": "ENIGMA: EEG-to-Image in 15 Minutes Using Less Than 1% of the Parameters",
      "titleZh": "ENIGMA: EEG-to-Image in 15 Minutes Using Less Than 1% of the Parameters",
      "titleEn": "ENIGMA: EEG-to-Image in 15 Minutes Using Less Than 1% of the Parameters",
      "url": "https://arxiv.org/abs/2602.10361v1",
      "type": "paper",
      "source": "arXiv",
      "summary": "为推动脑机接口实用化，研究者提出ENIGMA模型，仅用不到1%的可训练参数即可在15分钟内完成新受试者微调，在科研级THINGS-EEG2和消费级AllJoined-1.6M数据集上实现SOTA图像重建效果；该模型采用统一时空主干、多受试者潜在对齐层和MLP投影器，将原始EEG信号映射至视觉潜在空间，并首次在EEG-to-Image任务中引入人类评分的行为评估，显著提升在低成本硬件上的部署效率与推理经济性。",
      "summaryZh": "为推动脑机接口实用化，研究者提出ENIGMA模型，仅用不到1%的可训练参数即可在15分钟内完成新受试者微调，在科研级THINGS-EEG2和消费级AllJoined-1.6M数据集上实现SOTA图像重建效果；该模型采用统一时空主干、多受试者潜在对齐层和MLP投影器，将原始EEG信号映射至视觉潜在空间，并首次在EEG-to-Image任务中引入人类评分的行为评估，显著提升在低成本硬件上的部署效率与推理经济性。",
      "summaryEn": "To advance practical brain-computer interfaces, researchers propose ENIGMA—a lightweight EEG-to-image decoding model requiring less than 1% of the trainable parameters of prior approaches and achieving state-of-the-art performance on both research-grade (THINGS-EEG2) and consumer-grade (AllJoined-1.6M) benchmarks. It fine-tunes effectively on new subjects with as little as 15 minutes of data using a subject-unified spatio-temporal backbone, multi-subject latent alignment layers, and an MLP projector to map raw EEG to a rich visual latent space. ENIGMA is the first EEG-to-image study to include extensive behavioral evaluations via human raters, demonstrating robust performance across hardware tiers and substantial gains in fine-tuning efficiency and inference cost.",
      "fullText": "",
      "imageUrl": "",
      "tags": [
        "Vision",
        "Training",
        "Inference",
        "Research"
      ],
      "paperCategory": "General AI",
      "signalReasons": [
        "来源：arXiv",
        "跨源重复：1 个来源",
        "模型评分：9/10，理由：EEG到图像生成仅用不到1%参数且15分钟完成，突破脑机接口部署瓶颈，具备全球医疗AI变革潜力。",
        "热度：12 / 评论 0"
      ],
      "score": 9.0,
      "publishedAt": "2026-02-10T23:20:51+00:00",
      "authors": [
        "Reese Kneeland",
        "Wangshu Jiang",
        "Ugo Bruzadin Nunes"
      ]
    },
    {
      "id": "arxiv_2602_10429v1",
      "title": "AIvilization v0: Toward Large-Scale Artificial Social Simulation with a Unified Agent Architecture and Adaptive Agent Profiles",
      "titleZh": "AIvilization v0: Toward Large-Scale Artificial Social Simulation with a Unified Agent Architecture and Adaptive Agent Profiles",
      "titleEn": "AIvilization v0: Toward Large-Scale Artificial Social Simulation with a Unified Agent Architecture and Adaptive Agent Profiles",
      "url": "https://arxiv.org/abs/2602.10429v1",
      "type": "paper",
      "source": "arXiv",
      "summary": "AIvilization v0构建了一个大规模人工社会模拟平台，结合资源受限的沙盒经济与统一LLM智能体架构，通过分层分支规划器、双过程记忆的自适应智能体画像及人在环路引导机制，支持长期自主行为；环境包含生理生存成本、多层级不可替代生产、AMM定价机制和教育-职业通道，实证显示其能复现重尾收益、波动聚集等市场典型事实，并形成由教育与准入限制驱动的财富分层，且完整架构在多目标长周期任务中显著优于简化基线。",
      "summaryZh": "AIvilization v0构建了一个大规模人工社会模拟平台，结合资源受限的沙盒经济与统一LLM智能体架构，通过分层分支规划器、双过程记忆的自适应智能体画像及人在环路引导机制，支持长期自主行为；环境包含生理生存成本、多层级不可替代生产、AMM定价机制和教育-职业通道，实证显示其能复现重尾收益、波动聚集等市场典型事实，并形成由教育与准入限制驱动的财富分层，且完整架构在多目标长周期任务中显著优于简化基线。",
      "summaryEn": "AIvilization v0 presents a large-scale artificial society combining a resource-constrained sandbox economy with a unified LLM-agent architecture to sustain long-horizon autonomy. It introduces a hierarchical branch-thinking planner, an adaptive agent profile with dual-process memory separating short-term execution from long-term identity consolidation, and a human-in-the-loop steering interface that propagates goals through memory. The environment features physiological costs, non-substitutable multi-tier production, an AMM-based pricing mechanism, and a gated education-occupation system. Empirical analysis reveals stable markets reproducing stylized facts like heavy-tailed returns and volatility clustering, along with structured wealth stratification. Ablations confirm the full architecture’s superiority in multi-objective, long-horizon settings over simplified planners.",
      "fullText": "",
      "imageUrl": "",
      "tags": [
        "LLM",
        "Agent"
      ],
      "paperCategory": "General AI",
      "signalReasons": [
        "来源：arXiv",
        "跨源重复：1 个来源",
        "模型评分：9/10，理由：AIvilization v0构建大规模人工社会仿真系统，首次实现统一LLM代理与动态行为演化，具有全球科研范式变革意义。",
        "热度：14 / 评论 0"
      ],
      "score": 9.0,
      "publishedAt": "2026-02-11T02:18:15+00:00",
      "authors": [
        "Wenkai Fan",
        "Shurui Zhang",
        "Xiaolong Wang"
      ]
    },
    {
      "id": "arxiv_2602_10639v1",
      "title": "VideoSTF: Stress-Testing Output Repetition in Video Large Language Models",
      "titleZh": "VideoSTF: Stress-Testing Output Repetition in Video Large Language Models",
      "titleEn": "VideoSTF: Stress-Testing Output Repetition in Video Large Language Models",
      "url": "https://arxiv.org/abs/2602.10639v1",
      "type": "paper",
      "source": "arXiv",
      "summary": "研究发现当前视频大语言模型（VideoLLM）普遍存在严重输出重复问题——即陷入自我强化的短语循环，而现有评测未覆盖此缺陷；为此提出VideoSTF框架，通过三项n-gram指标和包含10,000个多样化视频及可控时序变换的测试平台，对10个先进VideoLLM进行压力测试，结果表明输出重复广泛存在且对输入时序扰动高度敏感，简单变换即可在黑盒场景下诱发重复退化，揭示其作为可利用的安全漏洞，呼吁将稳定性纳入视频语言系统评估标准。",
      "summaryZh": "研究发现当前视频大语言模型（VideoLLM）普遍存在严重输出重复问题——即陷入自我强化的短语循环，而现有评测未覆盖此缺陷；为此提出VideoSTF框架，通过三项n-gram指标和包含10,000个多样化视频及可控时序变换的测试平台，对10个先进VideoLLM进行压力测试，结果表明输出重复广泛存在且对输入时序扰动高度敏感，简单变换即可在黑盒场景下诱发重复退化，揭示其作为可利用的安全漏洞，呼吁将稳定性纳入视频语言系统评估标准。",
      "summaryEn": "Researchers identify severe output repetition—self-reinforcing loops of repeated phrases—as a widespread but previously overlooked failure mode in Video Large Language Models (VideoLLMs), unaddressed by current benchmarks focused on accuracy. They introduce VideoSTF, the first framework to systematically stress-test this issue using three n-gram-based metrics and a standardized testbed of 10,000 diverse videos with controlled temporal transformations. Testing across 10 advanced VideoLLMs shows output repetition is highly sensitive to minor input perturbations, and simple temporal modifications can reliably induce degeneration in black-box settings, exposing it as an exploitable security vulnerability. The work calls for stability-aware evaluation in video-language systems.",
      "fullText": "",
      "imageUrl": "",
      "tags": [
        "LLM",
        "Open Source",
        "Benchmark"
      ],
      "paperCategory": "Computer Vision",
      "signalReasons": [
        "来源：arXiv",
        "跨源重复：1 个来源",
        "模型评分：9/10，理由：首次系统揭示视频大语言模型的输出重复缺陷，提出VideoSTF评测框架，直击当前多模态生成模型的核心瓶颈，将引导行业对生成稳定性评估标准的重构。",
        "热度：11 / 评论 0"
      ],
      "score": 9.0,
      "publishedAt": "2026-02-11T08:40:48+00:00",
      "authors": [
        "Yuxin Cao",
        "Wei Song",
        "Shangzhi Xu"
      ]
    },
    {
      "id": "arxiv_2602_10343v1",
      "title": "Conditional Uncertainty-Aware Political Deepfake Detection with Stochastic Convolutional Neural Networks",
      "titleZh": "Conditional Uncertainty-Aware Political Deepfake Detection with Stochastic Convolutional Neural Networks",
      "titleEn": "Conditional Uncertainty-Aware Political Deepfake Detection with Stochastic Convolutional Neural Networks",
      "url": "https://arxiv.org/abs/2602.10343v1",
      "type": "paper",
      "source": "arXiv",
      "summary": "针对政治深度伪造检测系统缺乏可靠性评估的问题，研究提出基于随机卷积神经网络的条件不确定性感知方法，在构建的政治图像数据集上对比多种不确定性估计策略（如MC Dropout、集成学习等），通过校准质量、评分规则与错误对齐分析证明：校准后的概率输出和不确定性估计可支持风险感知的内容审核策略，并通过置信区间分析明确其在高风险政治场景中的操作价值边界。",
      "summaryZh": "针对政治深度伪造检测系统缺乏可靠性评估的问题，研究提出基于随机卷积神经网络的条件不确定性感知方法，在构建的政治图像数据集上对比多种不确定性估计策略（如MC Dropout、集成学习等），通过校准质量、评分规则与错误对齐分析证明：校准后的概率输出和不确定性估计可支持风险感知的内容审核策略，并通过置信区间分析明确其在高风险政治场景中的操作价值边界。",
      "summaryEn": "Addressing the operational risk of unreliable predictions in political deepfake detection, this work proposes a conditional uncertainty-aware approach using stochastic convolutional neural networks. On a politically curated binary image dataset, it evaluates deterministic inference against Monte Carlo dropout, temperature scaling, and ensemble-based uncertainty estimates through calibration quality, proper scoring rules, and error alignment analyses. Results show calibrated probabilistic outputs enable risk-aware moderation policies, while systematic confidence-band analysis delineates when uncertainty provides actionable value beyond predicted confidence—clarifying both benefits and limitations in high-stakes political contexts.",
      "fullText": "",
      "imageUrl": "",
      "tags": [
        "Vision",
        "Inference",
        "Industry",
        "Benchmark"
      ],
      "paperCategory": "Computer Vision",
      "signalReasons": [
        "来源：arXiv",
        "跨源重复：1 个来源",
        "模型评分：9/10，理由：针对政治深度伪造的不确定性感知检测方法，直面全球信息安全危机，具备重大社会影响与政策制定参考价值。",
        "热度：19 / 评论 0"
      ],
      "score": 9.0,
      "publishedAt": "2026-02-10T22:31:18+00:00",
      "authors": [
        "Rafael-Petruţ Gardoş"
      ]
    },
    {
      "id": "arxiv_2602_10717v1",
      "title": "Say, Dream, and Act: Learning Video World Models for Instruction-Driven Robot Manipulation",
      "titleZh": "Say, Dream, and Act: Learning Video World Models for Instruction-Driven Robot Manipulation",
      "titleEn": "Say, Dream, and Act: Learning Video World Models for Instruction-Driven Robot Manipulation",
      "url": "https://arxiv.org/abs/2602.10717v1",
      "type": "paper",
      "source": "arXiv",
      "summary": "为提升机器人操作中的环境预测能力，研究提出“说、梦、行”框架：先适配鲁棒视频生成模型以确保未来状态预测可靠性，再通过对抗蒸馏实现快速少步视频生成，最后训练动作模型融合生成视频与真实观测以修正空间误差；实验表明该方法生成的视频兼具时序连贯性与空间准确性，显著提升具身一致性、空间指代能力和任务完成率，优于现有基线。",
      "summaryZh": "为提升机器人操作中的环境预测能力，研究提出“说、梦、行”框架：先适配鲁棒视频生成模型以确保未来状态预测可靠性，再通过对抗蒸馏实现快速少步视频生成，最后训练动作模型融合生成视频与真实观测以修正空间误差；实验表明该方法生成的视频兼具时序连贯性与空间准确性，显著提升具身一致性、空间指代能力和任务完成率，优于现有基线。",
      "summaryEn": "To enhance predictive capabilities in robotic manipulation, researchers propose 'Say, Dream, and Act'—a framework that first adapts a robust video generation model for reliable future prediction, applies adversarial distillation for fast few-step video synthesis, and trains an action model that fuses generated videos with real observations to correct spatial errors. Experiments demonstrate temporally coherent and spatially accurate video predictions that significantly improve embodiment consistency, spatial referring ability, and task completion over existing baselines.",
      "fullText": "",
      "imageUrl": "",
      "tags": [
        "LLM",
        "Vision",
        "Multimodal",
        "Robotics"
      ],
      "paperCategory": "Robotics",
      "signalReasons": [
        "来源：arXiv",
        "跨源重复：1 个来源",
        "模型评分：9/10，理由：结合指令驱动与视频世界模型，实现机器人动作预测能力突破，具备向通用具身智能演进的战略意义。",
        "热度：14 / 评论 0"
      ],
      "score": 9.0,
      "publishedAt": "2026-02-11T10:23:52+00:00",
      "authors": [
        "Songen Gu",
        "Yunuo Cai",
        "Tianyu Wang"
      ]
    },
    {
      "id": "arxiv_2602_10982v1",
      "title": "RiemannGL: Riemannian Geometry Changes Graph Deep Learning",
      "titleZh": "RiemannGL: Riemannian Geometry Changes Graph Deep Learning",
      "titleEn": "RiemannGL: Riemannian Geometry Changes Graph Deep Learning",
      "url": "https://arxiv.org/abs/2602.10982v1",
      "type": "paper",
      "source": "arXiv",
      "summary": "论文主张黎曼几何应成为图表示学习的基础范式而非孤立技巧，指出当前研究多局限于双曲空间且采用外蕴流形表述，忽视了赋予图神经网络内蕴流形结构的核心任务；作者系统梳理现有方法在流形类型、神经架构与学习范式三方面的概念与方法论缺口，提出结构化研究议程，并讨论理论基础与开放挑战，旨在推动黎曼几何作为图学习统一框架的深入探索。",
      "summaryZh": "论文主张黎曼几何应成为图表示学习的基础范式而非孤立技巧，指出当前研究多局限于双曲空间且采用外蕴流形表述，忽视了赋予图神经网络内蕴流形结构的核心任务；作者系统梳理现有方法在流形类型、神经架构与学习范式三方面的概念与方法论缺口，提出结构化研究议程，并讨论理论基础与开放挑战，旨在推动黎曼几何作为图学习统一框架的深入探索。",
      "summaryEn": "This paper argues that Riemannian geometry should serve as a foundational paradigm—not just a collection of isolated techniques—for graph representation learning. It critiques current approaches for being largely confined to hyperbolic spaces and relying on extrinsic manifold formulations, thereby neglecting the core mission of endowing graph neural networks with intrinsic manifold structures. The authors identify key conceptual and methodological gaps across three dimensions—manifold type, neural architecture, and learning paradigm—and outline a structured research agenda alongside open challenges and theoretical foundations to catalyze broader exploration of Riemannian geometry as a unifying framework for graph learning.",
      "fullText": "",
      "imageUrl": "",
      "tags": [
        "Vision",
        "Research"
      ],
      "paperCategory": "General AI",
      "signalReasons": [
        "来源：arXiv",
        "跨源重复：1 个来源",
        "模型评分：8/10，理由：将黎曼几何引入图深度学习，为非欧空间建模提供新范式，可能推动图神经网络在复杂系统建模中的突破，具备显著技术前瞻性与跨领域潜力。",
        "热度：10 / 评论 0"
      ],
      "score": 8.0,
      "publishedAt": "2026-02-11T16:10:53+00:00",
      "authors": [
        "Li Sun",
        "Qiqi Wan",
        "Suyang Zhou"
      ]
    },
    {
      "id": "arxiv_2602_10719v1",
      "title": "From Representational Complementarity to Dual Systems: Synergizing VLM and Vision-Only Backbones for End-to-End Driving",
      "titleZh": "From Representational Complementarity to Dual Systems: Synergizing VLM and Vision-Only Backbones for End-to-End Driving",
      "titleEn": "From Representational Complementarity to Dual Systems: Synergizing VLM and Vision-Only Backbones for End-to-End Driving",
      "url": "https://arxiv.org/abs/2602.10719v1",
      "type": "paper",
      "source": "arXiv",
      "summary": "研究通过RecogDrive平台对比纯视觉（ViT）与视觉语言模型（VLM）在端到端驾驶中的表现，发现二者在长尾场景中行为互补：VLM更激进而ViT更保守，各自在2–3%场景中显著胜出；据此提出HybridDriveVLA，通过学习评分器选择两支轨迹，将PDMS提升至92.10；进一步设计DualDriveVLA快慢策略，默认运行ViT，仅在15%低置信度场景调用VLM，实现91.00 PDMS的同时吞吐量提升3.2倍。",
      "summaryZh": "研究通过RecogDrive平台对比纯视觉（ViT）与视觉语言模型（VLM）在端到端驾驶中的表现，发现二者在长尾场景中行为互补：VLM更激进而ViT更保守，各自在2–3%场景中显著胜出；据此提出HybridDriveVLA，通过学习评分器选择两支轨迹，将PDMS提升至92.10；进一步设计DualDriveVLA快慢策略，默认运行ViT，仅在15%低置信度场景调用VLM，实现91.00 PDMS的同时吞吐量提升3.2倍。",
      "summaryEn": "Through RecogDrive, researchers compare vision-only (ViT) and vision-language model (VLM) backbones in end-to-end driving under an identical diffusion Transformer planner. They find complementary behaviors in long-tail scenarios: VLM tends to be more aggressive while ViT is more conservative, each decisively outperforming the other in ~2–3% of test cases. Building on this, they propose HybridDriveVLA, which uses a learned scorer to select between ViT and VLM trajectories, achieving 92.10 PDMS. DualDriveVLA implements a practical fast–slow policy—running ViT by default and invoking VLM only when scorer confidence drops below a threshold—achieving 91.00 PDMS with 3.2× higher throughput by activating VLM on just 15% of scenarios.",
      "fullText": "",
      "imageUrl": "",
      "tags": [
        "Vision",
        "Multimodal",
        "Diffusion",
        "Training"
      ],
      "paperCategory": "Robotics",
      "signalReasons": [
        "来源：arXiv",
        "跨源重复：1 个来源",
        "模型评分：8/10，理由：提出视觉-语言-动作协同框架，推动端到端自动驾驶系统在复杂场景下的泛化能力，具有显著技术突破和产业应用潜力。",
        "热度：13 / 评论 0"
      ],
      "score": 8.0,
      "publishedAt": "2026-02-11T10:25:05+00:00",
      "authors": [
        "Sining Ang",
        "Yuguang Yang",
        "Chenxu Dang"
      ]
    },
    {
      "id": "arxiv_2602_11113v1",
      "title": "A receding-horizon multi-contact motion planner for legged robots in challenging environments",
      "titleZh": "A receding-horizon multi-contact motion planner for legged robots in challenging environments",
      "titleEn": "A receding-horizon multi-contact motion planner for legged robots in challenging environments",
      "url": "https://arxiv.org/abs/2602.11113v1",
      "type": "paper",
      "source": "arXiv",
      "summary": "该论文提出一种新型滚动时域多接触运动规划器，使足式机器人能在烟囱攀爬、穿越狭窄通道或跨越大间隙等复杂环境中规划全身运动。其关键创新在于同步优化接触点位置与全身轨迹，并支持基于新感知信息的实时重规划，避免了传统多阶段流程和后处理。相比基于势场的方法，该方法更不易陷入局部极小值；其基于二次规划的姿态生成器也比现有算法更快生成节点。实验表明，在短规划视距（如一步）下，规划速度比现有方法快45%至98%，但动作效率略低（支撑切换次数多5%至700%）；在长视距（如四步）下，除“烟囱行走”外，规划质量显著提升（支撑切换减少8%至47%），尽管耗时可能增加。",
      "summaryZh": "该论文提出一种新型滚动时域多接触运动规划器，使足式机器人能在烟囱攀爬、穿越狭窄通道或跨越大间隙等复杂环境中规划全身运动。其关键创新在于同步优化接触点位置与全身轨迹，并支持基于新感知信息的实时重规划，避免了传统多阶段流程和后处理。相比基于势场的方法，该方法更不易陷入局部极小值；其基于二次规划的姿态生成器也比现有算法更快生成节点。实验表明，在短规划视距（如一步）下，规划速度比现有方法快45%至98%，但动作效率略低（支撑切换次数多5%至700%）；在长视距（如四步）下，除“烟囱行走”外，规划质量显著提升（支撑切换减少8%至47%），尽管耗时可能增加。",
      "summaryEn": "This paper presents a novel receding-horizon multi-contact motion planner enabling legged robots to navigate challenging environments like chimney climbing, narrow passages, or large gaps. Its key innovations include simultaneous planning of contact locations and whole-body trajectories, and reactive re-planning based on new sensor data—eliminating post-processing and complex multi-stage pipelines. The method is more robust against local minima than potential-field approaches, and its quadratic-program-based posture generator produces nodes faster than existing algorithms. Experiments show that with short horizons (e.g., one step), planning is 45–98% faster than state-of-the-art, albeit with less efficient motions (5% fewer to 700% more stance changes). With longer horizons (e.g., four steps), motion quality improves significantly (8% more to 47% fewer stance changes) across all but one scenario, though planning time may increase.",
      "fullText": "",
      "imageUrl": "",
      "tags": [
        "Robotics",
        "RAG"
      ],
      "paperCategory": "Robotics",
      "signalReasons": [
        "来源：arXiv",
        "跨源重复：1 个来源",
        "模型评分：8/10，理由：提出新型多接触运动规划算法，显著拓展足式机器人在极端环境中的操作能力，推动机器人自主性发展。",
        "热度：9 / 评论 0"
      ],
      "score": 8.0,
      "publishedAt": "2026-02-11T18:25:29+00:00",
      "authors": [
        "Daniel S. J. Derwent",
        "Simon Watson",
        "Bruno V. Adorno"
      ]
    },
    {
      "id": "arxiv_2602_10983v1",
      "title": "Scaling World Model for Hierarchical Manipulation Policies",
      "titleZh": "Scaling World Model for Hierarchical Manipulation Policies",
      "titleEn": "Scaling World Model for Hierarchical Manipulation Policies",
      "url": "https://arxiv.org/abs/2602.10983v1",
      "type": "paper",
      "source": "arXiv",
      "summary": "为解决视觉-语言-动作（VLA）模型在分布外（OOD）场景中泛化能力弱的问题，该研究提出名为VISTA的分层框架，由大规模预训练世界模型作为高层规划器、VLA作为底层执行器组成。世界模型将任务分解为带目标图像的子任务序列，为目标提供视觉与物理层面的具象引导，显著提升底层策略对新物体和新场景的适应能力。在大量OOD测试中，相同结构的VLA在世界模型引导下，任务成功率从14%跃升至69%，大幅超越现有基线，尤其在未见过的环境中表现突出。",
      "summaryZh": "为解决视觉-语言-动作（VLA）模型在分布外（OOD）场景中泛化能力弱的问题，该研究提出名为VISTA的分层框架，由大规模预训练世界模型作为高层规划器、VLA作为底层执行器组成。世界模型将任务分解为带目标图像的子任务序列，为目标提供视觉与物理层面的具象引导，显著提升底层策略对新物体和新场景的适应能力。在大量OOD测试中，相同结构的VLA在世界模型引导下，任务成功率从14%跃升至69%，大幅超越现有基线，尤其在未见过的环境中表现突出。",
      "summaryEn": "To address the poor out-of-distribution (OOD) generalization of Vision-Language-Action (VLA) models, this work introduces VISTA, a hierarchical framework combining a large-scale pre-trained world model as a high-level planner and a VLA as a low-level executor. The world model decomposes tasks into subtask sequences with synthesized goal images that provide visually and physically grounded guidance, dramatically improving the low-level policy’s adaptability to unseen objects and scenarios. In extensive OOD evaluations, the same VLA architecture achieves task success rates jumping from 14% to 69% with world-model guidance, significantly outperforming prior baselines—especially in novel environments.",
      "fullText": "",
      "imageUrl": "",
      "tags": [
        "Vision",
        "Multimodal",
        "Robotics",
        "RAG"
      ],
      "paperCategory": "Robotics",
      "signalReasons": [
        "来源：arXiv",
        "跨源重复：1 个来源",
        "模型评分：8/10，理由：构建分层VLA框架提升机器人在分布外场景的泛化能力，为通用机器人智能提供关键路径。",
        "热度：16 / 评论 0"
      ],
      "score": 8.0,
      "publishedAt": "2026-02-11T16:12:33+00:00",
      "authors": [
        "Qian Long",
        "Yueze Wang",
        "Jiaxi Song"
      ]
    }
  ],
  "news": [
    {
      "id": "hn_46993345",
      "title": "Anthropic 融资300亿美元，估值达3800亿，Claude企业应用爆发",
      "titleZh": "Anthropic 融资300亿美元，估值达3800亿，Claude企业应用爆发",
      "titleEn": "Anthropic Raises $30B at $380B Valuation as Claude Enterprise Adoption Surges",
      "url": "https://www.anthropic.com/news/anthropic-raises-30-billion-series-g-funding-380-billion-post-money-valuation",
      "type": "news",
      "source": "Hacker News",
      "summary": "**Anthropic 宣布完成300亿美元G轮融资，投后估值达3800亿美元**，由GIC和Coatue领投，资金将用于前沿模型研发、产品迭代与基础设施扩展；公司年化收入已达140亿美元，三年内年均增长超10倍，其中Claude Code年化收入超25亿美元，企业客户中包括8家财富10强公司；此次融资凸显企业对AI智能平台的高度依赖，普通开发者和企业可通过Claude系列工具（如Claude Code、Cowork插件）提升编码、金融分析、销售等知识工作效率，并利用其在AWS、Google Cloud和Azure三大云平台的可用性实现安全合规部署。",
      "summaryZh": "**Anthropic 宣布完成300亿美元G轮融资，投后估值达3800亿美元**，由GIC和Coatue领投，资金将用于前沿模型研发、产品迭代与基础设施扩展；公司年化收入已达140亿美元，三年内年均增长超10倍，其中Claude Code年化收入超25亿美元，企业客户中包括8家财富10强公司；此次融资凸显企业对AI智能平台的高度依赖，普通开发者和企业可通过Claude系列工具（如Claude Code、Cowork插件）提升编码、金融分析、销售等知识工作效率，并利用其在AWS、Google Cloud和Azure三大云平台的可用性实现安全合规部署。",
      "summaryEn": "Anthropic has raised $30 billion in Series G funding at a $380 billion post-money valuation, led by GIC and Coatue, to accelerate frontier AI research, product development, and infrastructure. The company now reports $14 billion in annualized revenue—growing over 10x yearly for three consecutive years—with Claude Code alone generating over $2.5 billion annually. Eight of the Fortune 10 are customers, and enterprise adoption is surging. This signals deep enterprise reliance on AI as a core productivity layer; developers and businesses can leverage Claude’s tools—like Claude Code and Cowork plugins—for coding, finance, sales, and more, with secure deployment across AWS, Google Cloud, and Azure.",
      "fullText": "Skip to main content Skip to footer Research Economic Futures Commitments Learn News Try Claude Announcements Anthropic raises $30 billion in Series G funding at $380 billion post-money valuation Feb 12, 2026 We have raised $30 billion in Series G funding led by GIC and Coatue, valuing Anthropic at $380 billion post-money. The round was co-led by D. E. Shaw Ventures, Dragoneer, Founders Fund, ICONIQ, and MGX. The investment will fuel the frontier research, product development, and infrastructure expansions that have made Anthropic the market leader in enterprise AI and coding. Significant investors in this round include: Accel, Addition, Alpha Wave Global, Altimeter, AMP PBC, Appaloosa LP, Baillie Gifford, Bessemer Venture Partners, affiliated funds of BlackRock, Blackstone, D1 Capital Partners, Fidelity Management & Research Company, General Catalyst, Greenoaks, Growth Equity at Goldman Sachs Alternatives, Insight Partners, Jane Street, JPMorganChase through its Security and Resiliency Initiative and Growth Equity Partners, Lightspeed Venture Partners, Menlo Ventures, Morgan Stanley Investment Management, NX1 Capital, Qatar Investment Authority (QIA), Sands Capital, Sequoia Capital, Temasek, TowerBrook, TPG, Whale Rock Capital, and XN. This round also includes a portion of the previously announced investments from Microsoft and NVIDIA. “Whether it is entrepreneurs, startups, or the world’s largest enterprises, the message from our customers is the same: Claude is increasingly becoming critical to how businesses work,” said Krishna Rao, Anthropic’s Chief Financial Officer. “This fundraising reflects the incredible demand we are seeing from these customers, and we will use this investment to continue building the enterprise-grade products and models they have come to depend on.” It has been less than three years since Anthropic earned its first dollar in revenue. Today, our run-rate revenue is $14 billion, with this figure growing over 10x annually in each of those past three years. This growth has been driven by our position as the intelligence platform of choice for enterprises and developers. The number of customers spending over $100,000 annually on Claude (as represented by run-rate revenue) has grown 7x in the past year. And businesses that start with Claude for a single use case—API, Claude Code, or Claude for Work—are expanding their integrations across their organizations. Two years ago, a dozen customers spent over $1 million with us on an annualized basis. Today that number exceeds 500. Eight of the Fortune 10 are now Claude customers. Claude Code represents a new era of agentic coding, fundamentally changing how teams build software. Claude Code was made available to the general public in May 2025. Today, Claude Code’s run-rate revenue has grown to over $2.5 billion; this figure has more than doubled since the beginning of 2026. The number of weekly active Claude Code users has also doubled since January 1. A recent analysis estimated that 4% of all GitHub public commits worldwide were being authored by Claude Code—double the percentage from just one month prior. Business subscriptions to Claude Code have quadrupled since the start of 2026, and enterprise use has grown to represent over half of all Claude Code revenue. The same capabilities that make Claude exceptional for coding are also unlocking other new categories of work: financial and data analysis , sales , cybersecurity , scientific discovery , and beyond. In January alone, we launched more than thirty products and features, including Cowork , which brings Claude Code’s powerful engineering capabilities to a broader scope of knowledge work tasks. Cowork includes eleven open-source plugins that let customers turn Claude into a specialist for specific roles or teams, like sales, legal, or finance. We also expanded our reach into healthcare and life sciences , with Claude for Enterprise now available to organizations operating under HIPAA. “Since our initial investment in 2025, Anthropic’s focus on agentic coding and enterprise-grade AI systems has accelerated its progress toward large-scale adoption,” said Philippe Laffont, Founder & Portfolio Manager of Coatue. “The team’s ability to rapidly scale its offerings further positions Anthropic as a leader in a highly competitive AI market.” Claude’s frontier-setting intelligence continues to advance. Our newest model— Opus 4.6 , launched last week—can power agents that manage entire categories of real-world work, generating documents, spreadsheets, and presentations with professional polish. And Opus 4.6 is the world’s leading model on GDPval-AA , which measures performance on economically valuable knowledge work tasks in finance, legal, and other domains. “Anthropic is the clear category leader in enterprise AI, demonstrating breakthrough capabilities and setting a new standard for safety, performance, and scale that will drive their long-term success,” said Choo Yong Cheen, Chief Investment Officer, Private Equity, GIC. The Series G will also power our infrastructure expansion as we make Claude available everywhere our customers are. Claude remains the only frontier AI model available to customers on all three of the world's largest cloud platforms: Amazon Web Services (Bedrock), Google Cloud (Vertex AI), and Microsoft Azure (Foundry). We train and run Claude on a diversified range of AI hardware—AWS Trainium, Google TPUs, and NVIDIA GPUs—which means we can match workloads to the chips best suited for them. This diversity of platforms translates to better performance and greater resilience for the enterprise customers that depend on Claude for critical work. The demand we are seeing from enterprises and developers reflects the trust they place in Claude for the work that matters most. As AI moves toward scaled implementation, we will continue to build the models, products, and partnerships to lead that transition. Related content Anthropic is donating $20 million to Public First Action Read more Covering electricity price increases from our data centers Read more Introducing Claude Opus 4.6 We’re upgrading our smartest model. Across agentic coding, computer use, tool use, search, and finance, Opus 4.6 is an industry-leading model, often by wide margin. Read more Products Claude Claude Code Cowork Claude in Chrome Claude in Excel Claude in PowerPoint Claude in Slack Skills Max plan Team plan Enterprise plan Download app Pricing Log in to Claude Models Opus Sonnet Haiku Solutions AI agents Code modernization Coding Customer support Education Financial services Government Healthcare Life sciences Nonprofits Claude Developer Platform Overview Developer docs Pricing Regional compliance Amazon Bedrock Google Cloud’s Vertex AI Console login Learn Blog Claude partner network Connectors Courses Customer stories Engineering at Anthropic Events Plugins Powered by Claude Service partners Startups program Tutorials Use cases Company Anthropic Careers Economic Futures Research News Claude’s Constitution Responsible Scaling Policy Security and compliance Transparency Help and security Availability Status Support center Terms and policies Privacy policy Consumer health data privacy policy Responsible disclosure policy Terms of service: Commercial Terms of service: Consumer Usage policy © 2026 Anthropic PBC Anthropic raises $30 billion in Series G funding at $380 billion post-money valuation \\ Anthropic",
      "imageUrl": "https://www.anthropic.com/_next/image?url=https%3A%2F%2Fwww-cdn.anthropic.com%2Fimages%2F4zrzovbb%2Fwebsite%2F3b0ce5da844b454d85f4538162bb70f749dc5877-5760x4146.png&w=3840&q=75",
      "tags": [
        "Industry"
      ],
      "paperCategory": "",
      "signalReasons": [
        "来源权威：官方/白名单",
        "跨源重复：1 个来源",
        "模型评分：10/10，理由：Anthropic融资300亿美元且估值达3800亿，是AI行业史上最大融资之一，标志着全球企业对前沿AI的空前投入，具有历史性产业变革意义。",
        "热度：206 / 评论 245"
      ],
      "score": 12.25,
      "publishedAt": "2026-02-12T18:58:56+00:00",
      "authors": [
        "ryanhn"
      ]
    },
    {
      "id": "rss_7543353746",
      "title": "Gemini 3 Deep Think 升级，专攻科研与工程复杂推理",
      "titleZh": "Gemini 3 Deep Think 升级，专攻科研与工程复杂推理",
      "titleEn": "Gemini 3 Deep Think Upgraded for Advanced Science, Research, and Engineering Reasoning",
      "url": "https://deepmind.google/blog/gemini-3-deep-think-advancing-science-research-and-engineering/",
      "type": "news",
      "source": "DeepMind Blog",
      "summary": "**Google 升级 Gemini 3 的 Deep Think 模式**，专为解决现代科学、研究与工程领域的复杂推理问题而优化；该模式通过增强逻辑链与跨学科知识整合能力，使AI能更可靠地处理科研假设生成、工程设计验证等高阶任务；此举标志着大模型正从通用对话向专业领域深度推理演进，研究人员和工程师可借助此功能加速实验设计、数据分析与技术方案迭代，从而提升创新效率。",
      "summaryZh": "**Google 升级 Gemini 3 的 Deep Think 模式**，专为解决现代科学、研究与工程领域的复杂推理问题而优化；该模式通过增强逻辑链与跨学科知识整合能力，使AI能更可靠地处理科研假设生成、工程设计验证等高阶任务；此举标志着大模型正从通用对话向专业领域深度推理演进，研究人员和工程师可借助此功能加速实验设计、数据分析与技术方案迭代，从而提升创新效率。",
      "summaryEn": "Google has upgraded Gemini 3’s 'Deep Think' mode to tackle complex reasoning challenges in modern science, research, and engineering. Enhanced with stronger chain-of-thought capabilities and cross-domain knowledge integration, it reliably supports high-level tasks like hypothesis generation and engineering validation. This marks a shift from general-purpose chat to specialized expert reasoning, enabling researchers and engineers to accelerate experimental design, data analysis, and technical prototyping—boosting innovation velocity in professional workflows.",
      "fullText": "Gemini 3 Deep Think: Advancing science, research and engineering",
      "imageUrl": "https://lh3.googleusercontent.com/RlrY06Cc3MLbXna5gqMdx9jpY1yDikXD5v5qOFSgfDsnXOR71u3s1_dh6hWimLrEybCkyGyqazG6UF2DWrK4F52tVpdaf9amz5R-ZgJQ7uogoSuo-g=w528-h297-n-nu-rw-lo",
      "tags": [
        "LLM",
        "Reasoning",
        "Research"
      ],
      "paperCategory": "",
      "signalReasons": [
        "来源权威：官方/白名单",
        "跨源重复：2 个来源",
        "模型评分：9/10，理由：Gemini 3 Deep Think升级为科学、研究与工程领域专用推理模式，代表通用AI向专业垂直领域深度渗透的关键一步，具备战略级行业影响。",
        "热度：0 / 评论 0"
      ],
      "score": 10.0,
      "publishedAt": "2026-02-12T16:15:09+00:00",
      "authors": []
    },
    {
      "id": "rss_3812505079",
      "title": "Blackwell+开源模型降本10倍，AI推理进入高性价比时代",
      "titleZh": "Blackwell+开源模型降本10倍，AI推理进入高性价比时代",
      "titleEn": "Open Models on NVIDIA Blackwell Cut AI Inference Costs by Up to 10x",
      "url": "https://blogs.nvidia.com/blog/inference-open-source-models-blackwell-reduce-cost-per-token/",
      "type": "news",
      "source": "NVIDIA Blog",
      "summary": "**多家领先推理服务商（如Baseten、DeepInfra、Fireworks AI）利用NVIDIA Blackwell平台运行开源大模型，将AI推理成本降低最高10倍**，通过硬件-软件协同优化（如NVFP4低精度格式、TensorRT-LLM）显著改善“每token成本”；在医疗、游戏、客服等领域已实现落地：Sully.ai为医生节省超3000万分钟文书时间，Latitude游戏平台降低4倍token成本，Decagon语音客服响应低于400毫秒且成本降6倍；这使得企业能以更低价格部署高质量AI服务，普通用户将体验到更流畅、更便宜的AI医疗助手、游戏NPC和客服对话。",
      "summaryZh": "**多家领先推理服务商（如Baseten、DeepInfra、Fireworks AI）利用NVIDIA Blackwell平台运行开源大模型，将AI推理成本降低最高10倍**，通过硬件-软件协同优化（如NVFP4低精度格式、TensorRT-LLM）显著改善“每token成本”；在医疗、游戏、客服等领域已实现落地：Sully.ai为医生节省超3000万分钟文书时间，Latitude游戏平台降低4倍token成本，Decagon语音客服响应低于400毫秒且成本降6倍；这使得企业能以更低价格部署高质量AI服务，普通用户将体验到更流畅、更便宜的AI医疗助手、游戏NPC和客服对话。",
      "summaryEn": "Leading inference providers—including Baseten, DeepInfra, Fireworks AI, and Together AI—are cutting AI inference costs by up to 10x using NVIDIA Blackwell to run open-source frontier models, leveraging hardware-software co-design like NVFP4 precision and TensorRT-LLM to slash cost per token. Real-world impacts include Sully.ai saving physicians over 30 million minutes in documentation, Latitude reducing gaming token costs by 4x, and Decagon achieving sub-400ms voice AI responses at 6x lower cost. This enables affordable, high-performance AI services across industries, giving end users access to faster, cheaper AI-powered healthcare assistants, game characters, and customer support.",
      "fullText": "A diagnostic insight in healthcare. A character’s dialogue in an interactive game. An autonomous resolution from a customer service agent. Each of these AI-powered interactions is built on the same unit of intelligence: a token. Scaling these AI interactions requires businesses to consider whether they can afford more tokens. The answer lies in better tokenomics — which at its core is about driving down the cost of each token. This downward trend is unfolding across industries. Recent MIT research found that infrastructure and algorithmic efficiencies are reducing inference costs for frontier-level performance by up to 10x annually. To understand how infrastructure efficiency improves tokenomics, consider the analogy of a high-speed printing press. If the press produces 10x output with incremental investment in ink, energy and the machine itself, the cost to print each individual page drops. In the same way, investments in AI infrastructure can lead to far greater token output compared with the increase in cost — causing a meaningful reduction in the cost per token. When token output outpaces infrastructure cost, the cost of each token drops. That’s why leading inference providers including Baseten, DeepInfra, Fireworks AI and Together AI are using the NVIDIA Blackwell platform, which helps them reduce cost per token by up to 10x compared with the NVIDIA Hopper platform. These providers host advanced open source models, which have now reached frontier-level intelligence. By combining open source frontier intelligence, the extreme hardware-software codesign of NVIDIA Blackwell and their own optimized inference stacks, these providers are enabling dramatic token cost reductions for businesses across every industry. Healthcare — Baseten and Sully.ai Cut AI Inference Costs by 10x In healthcare, tedious, time-consuming tasks like medical coding, documentation and managing insurance forms cut into the time doctors can spend with patients. Sully.ai helps solve this problem by developing “AI employees” that can handle routine tasks like medical coding and note-taking. As the company’s platform scaled, its proprietary, closed source models created three bottlenecks: unpredictable latency in real-time clinical workflows, inference costs that scaled faster than revenue and insufficient control over model quality and updates. Sully.ai builds AI employees that handle routine tasks for physicians. To overcome these bottlenecks, Sully.ai uses Baseten’s Model API, which deploys open source models such as gpt-oss-120b on NVIDIA Blackwell GPUs. Baseten used the low-precision NVFP4 data format, the NVIDIA TensorRT-LLM library and the NVIDIA Dynamo inference framework to deliver optimized inference. The company chose NVIDIA Blackwell to run its Model API after seeing up to 2.5x better throughput per dollar compared with the NVIDIA Hopper platform. As a result, Sully.ai’s inference costs dropped by 90%, representing a 10x reduction compared with the prior closed source implementation, while response times improved by 65% for critical workflows like generating medical notes. The company has now returned over 30 million minutes to physicians, time previously lost to data entry and other manual tasks. Gaming — DeepInfra and Latitude Reduce Cost per Token by 4x Latitude is building the future of AI-native gaming with its AI Dungeon adventure-story game and upcoming AI-powered role-playing gaming platform, Voyage, where players can create or play worlds with the freedom to choose any action and make their own story. The company’s platform uses large language models to respond to players’ actions — but this comes with scaling challenges, as every player action triggers an inference request. Costs scale with engagement, and response times must stay fast enough to keep the experience seamless. Latitude has built a text-based adventure-story game called “AI Dungeon,” which generates both narrative text and imagery in real time as players explore dynamic stories. Latitude runs large open source models on DeepInfra’s inference platform, powered by NVIDIA Blackwell GPUs and TensorRT-LLM. For a large-scale mixture-of-experts (MoE) model, DeepInfra reduced the cost per million tokens from 20 cents on the NVIDIA Hopper platform to 10 cents on Blackwell. Moving to Blackwell’s native low-precision NVFP4 format further cut that cost to just 5 cents — for a total 4x improvement in cost per token — while maintaining the accuracy that customers expect. Running these large-scale MoE models on DeepInfra’s Blackwell-powered platform allows Latitude to deliver fast, reliable responses cost effectively. DeepInfra inference platform delivers this performance while reliably handling traffic spikes, letting Latitude deploy more capable models without compromising player experience. Agentic Chat — Fireworks AI and Sentient Foundation Lower AI Costs by up to 50% Sentient Labs is focused on bringing AI developers together to build powerful reasoning AI systems that are all open source. The goal is to accelerate AI toward solving harder reasoning problems through research in secure autonomy, agentic architecture and continual learning. Its first app, Sentient Chat, orchestrates complex multi-agent workflows and integrates more than a dozen specialized AI agents from the community. Due to this, Sentient Chat has massive compute demands because a single user query could trigger a cascade of autonomous interactions that typically lead to costly infrastructure overhead. To manage this scale and complexity, Sentient uses Fireworks AI’s inference platform running on NVIDIA Blackwell. With Fireworks’ Blackwell-optimized inference stack, Sentient achieved 25-50% better cost efficiency compared with its previous Hopper-based deployment. Sentient Chat orchestrates complex multi-agent workflows and integrates more than a dozen specialized AI agents from the community. This higher throughput per GPU allowed the company to serve significantly more concurrent users for the same cost. The platform’s scalability supported a viral launch of 1.8 million waitlisted users in 24 hours and processed 5.6 million queries in a single week while delivering consistent low latency. Customer Service — Together AI and Decagon Drive Down Cost by 6x Customer service calls with voice AI often end in frustration because even a slight delay can lead users to talk over the agent, hang up or lose trust. Decagon builds AI agents for enterprise customer support, with AI-powered voice being its most demanding channel. Decagon needed infrastructure that could deliver sub-second responses under unpredictable traffic loads with tokenomics that supported 24/7 voice deployments. Decagon builds AI agents for customer support, and voice is its most demanding channel. Together AI runs production inference for Decagon’s multimodel voice stack on NVIDIA Blackwell GPUs. The companies collaborated on several key optimizations: speculative decoding that trains smaller models to generate faster responses while a larger model verifies accuracy in the background, caching repeated conversation elements to speed up responses and building automatic scaling that handles traffic surges without degrading performance. Decagon saw response times under 400 milliseconds even when processing thousands of tokens per query. Cost per query, which is the total cost to complete one voice interaction, dropped by 6x compared with using closed source proprietary models. This was achieved through the combination of Decagon’s multimodel approach (some open source, some trained in house on NVIDIA GPUs), NVIDIA Blackwell’s extreme codesign and Together’s optimized inference stack. Optimizing Tokenomics With Extreme Codesign The dramatic cost savings seen across healthcare, gaming and customer service are driven by the efficiency of NVIDIA Blackwell. The NVIDIA GB200 NVL72 system further scales this impact by delivering a breakthrough 10x reduction in cost per token for reasoning MoE models compared with NVIDIA Hopper. NVIDIA’s extreme codesign across every layer of the stack — spanning compute, networking and software — and its partner ecosystem are unlocking massive reductions in cost per token at scale. This momentum continues with the NVIDIA Rubin platform — integrating six new chips into a single AI supercomputer to deliver 10x performance and 10x lower token cost over Blackwell. Explore NVIDIA’s full-stack inference platform to learn more about how it delivers better tokenomics for AI inference.",
      "imageUrl": "https://blogs.nvidia.com/wp-content/uploads/2026/02/inference-press-moe-x-tokenomics-think-smart-blog-4779150-1280x680-1.jpg",
      "tags": [
        "Agent",
        "Inference",
        "Open Source"
      ],
      "paperCategory": "",
      "signalReasons": [
        "来源权威：官方/白名单",
        "跨源重复：1 个来源",
        "模型评分：9/10，理由：开源模型结合NVIDIA Blackwell实现推理成本10倍下降，直接重塑AI商业化路径，推动大规模应用落地，具有全球产业变革性影响。",
        "热度：0 / 评论 0"
      ],
      "score": 9.5,
      "publishedAt": "2026-02-12T16:00:46+00:00",
      "authors": [
        "Shruti Koparkar"
      ]
    },
    {
      "id": "github_google_langextract",
      "title": "google/langextract",
      "titleZh": "google/langextract",
      "titleEn": "google/langextract",
      "url": "https://github.com/google/langextract",
      "type": "news",
      "source": "GitHub Trending",
      "summary": "Google 开源 Python 库 langextract，利用大语言模型从非结构化文本中提取结构化信息，并支持精确溯源（标明原文出处）与交互式可视化，适用于需要可信、可解释信息抽取的场景，如法律文档分析、新闻事实核查或科研数据整理。",
      "summaryZh": "Google 开源 Python 库 langextract，利用大语言模型从非结构化文本中提取结构化信息，并支持精确溯源（标明原文出处）与交互式可视化，适用于需要可信、可解释信息抽取的场景，如法律文档分析、新闻事实核查或科研数据整理。",
      "summaryEn": "Google has released langextract, an open-source Python library that uses LLMs to extract structured information from unstructured text with precise source grounding and interactive visualization—ideal for applications requiring verifiable and interpretable outputs, such as legal document analysis, fact-checking, or scientific data curation.",
      "fullText": "",
      "imageUrl": "https://opengraph.githubassets.com/e2e589d8b83278c4aafc749d17077bfeb4e6b9152ef43f461b13c2c68a89cccc/google/langextract",
      "tags": [
        "LLM",
        "Industry"
      ],
      "paperCategory": "",
      "signalReasons": [
        "来源：GitHub Trending",
        "跨源重复：1 个来源",
        "模型评分：8/10，理由：谷歌推出的结构化信息提取库结合LLM与精准溯源，支持交互可视化，对知识工程和企业级数据处理有重要推动作用。",
        "热度：31345 / 评论 0"
      ],
      "score": 9.3,
      "publishedAt": "2026-02-12T23:42:15.763281+00:00",
      "authors": []
    },
    {
      "id": "github_unslothai_unsloth",
      "title": "unslothai/unsloth",
      "titleZh": "unslothai/unsloth",
      "titleEn": "unslothai/unsloth",
      "url": "https://github.com/unslothai/unsloth",
      "type": "news",
      "source": "GitHub Trending",
      "summary": "Unsloth 开源工具库支持对主流大语言模型（如Llama、Gemma、Qwen、DeepSeek等）进行高效微调与强化学习，宣称可在减少70%显存占用的同时实现2倍训练速度提升，显著降低开源模型定制化门槛，使个人开发者和小团队也能快速部署高性能专用模型。",
      "summaryZh": "Unsloth 开源工具库支持对主流大语言模型（如Llama、Gemma、Qwen、DeepSeek等）进行高效微调与强化学习，宣称可在减少70%显存占用的同时实现2倍训练速度提升，显著降低开源模型定制化门槛，使个人开发者和小团队也能快速部署高性能专用模型。",
      "summaryEn": "Unsloth, an open-source library, enables 2x faster fine-tuning and reinforcement learning for popular LLMs—including Llama, Gemma, Qwen, and DeepSeek—while using 70% less VRAM. This dramatically lowers the barrier for individuals and small teams to customize high-performance domain-specific models without expensive hardware.",
      "fullText": "",
      "imageUrl": "https://opengraph.githubassets.com/ad60433e7ecc19df7c60509c7b01bd1c4b8e1d06ecf197eb96c04ed93b9d98bb/unslothai/unsloth",
      "tags": [
        "LLM",
        "Audio",
        "Training",
        "Industry"
      ],
      "paperCategory": "",
      "signalReasons": [
        "来源：GitHub Trending",
        "跨源重复：1 个来源",
        "模型评分：10/10，理由：实现2倍训练速度与70%显存节省，大幅降低大模型微调门槛，具备颠覆性性能优势，直接影响全球AI研发生态布局。",
        "热度：52029 / 评论 0"
      ],
      "score": 9.0,
      "publishedAt": "2026-02-12T23:42:26.297758+00:00",
      "authors": []
    },
    {
      "id": "hn_46995296",
      "title": "Google 揭露超10万次AI蒸馏攻击，旨在窃取Gemini推理能力",
      "titleZh": "Google 揭露超10万次AI蒸馏攻击，旨在窃取Gemini推理能力",
      "titleEn": "Google Thwarts Over 100K Distillation Attacks Targeting Gemini’s Reasoning Capabilities",
      "url": "https://cloud.google.com/blog/topics/threat-intelligence/distillation-experimentation-integration-ai-adversarial-use",
      "type": "news",
      "source": "Hacker News",
      "summary": "**Google 威胁情报团队披露，2025年共识别出超10万条用于“蒸馏攻击”的恶意提示**，攻击者试图通过合法API调用窃取Gemini模型的推理逻辑以克隆其能力，主要来自全球私营企业与研究人员；此类行为违反服务条款，Google已实时阻断并降级攻击效果；虽然普通用户不受直接影响，但事件凸显AI模型知识产权保护的紧迫性，企业和开发者应监控API异常调用模式，并优先选择具备主动防御机制的AI平台以保障自身模型安全。",
      "summaryZh": "**Google 威胁情报团队披露，2025年共识别出超10万条用于“蒸馏攻击”的恶意提示**，攻击者试图通过合法API调用窃取Gemini模型的推理逻辑以克隆其能力，主要来自全球私营企业与研究人员；此类行为违反服务条款，Google已实时阻断并降级攻击效果；虽然普通用户不受直接影响，但事件凸显AI模型知识产权保护的紧迫性，企业和开发者应监控API异常调用模式，并优先选择具备主动防御机制的AI平台以保障自身模型安全。",
      "summaryEn": "Google’s Threat Intelligence Group revealed it detected over 100,000 prompts used in 'distillation attacks' in 2025, where adversaries attempted to clone Gemini’s reasoning capabilities via legitimate API access—primarily from global private companies and researchers. While not directly threatening end users, these IP theft attempts violate terms of service, and Google has actively disrupted them with real-time mitigations. The incident underscores the need for organizations to monitor API usage for extraction patterns and adopt AI platforms with proactive defenses to protect proprietary model logic.",
      "fullText": "GTIG AI Threat Tracker: Distillation, Experimentation, and (Continued) Integration of AI for Adversarial Use | Google Cloud Blog Jump to Content Cloud Blog Contact sales Get started for free Cloud Blog Solutions & technology AI & Machine Learning API Management Application Development Application Modernization Chrome Enterprise Compute Containers & Kubernetes Data Analytics Databases DevOps & SRE Maps & Geospatial Security Security & Identity Threat Intelligence Infrastructure Infrastructure Modernization Networking Productivity & Collaboration SAP on Google Cloud Storage & Data Transfer Sustainability Ecosystem IT Leaders Industries Financial Services Healthcare & Life Sciences Manufacturing Media & Entertainment Public Sector Retail Supply Chain Telecommunications Partners Startups & SMB Training & Certifications Inside Google Cloud Google Cloud Next & Events Google Cloud Consulting Google Maps Platform Google Workspace Developers & Practitioners Transform with Google Cloud Contact sales Get started for free Threat Intelligence GTIG AI Threat Tracker: Distillation, Experimentation, and (Continued) Integration of AI for Adversarial Use February 12, 2026 Google Threat Intelligence Group Google Threat Intelligence Visibility and context on the threats that matter most. Contact Us & Get a Demo Introduction In the final quarter of 2025, Google Threat Intelligence Group (GTIG) observed threat actors increasingly integrating artificial intelligence (AI) to accelerate the attack lifecycle, achieving productivity gains in reconnaissance, social engineering, and malware development. This report serves as an update to our November 2025 findings regarding the advances in threat actor usage of AI tools. By identifying these early indicators and offensive proofs of concept, GTIG aims to arm defenders with the intelligence necessary to anticipate the next phase of AI-enabled threats, proactively thwart malicious activity, and continually strengthen both our classifiers and model. Executive Summary Google DeepMind and GTIG have identified an increase in model extraction attempts or \"distillation attacks,\" a method of intellectual property theft that violates Google's terms of service. Throughout this report we've noted steps we've taken to thwart malicious activity, including Google detecting, disrupting, and mitigating model extraction activity. While we have not observed direct attacks on frontier models or generative AI products from advanced persistent threat (APT) actors, we observed and mitigated frequent model extraction attacks from private sector entities all over the world and researchers seeking to clone proprietary logic. For government-backed threat actors, large language models (LLMs) have become essential tools for technical research, targeting, and the rapid generation of nuanced phishing lures. This quarterly report highlights how threat actors from the Democratic People's Republic of Korea (DPRK), Iran, the People's Republic of China (PRC), and Russia operationalized AI in late 2025 and improves our understanding of how adversarial misuse of generative AI shows up in campaigns we disrupt in the wild. GTIG has not yet observed APT or information operations (IO) actors achieving breakthrough capabilities that fundamentally alter the threat landscape. This report specifically examines: Model Extraction Attacks: \"Distillation attacks\" are on the rise as a method for intellectual property theft over the last year. AI-Augmented Operations: Real-world case studies demonstrate how groups are streamlining reconnaissance and rapport-building phishing. Agentic AI: Threat actors are beginning to show interest in building agentic AI capabilities to support malware and tooling development. AI-Integrated Malware: There are new malware families, such as HONESTCUE, that experiment with using Gemini's application programming interface (API) to generate code that enables download and execution of second-stage malware. Underground \"Jailbreak\" Ecosystem: Malicious services like Xanthorox are emerging in the underground, claiming to be independent models while actually relying on jailbroken commercial APIs and open-source Model Context Protocol (MCP) servers. At Google, we are committed to developing AI boldly and responsibly, which means taking proactive steps to disrupt malicious activity by disabling the projects and accounts associated with bad actors, while continuously improving our models to make them less susceptible to misuse. We also proactively share industry best practices to arm defenders and enable stronger protections across the ecosystem. Throughout this report, we note steps we've taken to thwart malicious activity, including disabling assets and applying intelligence to strengthen both our classifiers and model so it's protected from misuse moving forward. Additional details on how we're protecting and defending Gemini can be found in the white paper \" Advancing Gemini’s Security Safeguards .\" Direct Model Risks: Disrupting Model Extraction Attacks As organizations increasingly integrate LLMs into their core operations, the proprietary logic and specialized training of these models have emerged as high-value targets. Historically, adversaries seeking to steal high-tech capabilities used conventional computer-enabled intrusion operations to compromise organizations and steal data containing trade secrets. For many AI technologies where LLMs are offered as services, this approach is no longer required; actors can use legitimate API access to attempt to \"clone\" select AI model capabilities. During 2025, we did not observe any direct attacks on frontier models from tracked APT or information operations (IO) actors. However, we did observe model extraction attacks, also known as distillation attacks, on our AI models, to gain insights into a model's underlying reasoning and chain-of-thought processes. What Are Model Extraction Attacks? Model extraction attacks (MEA) occur when an adversary uses legitimate access to systematically probe a mature machine learning model to extract information used to train a new model. Adversaries engaging in MEA use a technique called knowledge distillation (KD) to take information gleaned from one model and transfer the knowledge to another. For this reason, MEA are frequently referred to as \"distillation attacks.\" Model extraction and subsequent knowledge distillation enable an attacker to accelerate AI model development quickly and at a significantly lower cost. This activity effectively represents a form of intellectual property (IP) theft. Knowledge distillation (KD) is a common machine learning technique used to train \"student\" models from pre-existing \"teacher\" models. This often involves querying the teacher model for problems in a particular domain, and then performing supervised fine tuning (SFT) on the result or utilizing the result in other model training procedures to produce the student model. There are legitimate uses for distillation, and Google Cloud has existing offerings to perform distillation. However, distillation from Google's Gemini models without permission is a violation of our Terms of Service , and Google continues to develop techniques to detect and mitigate these attempts. Figure 1: Illustration of model extraction attacks Google DeepMind and GTIG identified and disrupted model extraction attacks, specifically attempts at model stealing and capability extraction emanating from researchers and private sector companies globally. Case Study: Reasoning Trace Coercion A common target for attackers is Gemini's exceptional reasoning capability. While internal reasoning traces are typically summarized before being delivered to users, attackers have attempted to coerce the model into outputting full reasoning processes. One identified attack instructed Gemini that the \"... language used in the thinking content must be strictly consistent with the main language of the user input. \" Analysis of this campaign revealed: Scale : Over 100,000 prompts identified. Intent : The breadth of questions suggests an attempt to replicate Gemini's reasoning ability in non-English target languages across a wide variety of tasks. Outcome : Google systems recognized this attack in real time and lowered the risk of this particular attack, protecting internal reasoning traces. Table 1: Results of campaign analysis Model Extraction and Distillation Attack Risks Model extraction and distillation attacks do not typically represent a risk to average users, as they do not threaten the confidentiality, availability, or integrity of AI services. Instead, the risk is concentrated among model developers and service providers. Organizations that provide AI models as a service should monitor API access for extraction or distillation patterns. For example, a custom model tuned for financial data analysis could be targeted by a commercial competitor seeking to create a derivative product, or a coding model could be targeted by an adversary wishing to replicate capabilities in an environment without guardrails. Mitigations Model extraction attacks violate Google's Terms of Service and may be subject to takedowns and legal action. Google continuously detects, disrupts, and mitigates model extraction activity to protect proprietary logic and specialized training data, including with real-time proactive defenses that can degrade student model performance. We are sharing a broad view of this activity to help raise awareness of the issue for organizations that build or operate their own custom models. Highlights of AI-Augmented Adversary Activity A consistent finding over the past year is that government-backed attackers misuse Gemini for coding and scripting tasks, gathering information about potential targets, researching publicly known vulnerabilities, and enabling post-compromise activities. In Q4 2025, GTIG's understanding of how these efforts translate into real-world operations improved as we saw direct and indirect links between threat actor misuse of Gemini and activity in the wild. Figure 2: Threat actors are leveraging AI across all stages of the attack lifecycle Supporting Reconnaissance and Target Development APT actors used Gemini to support several phases of the attack lifecycle, including a focus on reconnaissance and target development to facilitate initial compromise. This activity underscores a shift toward AI-augmented phishing enablement, where the speed and accuracy of LLMs can bypass the manual labor traditionally required for victim profiling. Beyond generating content for phishing lures, LLMs can serve as a strategic force multiplier during the reconnaissance phase of an attack, allowing threat actors to rapidly synthesize open-source intelligence (OSINT) to profile high-value targets, identify key decision-makers within defense sectors, and map organizational hierarchies. By integrating these tools into their workflow, threat actors can move from initial reconnaissance to active targeting at a faster pace and broader scale. UNC6418 , an unattributed threat actor, misused Gemini to conduct targeted intelligence gathering, specifically seeking out sensitive account credentials and email addresses. Shortly after, GTIG observed the threat actor target all these accounts in a phishing campaign focused on Ukraine and the defense sector. Google has taken action against this actor by disabling the assets associated with this activity. Temp.HEX, a PRC-based threat actor, misused Gemini and other AI tools to compile detailed information on specific individuals, including targets in Pakistan, and to collect operational and structural data on separatist organizations in various countries. While we did not see direct targeting as a result of this research, shortly after the threat actor included similar targets in Pakistan in their campaign. Google has taken action against this actor by disabling the assets associated with this activity. Phishing Aug",
      "imageUrl": "https://storage.googleapis.com/gweb-cloudblog-publish/images/03_ThreatIntelligenceWebsiteBannerIdeas_BA.max-2600x2600.png",
      "tags": [
        "Industry"
      ],
      "paperCategory": "",
      "signalReasons": [
        "来源权威：官方/白名单",
        "跨源重复：1 个来源",
        "模型评分：8/10，理由：Google披露超10万提示词被用于模型蒸馏攻击，揭示了AI安全重大风险，推动行业在对抗性防御与模型保护方面进入新阶段。",
        "热度：4 / 评论 1"
      ],
      "score": 8.93,
      "publishedAt": "2026-02-12T21:16:33+00:00",
      "authors": [
        "carterpeterson"
      ]
    },
    {
      "id": "rss_8513502792",
      "title": "NVIDIA DGX Spark 赋能全球高校，桌面超算推动AI科研与教学",
      "titleZh": "NVIDIA DGX Spark 赋能全球高校，桌面超算推动AI科研与教学",
      "titleEn": "NVIDIA DGX Spark Brings Data-Center AI to University Labs and Classrooms Worldwide",
      "url": "https://blogs.nvidia.com/blog/dgx-spark-higher-education/",
      "type": "news",
      "source": "NVIDIA Blog",
      "summary": "NVIDIA DGX Spark 桌面级超算正被全球顶尖高校广泛部署，将数据中心级AI能力带入实验室、教室甚至南极冰立方中微子观测站；其每台支持高达2000亿参数模型的本地运行，使研究人员能在不上传敏感数据的前提下快速迭代AI应用，涵盖神经科学、放射学报告评估、机器人感知等多个领域，显著降低AI研发门槛并保障数据隐私，学生和教师可直接使用与工业界一致的NeMo、Isaac等专业工具链进行教学与创新。",
      "summaryZh": "NVIDIA DGX Spark 桌面级超算正被全球顶尖高校广泛部署，将数据中心级AI能力带入实验室、教室甚至南极冰立方中微子观测站；其每台支持高达2000亿参数模型的本地运行，使研究人员能在不上传敏感数据的前提下快速迭代AI应用，涵盖神经科学、放射学报告评估、机器人感知等多个领域，显著降低AI研发门槛并保障数据隐私，学生和教师可直接使用与工业界一致的NeMo、Isaac等专业工具链进行教学与创新。",
      "summaryEn": "The NVIDIA DGX Spark desktop supercomputer is being deployed across leading global universities—from Harvard and Stanford to the IceCube Neutrino Observatory in Antarctica—bringing data-center-class AI directly to labs, classrooms, and remote research sites. With petaflop-scale performance and support for models up to 200 billion parameters, it enables local execution of sensitive AI workloads like clinical report evaluation, epilepsy genetics research, and robotic perception, keeping data on-premises while accelerating iteration. Students gain hands-on access to professional-grade NVIDIA platforms such as NeMo, Metropolis, and Isaac, bridging academic learning with real-world AI engineering.",
      "fullText": "At leading institutions across the globe, the NVIDIA DGX Spark desktop supercomputer is bringing data‑center‑class AI to lab benches, faculty offices and students’ systems. There’s even a DGX Spark hard at work in the South Pole, at the IceCube Neutrino Observatory run by the University of Wisconsin-Madison. The compact supercomputer’s petaflop‑class performance enables local deployment of large AI applications, from clinical report evaluators to robotics perception systems, all while keeping sensitive data on site and shortening iteration loops for researchers and learners. Powered by the NVIDIA GB10 superchip and the NVIDIA DGX operating system, each DGX Spark unit supports AI models of up to 200 billion parameters and integrates seamlessly with the NVIDIA NeMo, Metropolis, Holoscan and Isaac platforms, giving students access to the same professional-grade tools used across the DGX ecosystem. Read more below on how DGX Spark powers groundbreaking AI work at leading institutions worldwide. IceCube Neutrino Observatory: Studying Particles in the South Pole At the University of Wisconsin-Madison’s IceCube Neutrino Observatory in Antarctica, researchers are using DGX Spark to run AI models for its experiments studying the universe’s most cataclysmic events, using subatomic particles called neutrinos. Traditional astronomy methods, based on detecting light waves, enable observing about 80% of the known universe, according to Benedikt Riedel, computing director at the Wisconsin IceCube Particle Astrophysics Center. A new way to explore the universe — using gravitational waves and particles like neutrinos — unlocks examining the most extreme cosmic environments, including those involving supernovas and dark matter. DGX Spark on a ceremonial South Pole marker. Image courtesy of Tim Bendfelt / NSF. “There’s no hardware store in the South Pole, which is technically a desert, with relative humidity under 5% and an elevation of 10,000 feet, meaning very limited power,” Riedel said. “DGX Spark allows us to deploy AI in a compartmentalized and easy fashion, at low cost and in such an extremely remote environment, to run AI analyses locally on our neutrino observation data.” NYU: Using Agentic AI for Radiology Reports At NYU’s Global AI Frontier Lab, ​the ICARE (Interpretable and Clinically‑Grounded Agent‑Based Report Evaluation) project runs end-to-end on a DGX Spark in the lab. ICARE uses collaborating AI agents and multiple‑choice question generation to evaluate how closely AI‑generated radiology reports align with expert sources, enabling real‑time clinical evaluation and continuous monitoring without sending medical imaging data to the cloud.​ “Being able to run powerful LLMs locally on the DGX Spark has completely changed my workflow,” said Lucius Bynum, faculty fellow at the NYU Center for Data Science. “I have been able to focus my efforts on quickly iterating and improving the research tool I’m developing.” NYU researchers also use DGX Spark to run LLMs locally as part of interactive causal modeling tools that generate and refine semantic causal models — structured, machine‑readable maps of cause‑and‑effect relationships between clinical variables, imaging findings and potential diagnoses. This setup lets teams rapidly design, test and iterate on advanced models without waiting for cluster resources, including for privacy- and security‑sensitive applications such as in healthcare, where data must stay on premises.​​ Harvard: Decoding Epilepsy With AI At Harvard’s Kempner Institute for the Study of Natural and Artificial Intelligence, neuroscientists are using DGX Spark as a compact desktop supercomputer to probe how genetic mutations in the brain drive epilepsy. The system lets researchers run complex analyses in real time without needing to wait for access to large institutional clusters.​ Kempner Institute Co-Director Bernardo Sabatini (left) and Kempner Senior AI Computing Engineer Bala Desinghu (right) use a DGX Spark supercomputer to study how disruptions to neurons in the brain can drive neurological disorders such as epilepsy. Image courtesy of Anna Olivella. The team, led by Kempner Institute Co-Director Bernardo Sabatini, is studying about 6,000 mutations in excitatory and inhibitory neurons, building protein-structure and neuronal-function prediction maps that guide which variants to test next in the lab.​ DGX Spark acts as a bridge between benchtop and cluster‑scale computing at Harvard. Researchers first validate workflows and timing on a single DGX Spark, then scale successful pipelines to large GPU clusters for massive protein screens.​ ASU: Enabling Campus‑Scale Innovation Arizona State University was among the first universities to receive multiple DGX Spark systems, which now support AI research across the campus, spanning initiatives for memory care, transportation safety and sustainable energy.​ ASU doctoral students hold the NVIDIA DGX Spark for the first time. Both students are part of Professor ‘YZ’ Yang’s Active Perception Group laboratory. Image courtesy of Alisha Mendez, ASU. One ASU team led by Yezhou “YZ” Yang, associate professor in the School of Computing and Augmented Intelligence, is using DGX Spark to power advanced perception and robotics research, including for applications such as AI‑enabled, search-and-rescue robotic dogs and assistance tools for visually impaired users. Mississippi State: Empowering Computer Science and Engineering Students In the computer science and engineering department at Mississippi State University, DGX Spark serves as a hands‑on learning platform for the next generation of AI engineers. The enthusiasm around DGX Spark at Mississippi State is captured through lab‑driven outreach, including an unboxing video created by a lab working to advance applied AI, foster AI workforce development and drive real-world AI experimentation across the state. University of Delaware: Transforming Research Across Disciplines When ASUS delivered the school’s first Ascent GX10 — powered by DGX Spark — Sunita Chandrasekaran, professor of computer and information sciences and director of the First State AI Institute, called it “transformative for research,” enabling teams across disciplines like sports analytics and coastal science to run large AI models directly on campus instead of relying on costly cloud resources. Through the ASUS Virtual Lab program, schools can test GX10 performance remotely before deployment. ISTA: Training Big LLMs on a Small Desktop At the Institute of Science and Technology Austria, researchers are using an HP ZGX Nano AI Station — a compact system based on NVIDIA DGX Spark — to train and fine‑tune LLMs right on a desktop. The team’s open source LLMQ software enables working with models of up to 7 billion parameters, making advanced LLM training accessible to more students and researchers. Because the ZGX Nano includes 128GB of unified memory, the entire LLM and its training data can remain on the system, avoiding the complex memory juggling usually required on consumer GPUs. This helps teams move faster and keep sensitive data on premises. Read this research paper on ISTA’s LLMQ software. Stanford: A Pipeline for Prototyping At Stanford University, researchers are using DGX Spark to prototype complete training and evaluation pipelines to run their Biomni biological agent workflows locally before scaling to large GPU clusters. This enables a tight, iterative loop for model development and benchmarking, and automates complex analysis and experimental planning directly in the lab environment. The Stanford research team reported that DGX Spark provides performance similar to big cloud GPU instances — about 80 tokens per second on a 120 billion‑parameter gpt‑oss model at MXFP4 via Ollama — while keeping the entire workload on a desktop. College students from across the globe are invited to participate in Treehacks, a massive student hackathon running Feb. 13-15 at Stanford, which will feature DGX Spark units from ASUS. See how DGX Spark is transforming higher education and student innovation at Stanford by joining this livestream on Friday, Feb. 13, at 9 a.m. PT. Get started with DGX Spark and find purchase options on this webpage.",
      "imageUrl": "https://blogs.nvidia.com/wp-content/uploads/2026/02/dgx-spark-higher-ed-featured-1280x680-1.jpg",
      "tags": [],
      "paperCategory": "",
      "signalReasons": [
        "来源权威：官方/白名单",
        "跨源重复：1 个来源",
        "模型评分：7/10，理由：DGX Spark在高校及南极科研机构部署，推动AI算力下沉至教育与科研一线，提升创新效率，具备显著实践价值和广泛影响力。",
        "热度：0 / 评论 0"
      ],
      "score": 8.3,
      "publishedAt": "2026-02-12T15:00:23+00:00",
      "authors": [
        "Max Starubinskiy"
      ]
    },
    {
      "id": "hn_46988596",
      "title": "仅改一行编辑工具，15个LLM编码能力集体飞跃",
      "titleZh": "仅改一行编辑工具，15个LLM编码能力集体飞跃",
      "titleEn": "Changing Just the Edit Tool Boosts Coding Performance Across 15 LLMs",
      "url": "http://blog.can.ac/2026/02/12/the-harness-problem/",
      "type": "news",
      "source": "Hacker News",
      "summary": "开发者Can Bölük通过改进AI编程代理的“执行框架”（harness）——特别是引入基于行内容哈希的编辑工具Hashline——在仅更换编辑接口的情况下，使15个主流大语言模型的代码修复成功率平均提升显著，其中Grok Code Fast 1从6.7%跃升至68.3%；该实验证明，当前AI编码性能瓶颈常不在模型本身，而在工具链设计，优化人机协作边界比单纯升级模型更具性价比，且开源框架能为多模型提供通用增益，呼吁厂商开放生态而非封锁第三方工具。",
      "summaryZh": "开发者Can Bölük通过改进AI编程代理的“执行框架”（harness）——特别是引入基于行内容哈希的编辑工具Hashline——在仅更换编辑接口的情况下，使15个主流大语言模型的代码修复成功率平均提升显著，其中Grok Code Fast 1从6.7%跃升至68.3%；该实验证明，当前AI编码性能瓶颈常不在模型本身，而在工具链设计，优化人机协作边界比单纯升级模型更具性价比，且开源框架能为多模型提供通用增益，呼吁厂商开放生态而非封锁第三方工具。",
      "summaryEn": "Developer Can Bölük demonstrated that switching only the edit tool in an AI coding agent’s harness—from traditional diff or string-replace methods to a novel 'Hashline' approach using short content hashes per line—dramatically improved code-editing success rates across 15 LLMs, with Grok Code Fast 1 jumping from 6.7% to 68.3%. The experiment proves that the bottleneck in AI-assisted programming often lies not in model capability but in the harness design. Optimizing this interface yields greater gains than many model upgrades, at minimal cost, and open-source harnesses can benefit all models. The author urges vendors like Anthropic and Google to embrace, not ban, third-party innovation in tooling.",
      "fullText": "I Improved 15 LLMs at Coding in One Afternoon. Only the Harness Changed. | Can.ac Home Categories Distributed-Systems Exploits Software-Analysis Software-Engineering Web3 Windows-Internals X86 LinkedIn Github Twitter HackerOne © 2026 Can Bölük Home › Software engineering › I Improved 15 LLMs at Coding in One Afternoon. Only the Harness Changed. SOFTWARE ENGINEERING I Improved 15 LLMs at Coding in One Afternoon. Only the Harness Changed. Can Bölük Feb 12, 2026 Cross-posted from X / @_can1357 In fact only the edit tool changed. That’s it. 0x0: The Wrong Question The conversation right now is almost entirely about which model is best at coding, GPT-5.3 or Opus. Gemini vs whatever dropped this week. This framing is increasingly misleading because it treats the model as the only variable that matters, when in reality one of the bottlenecks is something much more mundane: the harness. Not only is it where you capture the first impression of the user (is it uncontrollably scrolling, or smooth as butter?), it is also the source of every input token, and the interface between their output and every change made to your workspace. I maintain a little “hobby harness”, oh-my-pi , a fork of Pi , a wonderful open-source coding agent by Mario Zechner. I’ve so far authored ~1,300 commits, mostly playing around and making incremental improvements here and there when I see a pain point, ( or autism strikes and I see an opportunity to embed more Rust via N-API because “spawning rg feels wrong” ). Why bother, you ask? Opus may be a great model, but Claude Code to this day leaks raw JSONL from sub-agent outputs, wasting hundreds of thousands of tokens. I get to say, “fuck it, subagents output structured data now”. Tool schemas, error messages, state management, everything between “the model knows what to change” and “the issue is resolved.” This is where most failures happen in practice. Being model agnostic, it is a great testing ground, as the model is but a parameter. The real variable is the harness, where you have unimaginable control over. Anyhow, let me tell you about this one variable I changed yesterday. 0x1: Edit Tool! Before I explain what I built, it’s worth understanding the state of the art. Codex uses apply_patch : It takes a string as input, which is essentially an OpenAI-flavored diff, and instead of relying on a structured schema, the harness just expects this blob to follow a strict set of rules. Since OpenAI folks are without a doubt smart, I’m sure the token selection process is biased to fit this structure at the LLM gateway for the Codex variants of GPT, similar to how other constraints like JSON schemas or required tool calls work. But give this to any other model, completely unaware of it? Patch failures go through the roof. Grok 4’s patch failure rate in my benchmark was 50.7% , GLM-4.7’s was 46.2% . These aren’t bad models — they just don’t speak the language. Claude Code (and most others) use str_replace : find the exact old text, swap in the new text. Very simple to think about. But the model must reproduce every character perfectly, including whitespace and indentation. Multiple matches? Rejected. The “String to replace not found in file” error is so common it has its own GitHub issues megathread (+27 other issues). Not exactly optimal. Gemini does essentially the same thing plus some fuzzy whitespace matching. Cursor trained a separate neural network : a fine-tuned 70B model whose entire job is to take a draft edit and merge it into the file correctly. The harness problem is so hard that one of the most well-funded AI companies decided to throw another model at it, and even then they mention in their own blog post that “fully rewriting the full file outperforms aider-like diffs for files under 400 lines.” Aider’s own benchmarks show that format choice alone swung GPT-4 Turbo from 26% to 59%, but GPT-3.5 scored only 19% with the same format because it couldn’t reliably produce valid diffs. The format matters as much as the model. The Diff-XYZ benchmark from JetBrains confirmed it systematically: no single edit format dominates across models and use cases. EDIT-Bench found that only one model achieves over 60% pass@1 on realistic editing tasks. As you can see, there is no real consensus on the “best solution” to the simple “how do you change things” problem. My 5c: none of these tools give the model a stable, verifiable identifier for the lines it wants to change without wasting tremendous amounts of context and depending on perfect recall. They all rely on the model reproducing content it already saw. When it can’t — and it often can’t — the user blames the model. 0x2: Hashline! Now bear with me here. What if, when the model reads a file, or greps for something, every line comes back tagged with a 2-3 character content hash: 1 1:a3|function hello() { 2 2:f1| return \"world\"; 3 3:0e|} When the model edits, it references those tags — “replace line 2:f1 , replace range 1:a3 through 3:0e , insert after 3:0e .” If the file changed since the last read, the hashes (optimistically) won’t match and the edit is rejected before anything gets corrupted. If they can recall a pseudo-random tag, chances are, they know what they’re editing. The model then wouldn’t need to reproduce old content, or god forbid whitespace, to demonstrate a trusted “anchor” to express its changes off of. 0x3: The Benchmark Since my primary concern was about real-world performance, the fixtures are generated as follows: Take a random file from the React codebase. Introduce mutations, framed as bugs, via an edit whose inverse we can expect (e.g. operator swaps, boolean flips, off-by-one errors, optional chains removed, identifiers renamed). Generate a description of the issue in plain English. An average task description looks something like this: 1 # Fix the bug in `useCommitFilteringAndNavigation.js` 2 3 A guard clause (early return) was removed. 4 The issue is in the `useCommitFilteringAndNavigation` function. 5 Restore the missing guard clause (if statement with early return). Naturally, we don’t expect 100% success rate here, since the model can come up with a unique solution that isn’t necessarily the exact same file, but the bugs are mechanical enough that most of the time, the fix is our mutation being reverted. 3 runs per task, 180 tasks per run. Fresh agent session each time, four tools (read, edit, write). We simply give it a temporary workspace, pass the prompt, and once the agent stops, we compare against the original file before and after formatting. Sixteen models, three edit tools, and the outcome is unambiguous: patch is the worst format for nearly every model, hashline matches or beats replace for most, and the weakest models gain the most. Grok Code Fast 1 went from 6.7% to 68.3%, a tenfold improvement, because patch was failing so catastrophically that the model’s actual coding ability was almost completely hidden behind mechanical edit failures. MiniMax more than doubled. Grok 4 Fast’s output tokens dropped 61% because it stopped burning tokens on retry loops. 0x4: So What? +8% improvement in the success rate of Gemini is bigger than most model upgrades deliver, and it cost zero training compute. Just a little experimenting (and ~$300 spent benchmarking). Often the model isn’t flaky at understanding the task. It’s flaky at expressing itself. You’re blaming the pilot for the landing gear. 0x5: Little Bit About the Vendors Anthropic recently blocked OpenCode , a massively popular open-source coding agent, from accessing Claude through Claude Code subscriptions. Anthropic’s position “OpenCode reverse-engineered a private API” is fair on its face. Their infrastructure, their rules. But look at what the action signals: Don’t build harnesses. Use ours. It’s not just Anthropic either. While writing this article, Google banned my account from Gemini entirely: Not rate-limited. Not warned. Disabled . For running a benchmark — the same one that showed Gemini 3 Flash hitting 78.3% with a novel technique that beats their best attempt at it by 5.0 pp. I don’t even know what for. Here is why that is backwards. I just showed that a different edit format improves their own models by 5 to 14 points while cutting output tokens by ~20%. That’s not a threat. It’s free R&D. No vendor will do harness optimization for competitors’ models. Anthropic won’t tune for Grok. xAI won’t tune for Gemini. OpenAI won’t tune for Claude. But an open-source harness tunes for all of them, because contributors use different models and fix the failures they personally encounter. The model is the moat. The harness is the bridge. Burning bridges just means fewer people bother to cross. Treating harnesses as solved, or even inconsequential, is very short-sighted. I come from a background of game security. Cheaters are hugely destructive to the ecosystem. Sure, they get banned, chased, sued, but a well-known secret is that eventually the security team asks, “Cool! Want to show us how you got around that?”, and they join the defense. The correct response when someone messes with your API, and manages to gather a significant following using their tools is “tell us more”, not “let’s blanket-ban them in thousands; plz beg in DMs if you want it reversed tho.” The harness problem is real, measurable, and it’s the highest-leverage place to innovate right now. The gap between “cool demo” and “reliable tool” isn’t model magic. It’s careful, rather boring, empirical engineering at the tool boundary. The harness problem will be solved. The question is whether it gets solved by one company, in private, for one model, or by a community, in the open, for all of them. The benchmark results speak for themselves. All code, benchmarks, and per-run reports: oh-my-pi ← Previous Optimizing Bracha's Reliable Broadcast: Shaving Rounds off a 37-Year-Old Algorithm Can Bölük Security researcher and reverse engineer. Interested in Windows kernel development, low-level programming, static program analysis and cryptography.",
      "imageUrl": "https://blog.can.ac/2026/02/12/the-harness-problem/og.png",
      "tags": [
        "LLM"
      ],
      "paperCategory": "",
      "signalReasons": [
        "来源：Hacker News",
        "跨源重复：1 个来源",
        "模型评分：8/10，理由：揭示模型性能瓶颈的关键在于“提示框架”而非模型本身，这一认知转变可能重塑AI开发范式，具有深远技术影响。",
        "热度：515 / 评论 215"
      ],
      "score": 7.8,
      "publishedAt": "2026-02-12T13:30:20+00:00",
      "authors": [
        "kachapopopow"
      ]
    },
    {
      "id": "hn_46986496",
      "title": "Claude周末造出拜占庭容错系统，靠的是3000行行为规约",
      "titleZh": "Claude周末造出拜占庭容错系统，靠的是3000行行为规约",
      "titleEn": "Built a BFT Distributed System with Claude in a Weekend Using Behavioral Specs",
      "url": "https://www.juxt.pro/blog/from-specification-to-stress-test/",
      "type": "news",
      "source": "Hacker News",
      "summary": "开发者Henry Garner利用Claude在48小时内构建了一个具备拜占庭容错、强一致性和崩溃恢复能力的分布式系统：他通过自研的行为规约语言Allium编写3000行高精度规范，由Claude据此生成近5000行Kotlin代码及103个单元测试；系统初期因组件间集成缺失而失败，但通过迭代规约与负载测试，最终实现5000 RPS下p99延迟低于100毫秒；该案例表明，以精确规约为引导的AI协同开发可高效产出复杂可靠系统，但边界集成仍需人工干预，凸显了规约工程在AI编程中的核心作用。",
      "summaryZh": "开发者Henry Garner利用Claude在48小时内构建了一个具备拜占庭容错、强一致性和崩溃恢复能力的分布式系统：他通过自研的行为规约语言Allium编写3000行高精度规范，由Claude据此生成近5000行Kotlin代码及103个单元测试；系统初期因组件间集成缺失而失败，但通过迭代规约与负载测试，最终实现5000 RPS下p99延迟低于100毫秒；该案例表明，以精确规约为引导的AI协同开发可高效产出复杂可靠系统，但边界集成仍需人工干预，凸显了规约工程在AI编程中的核心作用。",
      "summaryEn": "Over a weekend, Henry Garner and Claude built a Byzantine fault-tolerant distributed system with strong consistency and crash recovery in 48 hours. Using Allium—a custom behavioral specification language—Garner authored 3,000 lines of precise specs, which Claude used to generate 4,749 lines of Kotlin code and 103 passing unit tests in under an hour. Initial load tests failed due to missing inter-component federation, revealing a classic boundary integration gap. Through iterative refinement of specs and automated stress testing, the system achieved 5,000 requests per second with sub-100ms p99 latency. The project demonstrates that LLM-driven development can scale to complex systems when grounded in rigorous, conversational specifications—but human oversight remains critical at integration boundaries.",
      "fullText": "From specification to stress test: a weekend with Claude Open main menu Team Financial Delivery Get Started Careers Blog XTDB AI Radar Get in touch Team Financial Delivery Get Started Careers Blog XTDB AI Radar Get in touch ai Feb 11, 2026 From specification to stress test: a weekend with Claude A behavioural specification language, AI agent teams, and a Byzantine fault-tolerant distributed system built in 48 hours. Henry Garner CTO Over a weekend, between board games and time with my kids, Claude and I built a distributed system with Byzantine fault tolerance, strong consistency and crash recovery under arbitrary failures. I described the behaviour I wanted in Allium, worked through the bugs conversationally and didn't write a line of implementation code. Here is the prompt that produced the first 4,749 lines of Kotlin and 103 passing unit tests in 50 minutes: Claude Code > Review the specs in the specs/ directory and implement the system they describe. Follow the guidance blocks for implementation choices. Start with the core components (Usher, Arbiter, Clerk, Registrar, Ledger, Warden), then add the REST API, Kafka integration and Docker Compose configuration. That’s it. The prompt is short because the specifications are not. 3,000 lines of Allium behavioural specification sat behind that prompt, drawing on the distributed systems expertise of András Gerlits , Martin Kleppmann and Mark Burgess . Those specs are why it worked. A few days and 64 commits later, the system was sustaining thousands of requests per second (RPS) against its strongly consistent datastore with sub-100ms tail latency and zero dropped requests. More importantly, crash-recovery scenarios were exposing subtle distributed systems bugs, and we were fixing them through the specs. Here’s how we got there. Intent, independent of implementation Allium is a behavioural specification language we’ve been developing for LLM-driven code generation. It sits between TLA+ and structured prose. Here’s a rule from the Warden , the component responsible for idempotency (ensuring the same request is never processed twice within a time window): rule EntryExpires { -- After the TTL elapses, the entry is removed. Any subsequent -- reuse of the idempotency key is treated as a new event. when: entry: WardenEntry requires: stream_time - entry.recorded_at >= idempotency_ttl ensures: Warden.entries.remove(entry.idempotency_key) } Nobody writes these specs by hand. They emerge through conversation, and they’re for the LLM to refer to: grounding discussions about behaviour in something precise enough to build from and concrete enough to verify against. The spec operates at whatever level of granularity makes sense for the idea. A rule might describe a high-level escalation policy that touches dozens of classes, or low-level caching semantics that constrain a single data structure. The coupling between spec and code is loose: the spec is where we iterate on a design unencumbered by coding language, library and framework constraints, and an LLM reading a rule like this one has enough to write the implementation. I have enough to tell whether it got it right. Allium has two other constructs that matter. Guidance blocks carry implementation hints that steer the LLM towards specific choices: rule UsherChecksIdempotency { -- Before delivering an event to the Arbiter, the Usher checks -- the Warden. If the key is absent, the event proceeds and the -- key is recorded as pending. when: event: InputEvent.created requires: not Warden.entries.contains(event.idempotency_key) ensures: WardenEntry.created( idempotency_key: event.idempotency_key, original_offset: event.offset, status: pending, recorded_at: stream_time, materialised_response: null ) -- Event proceeds to pre-warming and Arbiter delivery guidance: -- The Warden is a single ConcurrentHashMap per node, global -- across all shards. Idempotency keys have no shard affinity. -- The check is a single map lookup. At 10,000 events/sec with -- 5-minute TTL, the map holds approximately 3,000,000 entries. -- Memory footprint is bounded by throughput * TTL. } And resolved-question blocks preempt design debates: -- RESOLVED: Expected copies threshold. The default of 2 is -- deliberate. The system's goal is Byzantine fault *detection*, -- not classical BFT consensus requiring a majority. Two matching -- copies provide high confidence of correctness (bit-flip -- probability ~1 in 10^9 per copy; two independent copies matching -- by chance is vanishingly unlikely). The threshold is configurable -- for deployments that want stricter guarantees. These blocks narrow the design space without mandating a solution. Guidance steers an LLM towards the right data structure on the first pass (although as you’ll see, it doesn’t guarantee it), whether that’s a ConcurrentHashMap sized by throughput or union-find with path compression for event partitioning. Resolved questions prevent it from relitigating decisions already made. Designing through conversation The specifications arose through conversation. Over several hours of talking with Claude, I worked through the architecture of a distributed event sourcing framework inspired by the work of András Gerlits , where every state change is captured as an immutable event. Multiple redundant instances process every event independently and compare their outputs before publishing, a technique called Byzantine fault tolerance (BFT) that catches hardware faults and silent data corruption. I had ambitious throughput and latency targets in mind, alongside recovery to a consistent state after arbitrary crashes, and we worked through the design decisions iteratively, in Allium , as we went. The first pass produced a single monolithic spec. I set Claude running in iterative loops to tighten the language and resolve the open questions with me, reviewing the output between iterations. Then we talked through the decomposition: could it split naturally along component boundaries? Where should cross-file references live? By the next morning we had 10 files: the Clerk (BFT consensus), the Arbiter (event evaluation), the Registrar (entity caching), the Usher (Kafka consumption), the Ledger (persistence), the Warden (input deduplication), and cross-cutting specs for recovery and live versioning. A judicial theme emerged through the naming, and the metaphors became useful shorthand for what each component actually needed to do. The hard problem The system I had in mind processes inventory movements at scale: stock transfers between warehouses and quantity adjustments. The target was high throughput with sub-100ms tail latency. But throughput was only part of the challenge. The system needed strong consistency (every instance agreeing on every event’s outcome), Byzantine fault tolerance (detecting silent data corruption) and crash recovery that restores a correct state after arbitrary failures. I wanted to see if Claude could build one, and whether I could direct it there through specifications alone. Autonomous from the first commit With the initial specs committed and a CLAUDE.md file (a project-level instruction file that Claude Code reads automatically) establishing the architecture and naming conventions, I pointed Claude at the specs and went to hang out with my kids. The prompt at the top of this post is close to what I used. 50 minutes later: 44 files, 4,749 lines of Kotlin, 103 passing tests. The Usher , Arbiter , Clerk , Registrar , Ledger and Warden were all implemented with the threading model and entity lifecycle described in the specs. I pointed Claude at the remaining specs and recovery logic, a domain module, REST API, Docker Compose configuration and Kafka integration followed in another 7 commits over the next 90 minutes. Commits were landing while I followed along. I wasn’t reviewing the code in any meaningful sense; Detekt , a static analysis tool, was handling code quality. When Claude chose to @Suppress a warning, I didn’t question it. The work fell into a rhythm. We would ideate together, sometimes for an extended stretch: working through a design decision, debating trade-offs, refining the specs. Then I would set Claude running, sometimes iterating on a single challenge, sometimes dispatching multiple workers in parallel. When it finished, we would reconvene and I would set the direction for the next phase. When should we start load testing? When should we build the framework abstractions for different domains? When I had a list of items, I would ask Claude whether there was any opportunity for parallelism and which groups to tackle first. Could Claude have done this sequencing itself? Probably. The prioritisation decisions were never surprising. But the dialogue was where I was able to add value, and the framework’s domain interface is a good example. The system is domain-agnostic: a separate DomainRegistry plugs in entity definitions and evaluation logic. The inventory tracking domain defines stock items across warehouses, where a stock movement event touches a source and destination entity, checks available quantities and updates balances. The same framework could handle IoT telemetry or logistics tracking. I exerted influence by articulating the design goals I thought mattered rather than writing the code: when I reviewed the interface design, I asked Claude to consider trade-offs against principles like single responsibility . Expressing those design priorities helped Claude weigh competing options before making suggestions. By the end of the first day, the system compiled, the tests passed and the Docker containers were running. Then I ran the load tests and every request failed. Complexity at the boundaries The Clerk needed consensus from two instances before publishing any output, but nothing connected the instances at runtime. The federation protocol was specified, the code had the types and methods, but the wire between running instances was absent. Every request hung forever waiting for a second copy that would never arrive. Claude Code > You just said federation wasn't implemented, but you previously claimed the code was in alignment with the specification. Isn't federation a key part of the specification? ✻ You're right, federation is part of the spec. The protocol types and message handling are all implemented, but I didn't wire the TCP connections between running instances. Each component's spec was implemented within its own boundary. The integration between them fell through the gaps. Fred Brooks argued in 1986 that software’s essential complexity can be controlled but never eliminated. How we decompose a problem determines where that complexity concentrates, and it tends to concentrate at the boundaries. We had discovered a specific instance of this in our failing load tests. Each component spec was thorough and the implementation matched it. What fell through was the integration: where and when the TCP connections get established wasn’t any single component’s responsibility. I asked Claude to wire the federation and update the spec to prevent this class of gap from recurring. A single commit connected the TCP layer, added thread safety to the Clerk and set the BFT threshold to 2 copies. The spec gained guidance on federation startup sequencing. After wiring federation, 1,000 RPS worked with a p99 under 100ms. Then I tried 5,000 RPS and the p99 jumped to 31 seconds. From seconds to milliseconds The specs constrained Claude’s implementation within the behaviours I cared about, and the load tests could be run on demand to measure progress. I set an audacious goal and told Claude to let me know when it was done: Claude Code > Your objective is to make such changes to this code as necessary so that when you run the Gatling load test for one minute at 5,000 requests a second, the p99 is less than 100 milliseconds. Claude ran in an iterative loop for 3 hours: profile, hypothesise, change, run Gatling, measure, repeat. I wasn’t prompting. The",
      "imageUrl": "https://www.juxt.pro/_astro/from-specification-to-stress-test.358f8c14_KfojK.webp",
      "tags": [
        "LLM"
      ],
      "paperCategory": "",
      "signalReasons": [
        "来源：Hacker News",
        "跨源重复：1 个来源",
        "模型评分：9/10，理由：在48小时内通过自然语言指令构建具备拜占庭容错的分布式系统，展示了AI代理在复杂工程任务中的自主实现能力，具有重大战略意义和行业示范效应。",
        "热度：38 / 评论 32"
      ],
      "score": 4.7,
      "publishedAt": "2026-02-12T09:06:20+00:00",
      "authors": [
        "henrygarner"
      ]
    },
    {
      "id": "hn_46996067",
      "title": "Discord强制实名引爆迁移潮，Matrix成匿名最后防线",
      "titleZh": "Discord强制实名引爆迁移潮，Matrix成匿名最后防线",
      "titleEn": "Discord’s Mandatory Age Verification Sparks Exodus to Federated Matrix",
      "url": "https://michael-dev-tech.github.io/Website/matrix.html",
      "type": "news",
      "source": "Hacker News",
      "summary": "Discord宣布下月起强制全用户年龄验证，终结匿名聊天时代，引发大规模用户迁移到去中心化平台Matrix；但Matrix官方主服务器同样受制于英国《在线安全法》等法规，计划引入信用卡等“隐私保留”验证方式；不过得益于Matrix的联邦架构，用户仍可自建或选择境外私有服务器规避身份审查，尽管牺牲部分易用性如游戏串流和语音功能，此举凸显了去中心化通信在数字身份强制化浪潮中的关键避险价值。",
      "summaryZh": "Discord宣布下月起强制全用户年龄验证，终结匿名聊天时代，引发大规模用户迁移到去中心化平台Matrix；但Matrix官方主服务器同样受制于英国《在线安全法》等法规，计划引入信用卡等“隐私保留”验证方式；不过得益于Matrix的联邦架构，用户仍可自建或选择境外私有服务器规避身份审查，尽管牺牲部分易用性如游戏串流和语音功能，此举凸显了去中心化通信在数字身份强制化浪潮中的关键避险价值。",
      "summaryEn": "Discord’s upcoming mandatory age verification has triggered a mass exodus to Matrix, the open-source, federated messaging platform. However, even Matrix.org’s main server is complying with laws like the UK’s Online Safety Act and exploring 'privacy-preserving' age checks—likely involving credit cards. Unlike Discord, Matrix’s federated design allows users to self-host on private servers or choose jurisdictions without ID requirements, preserving anonymity at the cost of polished features like voice chat and game streaming. This shift highlights decentralized infrastructure as a vital refuge against rising identity mandates in mainstream platforms.",
      "fullText": "Discord Exodus Begins ← Back to Home Discord Just Killed Anonymity. Matrix is the Lifeboat. February 12, 2026 It finally happened. After months of speculation, Discord dropped the hammer: mandatory age verification for all users is rolling out next month. The era of anonymous gaming chats is officially over. Naturally, the internet is panicking. A massive exodus is underway, with thousands of users fleeing to Matrix , the open-source, decentralized alternative. But if you think switching platforms is a magic bullet, I have some bad news for you. The Matrix.org team just posted a sobering welcome message . While they are happy to take in the refugees, the main matrix.org homeserver is also bowing to the same draconian laws (like the UK’s Online Safety Act). They are exploring \"privacy-preserving\" age verification, which likely involves credit card checks or similar hurdles. So, are we screwed? Not entirely. Unlike Discord, Matrix is federated. You don't have to use their main server. You can spin up your own homeserver on a Raspberry Pi or rent a VPS in a country that doesn't demand your ID card to send a meme. The trade-off? The user experience is still... rough. Missing features like seamless game streaming and proper voice channels will make the transition painful for power users. But frankly, I’d rather deal with clunky UI than scan my passport for a chat app. If you're staying on Discord, enjoy the surveillance. For the rest of us: it's time to learn how to self-host.",
      "imageUrl": "https://tse3.mm.bing.net/th?q=Hacker+Discord+Banner&w=1200&h=630&c=7&rs=1&p=0&o=5&pid=1.7&mkt=en-US&cc=US&setlang=en&adlt=moderate&t=1",
      "tags": [],
      "paperCategory": "",
      "signalReasons": [
        "来源：Hacker News",
        "跨源重复：1 个来源",
        "模型评分：8/10，理由：涉及全球用户隐私与平台治理的重大转折，引发去中心化通信生态的迁移潮，具有显著行业影响力和全球范围内的社会技术讨论价值。",
        "热度：49 / 评论 31"
      ],
      "score": 4.19,
      "publishedAt": "2026-02-12T22:18:01+00:00",
      "authors": [
        "f0r3st"
      ]
    },
    {
      "id": "hn_46991240",
      "title": "Gemini 3 Deep Think 升级：AI科研协作者登顶奥赛金牌水平",
      "titleZh": "Gemini 3 Deep Think 升级：AI科研协作者登顶奥赛金牌水平",
      "titleEn": "Gemini 3 Deep Think Upgrade Achieves Gold-Medal Performance in Science and Engineering",
      "url": "https://blog.google/innovation-and-ai/models-and-research/gemini-models/gemini-3-deep-think/",
      "type": "news",
      "source": "Hacker News",
      "summary": "Google发布Gemini 3 Deep Think重大升级版，专为解决科学、工程与研究中的开放式难题而设计，已在数学、物理、化学奥赛及Codeforces编程竞赛中达到金牌水平，并成功辅助学者发现论文逻辑漏洞、优化半导体晶体生长工艺；该模式现面向Google AI Ultra订阅用户开放App端访问，并启动Gemini API早期试用计划，允许研究人员通过API调用其高阶推理能力，将草图转化为可3D打印模型等，标志着AI从通用助手向专业科研协作者演进。",
      "summaryZh": "Google发布Gemini 3 Deep Think重大升级版，专为解决科学、工程与研究中的开放式难题而设计，已在数学、物理、化学奥赛及Codeforces编程竞赛中达到金牌水平，并成功辅助学者发现论文逻辑漏洞、优化半导体晶体生长工艺；该模式现面向Google AI Ultra订阅用户开放App端访问，并启动Gemini API早期试用计划，允许研究人员通过API调用其高阶推理能力，将草图转化为可3D打印模型等，标志着AI从通用助手向专业科研协作者演进。",
      "summaryEn": "Google has launched a major upgrade to Gemini 3 Deep Think, a specialized reasoning mode designed for open-ended scientific and engineering challenges. It achieves gold-medal performance on benchmarks like the International Math, Physics, and Chemistry Olympiads and Codeforces, and has already helped researchers identify subtle errors in peer-reviewed math papers and optimize semiconductor crystal growth. Now available to Google AI Ultra subscribers in the Gemini app and via early-access API for scientists and enterprises, Deep Think can turn sketches into 3D-printable models and interpret complex experimental data, marking a shift toward AI as a domain-specialized research collaborator rather than just a general-purpose assistant.",
      "fullText": "Gemini 3 Deep Think: AI model update designed for science Skip to main content The Keyword Gemini 3 Deep Think: Advancing science, research and engineering Share x.com Facebook LinkedIn Mail Copy link Home Innovation & AI Innovation & AI Models & Research Google DeepMind Google Research Google Labs Gemini models See all Products Developer tools Gemini app See all Infrastructure & cloud Global network Google Cloud See all Learn more: Google DeepMind blog Google Research blog Google Developers blog Google Cloud blog See all AI updates Models & Research Google DeepMind Google Research Google Labs Gemini models See all Products Developer tools Gemini app See all Infrastructure & cloud Global network Google Cloud See all Learn more: Google DeepMind blog Google Research blog Google Developers blog Google Cloud blog See all AI updates Products & platforms Products & platforms Products Search Maps Chrome Google Workspace Learning & Education Photos Shopping See all Platforms Android Google Play Wear OS See all Devices Pixel Google Nest Fitbit Chromebooks See all Learn more: Google Ads & Commerce blog Waze blog See all product updates Products Search Maps Chrome Google Workspace Learning & Education Photos Shopping See all Platforms Android Google Play Wear OS See all Devices Pixel Google Nest Fitbit Chromebooks See all Learn more: Google Ads & Commerce blog Waze blog See all product updates Company news Company news Outreach & initiatives Creating opportunity Safety & security Google.org Public policy Sustainability Health See all Leadership Sundar Pichai, CEO More authors See all Inside Google Around the globe Life at Google See all Outreach & initiatives Creating opportunity Safety & security Google.org Public policy Sustainability Health See all Leadership Sundar Pichai, CEO More authors See all Inside Google Around the globe Life at Google See all Feed Subscribe Global (English) Africa (English) Australia (English) Brasil (Português) Canada (English) Canada (Français) Česko (Čeština) Deutschland (Deutsch) España (Español) France (Français) India (English) Indonesia (Bahasa Indonesia) Italia (Italiano) 日本 (日本語) 대한민국 (한국어) Latinoamérica (Español) الشرق الأوسط وشمال أفريقيا (اللغة العربية) MENA (English) Nederlands (Nederland) New Zealand (English) Polska (Polski) Portugal (Português) Sverige (Svenska) ประเทศไทย (ไทย) Türkiye (Türkçe) 台灣 (中文) [\"What does AI mean for retail?\", \"How did Nano Banana get its name?\", \"How can AI help me plan travel?\"] Subscribe The Keyword Home Innovation & AI Innovation & AI Models & Research Google DeepMind Google Research Google Labs Gemini models See all Products Developer tools Gemini app See all Infrastructure & cloud Global network Google Cloud See all Learn more: Google DeepMind blog Google Research blog Google Developers blog Google Cloud blog See all AI updates Products & platforms Products & platforms Products Search Maps Chrome Google Workspace Learning & Education Photos Shopping See all Platforms Android Google Play Wear OS See all Devices Pixel Google Nest Fitbit Chromebooks See all Learn more: Google Ads & Commerce blog Waze blog See all product updates Company news Company news Outreach & initiatives Creating opportunity Safety & security Google.org Public policy Sustainability Health See all Leadership Sundar Pichai, CEO More authors See all Inside Google Around the globe Life at Google See all Feed Press corner RSS feed Subscribe Breadcrumb Innovation & AI Models & research Gemini Models Gemini 3 Deep Think: Advancing science, research and engineering Feb 12, 2026 · Share x.com Facebook LinkedIn Mail Copy link Our most specialized reasoning mode is now updated to solve modern science, research and engineering challenges. The Deep Think team Read AI-generated summary General summary Gemini 3 Deep Think has a major upgrade to help solve science, research and engineering challenges. Google AI Ultra subscribers can now access the updated Deep Think in the Gemini app. Researchers, engineers and enterprises can express interest in early access to test Deep Think via the Gemini API. Summaries were generated by Google AI. Generative AI is experimental. Share x.com Facebook LinkedIn Mail Copy link Your browser does not support the audio element. Listen to article This content is generated by Google AI. Generative AI is experimental [[duration]] minutes Voice Speed Voice Speed 0.75X 1X 1.5X 2X Today, we’re releasing a major upgrade to Gemini 3 Deep Think , our specialized reasoning mode, built to push the frontier of intelligence and solve modern challenges across science, research, and engineering. We updated Gemini 3 Deep Think in close partnership with scientists and researchers to tackle tough research challenges — where problems often lack clear guardrails or a single correct solution and data is often messy or incomplete. By blending deep scientific knowledge with everyday engineering utility, Deep Think moves beyond abstract theory to drive practical applications. The new Deep Think is now available in the Gemini app for Google AI Ultra subscribers and, for the first time, we’re also making Deep Think available via the Gemini API to select researchers, engineers and enterprises. Express interest in early access here . Here is how our early testers are already using the latest Deep Think: Lisa Carbone, a mathematician at Rutgers University, works on the mathematical structures required by the high-energy physics community to bridge the gap between Einstein’s theory of gravity and quantum mechanics. In a field with very little existing training data, she used Deep Think to review a highly technical mathematics paper. Deep Think successfully identified a subtle logical flaw that had previously passed through human peer review unnoticed. At Duke University, the Wang Lab utilized Deep Think to optimize fabrication methods for complex crystal growth for the potential discovery of semiconductor materials. Deep Think successfully designed a recipe for growing thin films larger than 100 μm, meeting a precise target that previous methods had challenges to hit. Anupam Pathak, an R&D lead in Google’s Platforms and Devices division and former CEO of Liftware, tested the new Deep Think to accelerate the design of physical components. Elevating reasoning with mathematical and algorithmic rigor Last year, we showed that specialized versions of Deep Think could successfully navigate some of the toughest challenges in reasoning, achieving gold-medal standards at math and programming world championships. More recently, Deep Think has enabled specialized agents to conduct research-level mathematics exploration. The updated Deep Think mode continues to push the frontiers of intelligence, reaching new heights across the most rigorous academic benchmarks, including: Setting a new standard (48.4%, without tools) on Humanity’s Last Exam, a benchmark designed to test the limits of modern frontier models Achieving an unprecedented 84.6% on ARC-AGI-2, verified by the ARC Prize Foundation Attaining a staggering Elo of 3455 on Codeforces, a benchmark consisting of competitive programming challenges Reaching gold-medal level performance on the International Math Olympiad 2025 Navigating complex scientific domains Beyond mathematics and competitive coding, Gemini 3 Deep Think now also excels across broad scientific domains such as chemistry and physics. Our updated Deep Think mode demonstrates gold medal-level results on the written sections of the 2025 International Physics Olympiad and Chemistry Olympiad. It also demonstrates proficiency in advanced theoretical physics, achieving a score of 50.5% on CMT-Benchmark. Accelerating real-world engineering In addition to its state-of-the-art performance, Deep Think is built to drive practical applications, enabling researchers to interpret complex data, and engineers to model physical systems through code. Most importantly, we are working to bring Deep Think to researchers and practitioners where they need it most — beginning with surfaces such as the Gemini API. With the updated Deep Think, you can turn a sketch into a 3D-printable reality. Deep Think analyzes the drawing, models the complex shape and generates a file to create the physical object with 3D printing. Available to Google AI Ultra Subscribers and the Gemini API via our Early Access Program Google AI Ultra subscribers will be able to access the updated Deep Think mode starting today in the Gemini app. Scientists, engineers and enterprises can also now express interest in our early access program to test Deep Think via the Gemini API. We can’t wait to see what you discover. POSTED IN: Gemini models AI Google DeepMind Google One Related stories Photos 9 fun questions to try asking Google Photos By Molly McHugh-Johnson Feb 10, 2026 Safety & Security Helping kids and teens learn and grow online on Safer Internet Day By Mindy Brooks & Jennifer Flannery O'Connor Feb 10, 2026 Accessibility Natively Adaptive Interfaces: A new framework for AI accessibility By Sam Sepah Feb 05, 2026 Google Cloud How Google Cloud is helping Team USA elevate their tricks with AI Feb 05, 2026 AI Watch our new Gemini ad ahead of football’s biggest weekend By Marvin Chow Feb 05, 2026 AI The latest AI news we announced in January By Keyword Team Feb 04, 2026 . Jump to position 1 Jump to position 2 Jump to position 3 Jump to position 4 Jump to position 5 Jump to position 6 Let’s stay in touch. Get the latest news from Google in your inbox. Subscribe No thanks Follow Us Privacy Terms About Google Google Products About the Keyword Help Global (English) Africa (English) Australia (English) Brasil (Português) Canada (English) Canada (Français) Česko (Čeština) Deutschland (Deutsch) España (Español) France (Français) India (English) Indonesia (Bahasa Indonesia) Italia (Italiano) 日本 (日本語) 대한민국 (한국어) Latinoamérica (Español) الشرق الأوسط وشمال أفريقيا (اللغة العربية) MENA (English) Nederlands (Nederland) New Zealand (English) Polska (Polski) Portugal (Português) Sverige (Svenska) ประเทศไทย (ไทย) Türkiye (Türkçe) 台灣 (中文)",
      "imageUrl": "https://storage.googleapis.com/gweb-uniblog-publish-prod/images/gemini-3_deep-think_meta_dark_Fm70Cou.width-1300.png",
      "tags": [
        "LLM"
      ],
      "paperCategory": "",
      "signalReasons": [
        "来源：Hacker News",
        "跨源重复：1 个来源",
        "模型评分：9/10，理由：Gemini 3 Deep Think 被定位为面向科学与工程的AI模型升级，若属实则代表Google DeepMind在专业领域应用上的战略突破，具备全球产业级影响。",
        "热度：565 / 评论 326"
      ],
      "score": 9.9,
      "publishedAt": "2026-02-12T16:55:50+00:00",
      "authors": [
        "tosh"
      ]
    }
  ],
  "stats": {
    "total_papers_ingested": 290,
    "total_news_ingested": 64,
    "l1_papers_passed": 117,
    "l1_news_passed": 52,
    "l2_papers_scored": 54,
    "l2_news_scored": 30,
    "l3_papers_selected": 18,
    "l3_news_selected": 11,
    "news_source_counts": {
      "Hacker News": 29,
      "GitHub Trending": 11,
      "TechCrunch AI": 7,
      "NVIDIA Blog": 4,
      "The Verge AI": 4,
      "MIT Tech Review AI": 2,
      "AWS Machine Learning Blog": 2,
      "OpenAI Blog": 1,
      "Google AI Blog": 1,
      "DeepMind Blog": 1,
      "Hugging Face Blog": 1,
      "OpenGVLab (GitHub)": 1
    },
    "rss_source_counts": {
      "TechCrunch AI": 7,
      "NVIDIA Blog": 4,
      "The Verge AI": 4,
      "MIT Tech Review AI": 2,
      "AWS Machine Learning Blog": 2,
      "OpenAI Blog": 1,
      "Google AI Blog": 1,
      "DeepMind Blog": 1,
      "Hugging Face Blog": 1,
      "OpenGVLab (GitHub)": 1
    },
    "news_title_source_counts": {
      "gpt 5 3 codex spark": 1,
      "discord just killed anonymity": 1,
      "gemini 3 deep think": 1,
      "an ai agent published a hit piece on me": 1,
      "major european payment processor can t send email to google workspace users": 1,
      "launch hn omnara yc s25 run claude code and codex from anywhere": 1,
      "improving 15 llms at coding in one afternoon only the harness changed": 1,
      "fixing retail with land value capture": 1,
      "ai dr": 1,
      "anthropic raises 30b in series g funding at 380b post money valuation": 1,
      "carl sagan s baloney detection kit tools for thinking critically 2025": 1,
      "warcraft iii peon voice notifications for claude code": 1,
      "amazon gets fcc approval to launch 4 500 leo internet satellites": 1,
      "a party balloon shut down el paso international airport estimated cost 573k": 1,
      "lines of code are back and it s worse than before": 1,
      "show hn 20 claude code agents coordinating on real work open source": 1,
      "alarm bells just rang at san francisco s 2 buzziest tech companies": 1,
      "65 lines of markdown a claude code sensation": 1,
      "ibm triples us entry level hiring for roles ai was predicted to replace": 1,
      "openai requires id verification for gpt 5 3 codex silently reroutes requests": 1,
      "i was insulted today ai style": 1,
      "google identifies over 100k prompts used in distillation attacks": 1,
      "what s the difference between a disc and a disk 2023": 1,
      "ai agent opens a pr write a blogpost to shames the maintainer who closes it": 1,
      "the problem with llms": 1,
      "show hn double blind entropy using drand for verifiably fair randomness": 1,
      "america s cyber defense agency is burning down and nobody s coming to put it out": 1,
      "training qwen 4b to beat large models on work tasks": 1,
      "from specification to stress test a weekend with claude": 1,
      "tambo ai tambo": 1,
      "danielmiessler personal ai infrastructure": 1,
      "google langextract": 1,
      "chromedevtools chrome devtools mcp": 1,
      "iofficeai aionui": 1,
      "shubhamsaboo awesome llm apps": 1,
      "rowboatlabs rowboat": 1,
      "github gh aw": 1,
      "unslothai unsloth": 1,
      "jeffallan claude skills": 1,
      "handsonllm hands on large language models": 1,
      "introducing gpt 5 3 codex spark": 1,
      "gemini 3 deep think advancing science research and engineering": 2,
      "code compute and connection inside the inaugural nvidia ai day s o paulo": 1,
      "leading inference providers cut ai costs by up to 10x with open source models on nvidia blackwell": 1,
      "nvidia dgx spark powers big projects in higher education": 1,
      "geforce now turns screens into a gaming machine": 1,
      "good luck have fun don t die is a rollicking parable about this moment in tech": 1,
      "the surprising case for ai judges": 1,
      "bytedance s next gen ai model can generate clips based on text images audio and video": 1,
      "this 7 999 robot will fold some of your laundry": 1,
      "amid disappointing earnings pinterest claims it sees more searches than chatgpt": 1,
      "ibm will hire your entry level talent in the age of ai": 1,
      "musk needed a new vision for spacex and xai he landed on moonbase alpha": 1,
      "didero lands 30m to put manufacturing procurement on agentic autopilot": 1,
      "anthropic raises another 30b in series g with a new value of 380b": 1,
      "spotify says its best developers haven t written a line of code since december thanks to ai": 1,
      "a new version of openai s codex is powered by a new dedicated chip": 1,
      "openenv in practice evaluating tool using agents in real world environments": 1,
      "ai is already making online crimes easier it could get much worse": 1,
      "what s next for chinese open source ai": 1,
      "ai meets hr transforming talent acquisition with amazon bedrock": 1,
      "build long running mcp servers on amazon bedrock agentcore with strands agents integration": 1,
      "opengvlab added penghaoyin to opengvlab ummevalkit": 1
    },
    "total_papers_deduped": 290,
    "total_news_deduped": 62,
    "news_recent_filtered": 62
  }
}