{
  "date": "2026-02-16",
  "generatedAt": "2026-02-16T23:45:41.933326",
  "introduction": "今日AI领域迎来多项关键进展：NVIDIA Blackwell Ultra平台性能跃升，为智能体AI带来高达50倍的推理加速与显著成本下降；多篇顶分论文聚焦多模态智能体、安全对齐、视频生成控制及具身推理等前沿方向。同时，Ricursive Intelligence以40亿美元估值完成3.35亿美元融资，凸显资本对顶尖人才团队的高度认可。在应用层面，AI视频生成引发好莱坞版权争议，推动行业安全机制升级；印度AI峰会汇聚全球科技巨头，加速本土算力基建布局。此外，开发者利用AI“蜂群”成功构建SQLite子集，展示了编码智能体的工程潜力。",
  "introductionZh": "今日AI领域迎来多项关键进展：NVIDIA Blackwell Ultra平台性能跃升，为智能体AI带来高达50倍的推理加速与显著成本下降；多篇顶分论文聚焦多模态智能体、安全对齐、视频生成控制及具身推理等前沿方向。同时，Ricursive Intelligence以40亿美元估值完成3.35亿美元融资，凸显资本对顶尖人才团队的高度认可。在应用层面，AI视频生成引发好莱坞版权争议，推动行业安全机制升级；印度AI峰会汇聚全球科技巨头，加速本土算力基建布局。此外，开发者利用AI“蜂群”成功构建SQLite子集，展示了编码智能体的工程潜力。",
  "introductionEn": "Today’s AI landscape features breakthroughs across hardware, agents, and safety. NVIDIA’s Blackwell Ultra delivers up to 50x faster inference for agentic AI at lower costs, while top-tier papers advance multimodal browsing agents, game-theoretic safety benchmarks, and controllable video generation. Ricursive Intelligence raised $335M at a $4B valuation, underscoring investor confidence in elite AI talent. Meanwhile, ByteDance’s Seedance 2.0 faces Hollywood backlash over deepfake copyright concerns, prompting stronger safeguards. The India AI Summit draws global tech leaders to boost domestic infrastructure, and an experiment using a small LLM swarm successfully built a functional SQLite-like engine—demonstrating the growing engineering capability of AI coding agents.",
  "longformScript": "今天，AI世界又经历了一轮密集的进化：从底层算力到上层应用，从资本押注到法律边界，多个关键节点同时松动。一边是硬件性能与成本结构的颠覆性突破，另一边是生成内容引发的版权风暴；一边是印度加速构建本土AI生态，另一边是开发者用AI“蜂群”写出了数据库引擎。这些看似分散的事件，其实共同指向一个趋势——AI正从模型竞赛转向系统级落地，而代价、风险与机会也前所未有地交织在一起。\n\n先看算力端。NVIDIA刚刚发布的Blackwell Ultra平台，不只是又一次性能升级，它直接重构了AI代理的经济模型。在需要长上下文、低延迟交互的场景里，比如实时编码助手或多轮任务规划，每token推理成本最高可降35倍，每兆瓦吞吐量提升50倍。这意味着过去只能在实验室跑的复杂智能体，现在有可能大规模部署到普通开发者的工具链中。更关键的是，这种软硬协同的设计思路，正在把“智能体AI”从概念推向实用——不再是单次问答，而是能持续思考、调用工具、自我修正的交互式系统。而就在同一时间，一家叫Ricursive Intelligence的初创公司，悄悄拿下了3.35亿美元融资，估值冲到40亿美元。它的创始人来自Google Brain和Anthropic，但不造大模型，也不做应用，而是用AI自动设计芯片。目标很明确：把原本要一年的芯片设计周期压缩到几小时，并让AI硬件和模型能更快协同迭代。如果这条路走通，未来我们看到的不仅是更强的GPU，而是整个算力基础设施的自我进化能力。\n\n当然，算力扩张也带来了现实压力。西部数据刚刚宣布，2026年全年硬盘产能已被全球前七大AI客户全部包下，消费级市场几乎被挤出供应链。这意味着普通用户买硬盘可能更贵、更难，甚至影响游戏主机等消费电子产品的发布节奏。而在印度，这种压力正转化为基建狂潮。首届AI峰会刚落幕，11亿美元国家风投基金启动，黑石领投12亿美元支持本地GPU集群公司Neysa扩产至2万台，另一家初创C2i则专注解决数据中心供电损耗问题——把电力转换效率从80%提升到90%，每兆瓦省下100千瓦。这些动作背后，是一个清晰的战略：印度不再只想做AI的用户，而是要成为算力、人才和生态的供给方。OpenAI、Anthropic纷纷设点，ChatGPT在印周活破亿，说明这里既是市场，也正成为新的创新节点。\n\n但技术跑得越快，法律和伦理的刹车就越显紧迫。字节跳动的Seedance 2.0视频生成模型最近惹上了大麻烦——它能逼真地合成汤姆·克鲁斯和布拉德·皮特打斗的画面，结果引来迪士尼、派拉蒙和好莱坞工会集体抗议。这已经不是简单的“以假乱真”，而是直接踩到了肖像权和IP授权的红线。公司迅速回应要加装版权保护机制，但问题没那么简单：当AI能任意重组名人形象、声音甚至表演风格时，现有的法律框架是否还适用？对普通用户来说，随手生成一段“明星打架”的短视频，可能就埋下了侵权隐患。行业亟需的不仅是技术上的水印或过滤，更是内容溯源、授权链条和责任界定的新规则。\n\n与此同时，在开发者社区，一场静默的实验正在验证AI编程的极限。一位开发者协调Claude、Codex和Gemini三个AI代理，协作写出了一个类SQLite的数据库引擎，包含解析器、查询规划器、B+树存储等核心模块，通过了64个标准测试用例。听起来很酷，但过程暴露了多智能体协作的痛点：超过一半的代码提交用于处理锁和状态同步，缺乏并发支持，还有大量冗余。不过，这个项目开源后提供了一个可复现的框架，也揭示了未来AI编程的关键前提——必须有清晰的模块边界、高频测试反馈和共享的状态文档。类似地，GitHub上新冒头的Letta Code和Synkra AIOS Core，分别从“记忆为中心”和“操作系统级调度”角度，探索如何让AI在长期项目中保持上下文连贯、实现端到端自动化。这些工具未必明天就改变你的工作流，但它们代表了下一代开发范式的雏形：AI不是一次性代码生成器，而是能持续参与、演进和维护的协作者。\n\n那么，作为听众，该怎么理解今天这一连串变化？首先，算力成本的断崖式下降和本地化基建的崛起，意味着更多人能接触高性能AI服务，但也要警惕硬件资源向巨头集中带来的供应挤压。其次，生成式AI的创作自由正撞上法律高墙，使用这类工具时，别只盯着效果，更要考虑合规边界。最后，AI编程的潜力正在从“写函数”迈向“建系统”，但现阶段仍需人类深度介入架构设计与质量控制。换句话说，AI的能力半径在扩大，但责任半径也在同步延伸。\n\n今天的AI世界，既在加速奔向更强大的智能体时代，也在为失控的风险紧急补课。技术不会停下，但如何让它跑得稳、跑得久，或许比跑得多快更重要。",
  "longformScriptZh": "今天，AI世界又经历了一轮密集的进化：从底层算力到上层应用，从资本押注到法律边界，多个关键节点同时松动。一边是硬件性能与成本结构的颠覆性突破，另一边是生成内容引发的版权风暴；一边是印度加速构建本土AI生态，另一边是开发者用AI“蜂群”写出了数据库引擎。这些看似分散的事件，其实共同指向一个趋势——AI正从模型竞赛转向系统级落地，而代价、风险与机会也前所未有地交织在一起。\n\n先看算力端。NVIDIA刚刚发布的Blackwell Ultra平台，不只是又一次性能升级，它直接重构了AI代理的经济模型。在需要长上下文、低延迟交互的场景里，比如实时编码助手或多轮任务规划，每token推理成本最高可降35倍，每兆瓦吞吐量提升50倍。这意味着过去只能在实验室跑的复杂智能体，现在有可能大规模部署到普通开发者的工具链中。更关键的是，这种软硬协同的设计思路，正在把“智能体AI”从概念推向实用——不再是单次问答，而是能持续思考、调用工具、自我修正的交互式系统。而就在同一时间，一家叫Ricursive Intelligence的初创公司，悄悄拿下了3.35亿美元融资，估值冲到40亿美元。它的创始人来自Google Brain和Anthropic，但不造大模型，也不做应用，而是用AI自动设计芯片。目标很明确：把原本要一年的芯片设计周期压缩到几小时，并让AI硬件和模型能更快协同迭代。如果这条路走通，未来我们看到的不仅是更强的GPU，而是整个算力基础设施的自我进化能力。\n\n当然，算力扩张也带来了现实压力。西部数据刚刚宣布，2026年全年硬盘产能已被全球前七大AI客户全部包下，消费级市场几乎被挤出供应链。这意味着普通用户买硬盘可能更贵、更难，甚至影响游戏主机等消费电子产品的发布节奏。而在印度，这种压力正转化为基建狂潮。首届AI峰会刚落幕，11亿美元国家风投基金启动，黑石领投12亿美元支持本地GPU集群公司Neysa扩产至2万台，另一家初创C2i则专注解决数据中心供电损耗问题——把电力转换效率从80%提升到90%，每兆瓦省下100千瓦。这些动作背后，是一个清晰的战略：印度不再只想做AI的用户，而是要成为算力、人才和生态的供给方。OpenAI、Anthropic纷纷设点，ChatGPT在印周活破亿，说明这里既是市场，也正成为新的创新节点。\n\n但技术跑得越快，法律和伦理的刹车就越显紧迫。字节跳动的Seedance 2.0视频生成模型最近惹上了大麻烦——它能逼真地合成汤姆·克鲁斯和布拉德·皮特打斗的画面，结果引来迪士尼、派拉蒙和好莱坞工会集体抗议。这已经不是简单的“以假乱真”，而是直接踩到了肖像权和IP授权的红线。公司迅速回应要加装版权保护机制，但问题没那么简单：当AI能任意重组名人形象、声音甚至表演风格时，现有的法律框架是否还适用？对普通用户来说，随手生成一段“明星打架”的短视频，可能就埋下了侵权隐患。行业亟需的不仅是技术上的水印或过滤，更是内容溯源、授权链条和责任界定的新规则。\n\n与此同时，在开发者社区，一场静默的实验正在验证AI编程的极限。一位开发者协调Claude、Codex和Gemini三个AI代理，协作写出了一个类SQLite的数据库引擎，包含解析器、查询规划器、B+树存储等核心模块，通过了64个标准测试用例。听起来很酷，但过程暴露了多智能体协作的痛点：超过一半的代码提交用于处理锁和状态同步，缺乏并发支持，还有大量冗余。不过，这个项目开源后提供了一个可复现的框架，也揭示了未来AI编程的关键前提——必须有清晰的模块边界、高频测试反馈和共享的状态文档。类似地，GitHub上新冒头的Letta Code和Synkra AIOS Core，分别从“记忆为中心”和“操作系统级调度”角度，探索如何让AI在长期项目中保持上下文连贯、实现端到端自动化。这些工具未必明天就改变你的工作流，但它们代表了下一代开发范式的雏形：AI不是一次性代码生成器，而是能持续参与、演进和维护的协作者。\n\n那么，作为听众，该怎么理解今天这一连串变化？首先，算力成本的断崖式下降和本地化基建的崛起，意味着更多人能接触高性能AI服务，但也要警惕硬件资源向巨头集中带来的供应挤压。其次，生成式AI的创作自由正撞上法律高墙，使用这类工具时，别只盯着效果，更要考虑合规边界。最后，AI编程的潜力正在从“写函数”迈向“建系统”，但现阶段仍需人类深度介入架构设计与质量控制。换句话说，AI的能力半径在扩大，但责任半径也在同步延伸。\n\n今天的AI世界，既在加速奔向更强大的智能体时代，也在为失控的风险紧急补课。技术不会停下，但如何让它跑得稳、跑得久，或许比跑得多快更重要。",
  "longformScriptEn": "Today’s AI landscape is defined by a powerful convergence: breakthroughs in hardware efficiency, the rise of autonomous coding agents, and mounting pressure to align innovation with legal and ethical guardrails. From NVIDIA’s cost-slashing Blackwell Ultra chips to India’s billion-dollar push for sovereign AI infrastructure, the ecosystem is accelerating on multiple fronts—while simultaneously confronting real-world consequences of unchecked generative power. As agentic systems grow more capable, the line between engineering marvel and societal risk continues to blur.\n\nLet’s start with the engine room: compute. NVIDIA’s new Blackwell Ultra platform isn’t just an incremental upgrade—it’s a paradigm shift for agentic AI. By co-designing hardware and software at an extreme level, NVIDIA claims up to 35x lower cost per token and 50x higher throughput per megawatt compared to its previous Hopper architecture. That means real-time, multi-step AI applications—like coding assistants that reason through complex debugging sessions—can now scale affordably. This isn’t theoretical; it’s already enabling developers to deploy low-latency, long-context agents without breaking the bank. But even as NVIDIA powers the frontier, others are reimagining how chips themselves are made. Enter Ricursive Intelligence, a stealthy startup founded by ex-Google Brain and Anthropic engineers, which just raised $335 million at a $4 billion valuation in only four months. Ricursive uses AI to automate chip design, slashing development cycles from over a year to mere hours for clients like NVIDIA and AMD. Their self-improving platform could catalyze a feedback loop where better AI designs better hardware, which in turn trains better AI—a potential accelerant toward both energy-efficient computing and, some speculate, AGI.\n\nMeanwhile, the creative frontier is hitting legal walls. ByteDance’s Seedance 2.0, a hyperrealistic AI video generator, drew fierce backlash from Disney, Paramount, and Hollywood unions after deepfakes surfaced featuring Tom Cruise and Brad Pitt without consent. The controversy underscores a growing tension: generative models can now replicate likenesses and styles with uncanny fidelity, but copyright law hasn’t caught up. In response, ByteDance is tightening safeguards—adding watermarking, usage logs, and stricter input filters—but the episode serves as a cautionary tale. As these tools proliferate, developers and enterprises must prioritize content attribution and licensing frameworks before regulators impose blunt restrictions that stifle innovation altogether.\n\nOn the global stage, India is making a bold play to become an AI powerhouse in its own right. At its inaugural AI Impact Summit, the country unveiled a $1.1 billion national venture fund backed by global giants like OpenAI, Anthropic, and NVIDIA. With over 100 million weekly ChatGPT users—second only to the U.S.—India’s market is too large to ignore. But this isn’t just about consumption; it’s about sovereignty. Infrastructure startup Neysa just secured up to $1.2 billion from Blackstone to scale its GPU fleet from 1,200 to over 20,000 units, aiming to serve local enterprises and government agencies with compliant, low-latency compute. Complementing this, Indian startup C2i raised $15 million to tackle a silent bottleneck: power loss in data centers. Their “grid-to-GPU” system cuts energy waste from 20% to 10%, saving 100 kilowatts per megawatt—a critical efficiency gain as AI’s thirst for electricity strains grids worldwide. Together, these moves signal India’s ambition not just to adopt AI, but to build its own stack—from silicon to software.\n\nAnd speaking of software, the dream of AI that can build AI is inching closer to reality. In a striking experiment, developer Kian Kyars orchestrated a small swarm of agents—Claude, Codex, and Gemini—to collaboratively build a SQLite-like database engine in Rust. Over 19,000 lines of code later, the system passed dozens of SQL tests, implementing core components like B+ trees, WAL logging, and transaction semantics. Yet the effort revealed sobering limits: more than half the work went into coordination overhead, concurrency was nonexistent, and code duplication persisted despite mitigation attempts. Still, open-source frameworks like parallel-ralph, Letta Code, and Synkra’s AIOS Core are pushing the envelope. Letta treats persistent memory as a first-class resource, enabling agents to retain context across sessions; AIOS Core goes further, treating AI itself as an operating-system-level primitive that orchestrates everything from frontend design to backend deployment. These aren’t just tools—they’re blueprints for a future where software builds itself, incrementally and intelligently.\n\nSo what should you watch next? First, monitor how regulatory responses to deepfakes evolve—especially in the U.S. and EU—as they’ll shape the boundaries of generative media. Second, track India’s infrastructure rollout: if Neysa and C2i deliver, they could offer a compelling alternative to Western cloud dominance for emerging markets. Third, keep an eye on agent coordination frameworks; the gap between demo and production remains wide, but the trajectory is clear. Risks abound—hardware shortages are already biting, with Western Digital selling out its entire 2026 hard drive supply to AI firms, leaving consumers scrambling. Opportunities lie in efficiency: whether it’s Ricursive cutting chip design time or C2i trimming power loss, the next wave of value may come not from bigger models, but smarter systems.\n\nIn sum, today’s AI story is one of simultaneous expansion and constraint. Compute is getting cheaper and faster, agents are learning to collaborate, and nations are racing to build sovereign stacks—but legal, ethical, and physical bottlenecks are tightening just as quickly. The winners won’t just be those who push capability forward, but those who navigate this complex terrain with foresight, responsibility, and precision. For now, the engines are running hot, the code is compiling, and the world is watching.",
  "audioUrl": "",
  "papers": [
    {
      "id": "arxiv_2602_12876v1",
      "title": "BrowseComp-$V^3$: A Visual, Vertical, and Verifiable Benchmark for Multimodal Browsing Agents",
      "titleZh": "BrowseComp-$V^3$: A Visual, Vertical, and Verifiable Benchmark for Multimodal Browsing Agents",
      "titleEn": "BrowseComp-$V^3$: A Visual, Vertical, and Verifiable Benchmark for Multimodal Browsing Agents",
      "url": "https://arxiv.org/abs/2602.12876v1",
      "type": "paper",
      "source": "arXiv",
      "summary": "为评估多模态浏览智能体的深度搜索能力，研究者提出了BrowseComp-V³基准，包含300个跨领域、需多跳跨模态推理的挑战性问题，所有证据均公开可查；该基准引入基于子目标的专家验证过程评估机制，并配套提出统一代理框架OmniSeeker，实验显示当前最先进模型准确率仅36%，揭示了多模态信息整合与细粒度感知的关键瓶颈。",
      "summaryZh": "为评估多模态浏览智能体的深度搜索能力，研究者提出了BrowseComp-V³基准，包含300个跨领域、需多跳跨模态推理的挑战性问题，所有证据均公开可查；该基准引入基于子目标的专家验证过程评估机制，并配套提出统一代理框架OmniSeeker，实验显示当前最先进模型准确率仅36%，揭示了多模态信息整合与细粒度感知的关键瓶颈。",
      "summaryEn": "To evaluate deep search capabilities of multimodal browsing agents, researchers introduce BrowseComp-V³, a benchmark of 300 challenging, cross-domain questions requiring multi-hop, cross-modal reasoning with all evidence publicly accessible. It features an expert-validated, subgoal-driven process evaluation and a unified agent framework, OmniSeeker. Experiments show state-of-the-art models achieve only 36% accuracy, exposing critical bottlenecks in multimodal integration and fine-grained perception.",
      "fullText": "",
      "imageUrl": "",
      "tags": [
        "LLM",
        "Multimodal",
        "Agent",
        "Reasoning"
      ],
      "paperCategory": "General AI",
      "signalReasons": [
        "来源：arXiv",
        "跨源重复：1 个来源",
        "模型评分：9/10，理由：构建首个视觉、垂直、可验证的多模态浏览代理基准，填补领域空白，对AI代理能力评估具有战略级影响力。",
        "热度：22 / 评论 0"
      ],
      "score": 9.0,
      "publishedAt": "2026-02-13T12:25:13+00:00",
      "authors": [
        "Huanyao Zhang",
        "Jiepeng Zhou",
        "Bo Li"
      ]
    },
    {
      "id": "arxiv_2602_12962v1",
      "title": "TriGen: NPU Architecture for End-to-End Acceleration of Large Language Models based on SW-HW Co-Design",
      "titleZh": "TriGen: NPU Architecture for End-to-End Acceleration of Large Language Models based on SW-HW Co-Design",
      "titleEn": "TriGen: NPU Architecture for End-to-End Acceleration of Large Language Models based on SW-HW Co-Design",
      "url": "https://arxiv.org/abs/2602.12962v1",
      "type": "paper",
      "source": "arXiv",
      "summary": "针对资源受限设备上大语言模型（LLM）端到端推理的挑战，研究者提出TriGen——一种软硬件协同设计的NPU架构，通过微缩放（MX）低精度计算、基于查找表（LUT）的非线性操作优化及面向片上内存限制的调度策略，在保持精度几乎无损的前提下，相比基线NPU平均提升2.73倍性能并减少52%内存传输。",
      "summaryZh": "针对资源受限设备上大语言模型（LLM）端到端推理的挑战，研究者提出TriGen——一种软硬件协同设计的NPU架构，通过微缩放（MX）低精度计算、基于查找表（LUT）的非线性操作优化及面向片上内存限制的调度策略，在保持精度几乎无损的前提下，相比基线NPU平均提升2.73倍性能并减少52%内存传输。",
      "summaryEn": "Addressing the challenge of end-to-end LLM inference on resource-constrained devices, researchers propose TriGen, an NPU architecture co-designed in software and hardware. It leverages microscaling (MX) for low-precision computation, replaces specialized nonlinear hardware with fast LUTs, and employs memory-aware scheduling. TriGen achieves 2.73× average speedup and 52% less memory transfer over baseline NPUs with negligible accuracy loss.",
      "fullText": "",
      "imageUrl": "",
      "tags": [
        "LLM",
        "Inference",
        "RAG"
      ],
      "paperCategory": "General AI",
      "signalReasons": [
        "来源：arXiv",
        "跨源重复：1 个来源",
        "模型评分：9/10，理由：基于软硬件协同设计的NPU架构实现端侧LLM全链路加速，突破设备算力瓶颈，对边缘AI落地具有里程碑意义。",
        "热度：16 / 评论 0"
      ],
      "score": 9.0,
      "publishedAt": "2026-02-13T14:28:31+00:00",
      "authors": [
        "Jonghun Lee",
        "Junghoon Lee",
        "Hyeonjin Kim"
      ]
    },
    {
      "id": "arxiv_2602_12316v1",
      "title": "GT-HarmBench: Benchmarking AI Safety Risks Through the Lens of Game Theory",
      "titleZh": "GT-HarmBench: Benchmarking AI Safety Risks Through the Lens of Game Theory",
      "titleEn": "GT-HarmBench: Benchmarking AI Safety Risks Through the Lens of Game Theory",
      "url": "https://arxiv.org/abs/2602.12316v1",
      "type": "paper",
      "source": "arXiv",
      "summary": "为评估前沿AI系统在多智能体环境中的安全风险，研究者构建了GT-HarmBench基准，涵盖2,009个源自真实AI风险场景的博弈论情境（如囚徒困境、猎鹿博弈等）；测试显示15个前沿模型仅在62%情况下选择社会有益行为，而通过博弈论干预可将该比例提升最多18%，凸显当前AI在协调与冲突避免方面的可靠性缺口。",
      "summaryZh": "为评估前沿AI系统在多智能体环境中的安全风险，研究者构建了GT-HarmBench基准，涵盖2,009个源自真实AI风险场景的博弈论情境（如囚徒困境、猎鹿博弈等）；测试显示15个前沿模型仅在62%情况下选择社会有益行为，而通过博弈论干预可将该比例提升最多18%，凸显当前AI在协调与冲突避免方面的可靠性缺口。",
      "summaryEn": "To assess AI safety risks in multi-agent settings, researchers introduce GT-HarmBench—a benchmark of 2,009 realistic scenarios based on game-theoretic structures like Prisoner’s Dilemma and Stag Hunt. Testing 15 frontier models reveals they choose socially beneficial actions only 62% of the time, but game-theoretic interventions can improve this by up to 18%, highlighting significant reliability gaps in coordination and conflict avoidance.",
      "fullText": "",
      "imageUrl": "",
      "tags": [
        "Agent",
        "Reasoning",
        "Open Source",
        "Benchmark"
      ],
      "paperCategory": "General AI",
      "signalReasons": [
        "来源：arXiv",
        "跨源重复：1 个来源",
        "模型评分：9/10，理由：首个面向多智能体环境的AI安全基准，直击前沿系统部署中的协同风险，具有全球产业政策参考价值。",
        "热度：14 / 评论 0"
      ],
      "score": 9.0,
      "publishedAt": "2026-02-12T17:29:52+00:00",
      "authors": [
        "Pepijn Cobben",
        "Xuanqiang Angelo Huang",
        "Thao Amelia Pham"
      ]
    },
    {
      "id": "arxiv_2602_12758v1",
      "title": "VineetVC: Adaptive Video Conferencing Under Severe Bandwidth Constraints Using Audio-Driven Talking-Head Reconstruction",
      "titleZh": "VineetVC: Adaptive Video Conferencing Under Severe Bandwidth Constraints Using Audio-Driven Talking-Head Reconstruction",
      "titleEn": "VineetVC: Adaptive Video Conferencing Under Severe Bandwidth Constraints Using Audio-Driven Talking-Head Reconstruction",
      "url": "https://arxiv.org/abs/2602.12758v1",
      "type": "paper",
      "source": "arXiv",
      "summary": "面对严重带宽受限下的视频会议稳定性问题，研究者提出VineetVC系统，通过WebRTC与音频驱动的说话人头像重建通路相结合，在浏览器端实现实时带宽感知切换，将外发视频流替换为仅32.80 kbps的合成视频，显著降低带宽需求并维持通信可用性。",
      "summaryZh": "面对严重带宽受限下的视频会议稳定性问题，研究者提出VineetVC系统，通过WebRTC与音频驱动的说话人头像重建通路相结合，在浏览器端实现实时带宽感知切换，将外发视频流替换为仅32.80 kbps的合成视频，显著降低带宽需求并维持通信可用性。",
      "summaryEn": "To address video conferencing instability under severe bandwidth constraints, researchers propose VineetVC—an adaptive system combining WebRTC with an audio-driven talking-head reconstruction pipeline. The browser client dynamically switches to a synthesized video stream at a median bitrate of 32.80 kbps, drastically reducing bandwidth usage while preserving call usability.",
      "fullText": "",
      "imageUrl": "",
      "tags": [
        "Vision",
        "Audio"
      ],
      "paperCategory": "General AI",
      "signalReasons": [
        "来源：arXiv",
        "跨源重复：1 个来源",
        "模型评分：9/10，理由：在极端带宽下实现高质量视频会议，突破通信瓶颈，对全球远程协作基础设施有深远影响。",
        "热度：5 / 评论 0"
      ],
      "score": 9.0,
      "publishedAt": "2026-02-13T09:37:10+00:00",
      "authors": [
        "Vineet Kumar Rakesh",
        "Soumya Mazumdar",
        "Tapas Samanta"
      ]
    },
    {
      "id": "arxiv_2602_12641v1",
      "title": "Artic: AI-oriented Real-time Communication for MLLM Video Assistant",
      "titleZh": "Artic: AI-oriented Real-time Communication for MLLM Video Assistant",
      "titleEn": "Artic: AI-oriented Real-time Communication for MLLM Video Assistant",
      "url": "https://arxiv.org/abs/2602.12641v1",
      "type": "paper",
      "source": "arXiv",
      "summary": "针对多模态大语言模型（MLLM）视频助手与传统实时通信（RTC）框架不匹配的问题，研究者提出Artic——首个面向AI理解视频的RTC框架，通过响应能力感知的码率控制、上下文感知的零开销流媒体及首个降质视频理解基准DeViBench，在真实上行链路下将MLLM准确率提升15.12%、延迟降低135.31毫秒。",
      "summaryZh": "针对多模态大语言模型（MLLM）视频助手与传统实时通信（RTC）框架不匹配的问题，研究者提出Artic——首个面向AI理解视频的RTC框架，通过响应能力感知的码率控制、上下文感知的零开销流媒体及首个降质视频理解基准DeViBench，在真实上行链路下将MLLM准确率提升15.12%、延迟降低135.31毫秒。",
      "summaryEn": "To bridge the mismatch between traditional RTC frameworks and MLLM-based video assistants, researchers propose Artic—an AI-oriented RTC framework that shifts focus from 'humans watching video' to 'AI understanding video.' It introduces response-aware bitrate capping, context-aware streaming, and DeViBench, the first benchmark for RTC-induced video degradation. Real-world tests show 15.12% higher accuracy and 135.31 ms lower latency versus existing methods.",
      "fullText": "",
      "imageUrl": "",
      "tags": [
        "LLM",
        "Multimodal",
        "Open Source",
        "Benchmark"
      ],
      "paperCategory": "General AI",
      "signalReasons": [
        "来源：arXiv",
        "跨源重复：1 个来源",
        "模型评分：9/10，理由：提出面向多模态大模型视频助手的实时通信架构Artic，突破云侧MLLM与终端交互延迟瓶颈，有望重塑人机交互范式，具备全球产业变革潜力。",
        "热度：13 / 评论 0"
      ],
      "score": 9.0,
      "publishedAt": "2026-02-13T06:00:17+00:00",
      "authors": [
        "Jiangkai Wu",
        "Zhiyuan Ren",
        "Junquan Zhong"
      ]
    },
    {
      "id": "arxiv_2602_12556v1",
      "title": "SD-MoE: Spectral Decomposition for Effective Expert Specialization",
      "titleZh": "SD-MoE: Spectral Decomposition for Effective Expert Specialization",
      "titleEn": "SD-MoE: Spectral Decomposition for Effective Expert Specialization",
      "url": "https://arxiv.org/abs/2602.12556v1",
      "type": "paper",
      "source": "arXiv",
      "summary": "针对混合专家（MoE）模型中专家专业化不足的问题，研究者从谱分析角度发现参数与梯度空间存在高度重叠的主成分，导致专家功能趋同；为此提出谱解耦MoE（SD-MoE），通过对参数和梯度进行谱分解，在几乎不增加计算开销的前提下有效促进专家专业化，并在Qwen、DeepSeek等架构中验证了其通用性和性能提升。",
      "summaryZh": "针对混合专家（MoE）模型中专家专业化不足的问题，研究者从谱分析角度发现参数与梯度空间存在高度重叠的主成分，导致专家功能趋同；为此提出谱解耦MoE（SD-MoE），通过对参数和梯度进行谱分解，在几乎不增加计算开销的前提下有效促进专家专业化，并在Qwen、DeepSeek等架构中验证了其通用性和性能提升。",
      "summaryEn": "Addressing ineffective expert specialization in Mixture-of-Experts (MoE) models, researchers identify—via spectral analysis—that dominant parameter and gradient components are highly overlapping across experts due to low-rank structures in human language. They propose Spectral-Decoupled MoE (SD-MoE), which decomposes parameters and gradients in spectral space, enabling effective specialization with minimal overhead and consistent gains across architectures like Qwen and DeepSeek.",
      "fullText": "",
      "imageUrl": "",
      "tags": [
        "LLM"
      ],
      "paperCategory": "General AI",
      "signalReasons": [
        "来源：arXiv",
        "跨源重复：1 个来源",
        "模型评分：9/10，理由：提出SD-MoE通过谱分解实现专家有效专业化，解决MoE架构中专家冗余与功能趋同的核心难题，显著提升模型扩展效率，具备行业级影响力。",
        "热度：12 / 评论 0"
      ],
      "score": 9.0,
      "publishedAt": "2026-02-13T03:07:26+00:00",
      "authors": [
        "Ruijun Huang",
        "Fang Dong",
        "Xin Zhang"
      ]
    },
    {
      "id": "arxiv_2602_12916v1",
      "title": "Reliable Thinking with Images",
      "titleZh": "Reliable Thinking with Images",
      "titleEn": "Reliable Thinking with Images",
      "url": "https://arxiv.org/abs/2602.12916v1",
      "type": "paper",
      "source": "arXiv",
      "summary": "针对“图像思维”（Thinking with Images, TWI）方法中因视觉线索错误引发的“噪声思维”（Noisy Thinking）问题，研究者提出可靠图像思维（RTWI）方法，通过统一文本中心化方式评估视觉与推理链的可靠性，并结合鲁棒过滤与投票机制防止错误累积，在七个基准上显著提升多模态大语言模型的推理鲁棒性。",
      "summaryZh": "针对“图像思维”（Thinking with Images, TWI）方法中因视觉线索错误引发的“噪声思维”（Noisy Thinking）问题，研究者提出可靠图像思维（RTWI）方法，通过统一文本中心化方式评估视觉与推理链的可靠性，并结合鲁棒过滤与投票机制防止错误累积，在七个基准上显著提升多模态大语言模型的推理鲁棒性。",
      "summaryEn": "To tackle 'Noisy Thinking' (NT)—error propagation caused by imperfect visual cues in Thinking with Images (TWI)—researchers propose Reliable Thinking with Images (RTWI). It estimates the reliability of visual and textual reasoning in a unified text-centric manner and uses robust filtering and voting to prevent error accumulation, significantly improving MLLM reasoning robustness across seven benchmarks.",
      "fullText": "",
      "imageUrl": "",
      "tags": [
        "LLM",
        "Vision",
        "Multimodal",
        "Reasoning"
      ],
      "paperCategory": "Computer Vision",
      "signalReasons": [
        "来源：arXiv",
        "跨源重复：1 个来源",
        "模型评分：9/10，理由：Thinking with Images扩展Chain-of-Thought至多模态推理，为MLLMs提供可解释的视觉-语言协同推理范式，具有重大战略意义。",
        "热度：16 / 评论 0"
      ],
      "score": 9.0,
      "publishedAt": "2026-02-13T13:22:04+00:00",
      "authors": [
        "Haobin Li",
        "Yutong Yang",
        "Yijie Lin"
      ]
    },
    {
      "id": "arxiv_2602_13185v1",
      "title": "FlexAM: Flexible Appearance-Motion Decomposition for Versatile Video Generation Control",
      "titleZh": "FlexAM: Flexible Appearance-Motion Decomposition for Versatile Video Generation Control",
      "titleEn": "FlexAM: Flexible Appearance-Motion Decomposition for Versatile Video Generation Control",
      "url": "https://arxiv.org/abs/2602.13185v1",
      "type": "paper",
      "source": "arXiv",
      "summary": "为实现更通用的视频生成控制，研究者提出FlexAM框架，通过新颖的3D控制信号将视频动态表示为点云，并引入多频位置编码、深度感知编码及灵活控制机制，有效解耦外观与运动，支持图像到视频、视频到视频编辑、摄像机控制及空间对象编辑等多种任务，在多项实验中表现优于现有方法。",
      "summaryZh": "为实现更通用的视频生成控制，研究者提出FlexAM框架，通过新颖的3D控制信号将视频动态表示为点云，并引入多频位置编码、深度感知编码及灵活控制机制，有效解耦外观与运动，支持图像到视频、视频到视频编辑、摄像机控制及空间对象编辑等多种任务，在多项实验中表现优于现有方法。",
      "summaryEn": "To enable versatile and generalizable video generation control, researchers propose FlexAM—a framework that disentangles appearance and motion using a novel 3D control signal represented as a point cloud. Enhanced with multi-frequency and depth-aware positional encoding, FlexAM supports diverse tasks including I2V/V2V editing, camera control, and spatial object manipulation, outperforming existing methods across all evaluated tasks.",
      "fullText": "",
      "imageUrl": "",
      "tags": [
        "3D"
      ],
      "paperCategory": "Computer Vision",
      "signalReasons": [
        "来源：arXiv",
        "跨源重复：1 个来源",
        "模型评分：9/10，理由：FlexAM实现外观与运动的统一解耦，为视频生成提供可扩展、通用的控制路径，是视频生成领域的关键架构创新。",
        "热度：6 / 评论 0"
      ],
      "score": 9.0,
      "publishedAt": "2026-02-13T18:52:11+00:00",
      "authors": [
        "Mingzhi Sheng",
        "Zekai Gu",
        "Peng Li"
      ]
    },
    {
      "id": "arxiv_2602_12322v1",
      "title": "ForeAct: Steering Your VLA with Efficient Visual Foresight Planning",
      "titleZh": "ForeAct: Steering Your VLA with Efficient Visual Foresight Planning",
      "titleEn": "ForeAct: Steering Your VLA with Efficient Visual Foresight Planning",
      "url": "https://arxiv.org/abs/2602.12322v1",
      "type": "paper",
      "source": "arXiv",
      "summary": "研究提出ForeAct——一种高效视觉前瞻规划框架，通过生成高质量的未来图像（640×480分辨率，H100 GPU上仅需0.33秒）和子任务描述，引导视觉-语言-动作（VLA）模型逐步执行复杂指令；该方法无需修改现有VLA架构，仅通过增强视觉输入即可集成，在11项真实多步任务中平均成功率高达87.4%，较基线π₀提升40.9个百分点，显著增强了开放世界环境中VLA的准确性与泛化能力。",
      "summaryZh": "研究提出ForeAct——一种高效视觉前瞻规划框架，通过生成高质量的未来图像（640×480分辨率，H100 GPU上仅需0.33秒）和子任务描述，引导视觉-语言-动作（VLA）模型逐步执行复杂指令；该方法无需修改现有VLA架构，仅通过增强视觉输入即可集成，在11项真实多步任务中平均成功率高达87.4%，较基线π₀提升40.9个百分点，显著增强了开放世界环境中VLA的准确性与泛化能力。",
      "summaryEn": "The paper introduces ForeAct, an efficient visual foresight planning framework that guides Vision-Language-Action (VLA) models step-by-step by generating high-quality future observations (640×480 resolution in just 0.33s on an H100 GPU) and subtask descriptions. It integrates seamlessly with state-of-the-art VLAs by simply augmenting their visual inputs—no architectural changes needed—and achieves an 87.4% average success rate across 11 real-world multi-step tasks, a +40.9% absolute improvement over the π₀ baseline, significantly boosting accuracy and generalization in open-world settings.",
      "fullText": "",
      "imageUrl": "",
      "tags": [
        "LLM",
        "Vision",
        "Multimodal",
        "Robotics"
      ],
      "paperCategory": "Robotics",
      "signalReasons": [
        "来源：arXiv",
        "跨源重复：1 个来源",
        "模型评分：9/10，理由：提出视觉前瞻规划（ForeAct）以引导VLA模型执行开放世界任务，显著提升机器人指令理解与行动能力，具有战略级影响力。",
        "热度：12 / 评论 0"
      ],
      "score": 9.0,
      "publishedAt": "2026-02-12T18:56:27+00:00",
      "authors": [
        "Zhuoyang Zhang",
        "Shang Yang",
        "Qinghao Hu"
      ]
    },
    {
      "id": "arxiv_2602_13028v1",
      "title": "Human-Aligned MLLM Judges for Fine-Grained Image Editing Evaluation: A Benchmark, Framework, and Analysis",
      "titleZh": "Human-Aligned MLLM Judges for Fine-Grained Image Editing Evaluation: A Benchmark, Framework, and Analysis",
      "titleEn": "Human-Aligned MLLM Judges for Fine-Grained Image Editing Evaluation: A Benchmark, Framework, and Analysis",
      "url": "https://arxiv.org/abs/2602.13028v1",
      "type": "paper",
      "source": "arXiv",
      "summary": "针对传统图像编辑评估指标粗糙、难以反映人类感知的问题，研究提出基于多模态大语言模型（MLLM）的细粒度评判框架，将评估分解为涵盖图像保留、编辑质量和指令忠实度的12个可解释因子，并构建了包含人类判断、MLLM评分与模型输出的人类验证基准；实验证明该MLLM评判器在细粒度上高度对齐人类评价，能有效识别过度编辑或语义偏差，为图像编辑模型的开发与比较提供了可靠、可扩展的评估基础。",
      "summaryZh": "针对传统图像编辑评估指标粗糙、难以反映人类感知的问题，研究提出基于多模态大语言模型（MLLM）的细粒度评判框架，将评估分解为涵盖图像保留、编辑质量和指令忠实度的12个可解释因子，并构建了包含人类判断、MLLM评分与模型输出的人类验证基准；实验证明该MLLM评判器在细粒度上高度对齐人类评价，能有效识别过度编辑或语义偏差，为图像编辑模型的开发与比较提供了可靠、可扩展的评估基础。",
      "summaryEn": "To address the coarse and uninterpretable nature of traditional image editing metrics, this work proposes a fine-grained Multimodal Large Language Model (MLLM)-as-a-Judge framework that decomposes evaluation into twelve interpretable factors spanning image preservation, edit quality, and instruction fidelity. It introduces a human-validated benchmark integrating human judgments, MLLM evaluations, model outputs, and conventional metrics. Extensive studies show the MLLM judges closely align with human assessments at fine granularity and reliably detect over-editing or semantic inaccuracies, offering a scalable and trustworthy foundation for evaluating and improving image editing models.",
      "fullText": "",
      "imageUrl": "",
      "tags": [
        "LLM",
        "Vision",
        "Multimodal",
        "Benchmark"
      ],
      "paperCategory": "Computer Vision",
      "signalReasons": [
        "来源：arXiv",
        "跨源重复：1 个来源",
        "模型评分：9/10，理由：构建人类对齐的多模态大模型评估基准，填补图像编辑评价中的人类感知空白，对AI伦理与评测体系有深远影响。",
        "热度：15 / 评论 0"
      ],
      "score": 9.0,
      "publishedAt": "2026-02-13T15:34:32+00:00",
      "authors": [
        "Runzhou Liu",
        "Hailey Weingord",
        "Sejal Mittal"
      ]
    },
    {
      "id": "arxiv_2602_12540v1",
      "title": "Self-Supervised JEPA-based World Models for LiDAR Occupancy Completion and Forecasting",
      "titleZh": "Self-Supervised JEPA-based World Models for LiDAR Occupancy Completion and Forecasting",
      "titleEn": "Self-Supervised JEPA-based World Models for LiDAR Occupancy Completion and Forecasting",
      "url": "https://arxiv.org/abs/2602.12540v1",
      "type": "paper",
      "source": "arXiv",
      "summary": "研究提出AD-LiST-JEPA——一种基于联合嵌入预测架构（JEPA）的自监督世界模型，利用大量未标注LiDAR数据学习自动驾驶环境的时空演化规律；该模型通过下游LiDAR占用补全与预测（OCF）任务验证，证明其预训练编码器能有效提升感知与预测联合性能，为构建可扩展、无需人工标注的自动驾驶世界模型提供了新路径。",
      "summaryZh": "研究提出AD-LiST-JEPA——一种基于联合嵌入预测架构（JEPA）的自监督世界模型，利用大量未标注LiDAR数据学习自动驾驶环境的时空演化规律；该模型通过下游LiDAR占用补全与预测（OCF）任务验证，证明其预训练编码器能有效提升感知与预测联合性能，为构建可扩展、无需人工标注的自动驾驶世界模型提供了新路径。",
      "summaryEn": "The paper presents AD-LiST-JEPA, a self-supervised world model for autonomous driving based on Joint-Embedding Predictive Architecture (JEPA), which learns spatiotemporal environmental dynamics from large-scale unlabeled LiDAR data. Evaluated on the downstream LiDAR Occupancy Completion and Forecasting (OCF) task—a joint assessment of perception and prediction—the pretrained encoder demonstrates improved performance, offering a scalable, annotation-free approach to building world models for autonomous vehicles.",
      "fullText": "",
      "imageUrl": "",
      "tags": [
        "Agent",
        "RAG",
        "Research"
      ],
      "paperCategory": "Computer Vision",
      "signalReasons": [
        "来源：arXiv",
        "跨源重复：1 个来源",
        "模型评分：9/10，理由：基于JEPA的自监督世界模型用于LiDAR占用补全与预测，是自动驾驶长期规划能力的关键突破，具备战略级产业影响。",
        "热度：11 / 评论 0"
      ],
      "score": 9.0,
      "publishedAt": "2026-02-13T02:42:21+00:00",
      "authors": [
        "Haoran Zhu",
        "Anna Choromanska"
      ]
    },
    {
      "id": "arxiv_2602_12820v1",
      "title": "3DLAND: 3D Lesion Abdominal Anomaly Localization Dataset",
      "titleZh": "3DLAND: 3D Lesion Abdominal Anomaly Localization Dataset",
      "titleEn": "3DLAND: 3D Lesion Abdominal Anomaly Localization Dataset",
      "url": "https://arxiv.org/abs/2602.12820v1",
      "type": "paper",
      "source": "arXiv",
      "summary": "为弥补腹部CT医学影像数据集中三维标注缺失、多器官覆盖不足及病灶-器官关联不明确等问题，研究团队发布了3DLAND数据集，包含6000余例增强CT扫描和20000多个高保真3D病灶标注，覆盖肝、肾、胰等七种腹部器官；其三阶段标注流程经放射科专家验证，表面Dice分数超0.75，支持异常检测、定位与跨器官迁移学习，为器官感知型3D分割模型设立新基准，相关数据与代码已开源。",
      "summaryZh": "为弥补腹部CT医学影像数据集中三维标注缺失、多器官覆盖不足及病灶-器官关联不明确等问题，研究团队发布了3DLAND数据集，包含6000余例增强CT扫描和20000多个高保真3D病灶标注，覆盖肝、肾、胰等七种腹部器官；其三阶段标注流程经放射科专家验证，表面Dice分数超0.75，支持异常检测、定位与跨器官迁移学习，为器官感知型3D分割模型设立新基准，相关数据与代码已开源。",
      "summaryEn": "To address gaps in existing abdominal CT datasets—such as missing 3D annotations, limited multi-organ coverage, and unclear lesion-to-organ associations—the team introduces 3DLAND, a large-scale benchmark with over 6,000 contrast-enhanced CT volumes and 20,000+ high-fidelity 3D lesion annotations linked to seven abdominal organs. Validated by radiologists with surface Dice scores >0.75, its three-phase pipeline enables scalable evaluation of anomaly detection, localization, and cross-organ transfer learning. The dataset and code are publicly available, establishing a new standard for organ-aware 3D medical segmentation.",
      "fullText": "",
      "imageUrl": "",
      "tags": [
        "3D",
        "RAG",
        "Reasoning",
        "Research"
      ],
      "paperCategory": "Computer Vision",
      "signalReasons": [
        "来源：arXiv",
        "跨源重复：1 个来源",
        "模型评分：9/10，理由：3DLAND 是首个大规模、多器官、三维精准标注的腹部病变数据集，填补医疗影像关键空白，将推动AI在临床诊断中的落地应用。",
        "热度：12 / 评论 0"
      ],
      "score": 9.0,
      "publishedAt": "2026-02-13T11:08:15+00:00",
      "authors": [
        "Mehran Advand",
        "Zahra Dehghanian",
        "Navid Faraji"
      ]
    },
    {
      "id": "arxiv_2602_12385v1",
      "title": "Zero-Shot Adaptation to Robot Structural Damage via Natural Language-Informed Kinodynamics Modeling",
      "titleZh": "Zero-Shot Adaptation to Robot Structural Damage via Natural Language-Informed Kinodynamics Modeling",
      "titleEn": "Zero-Shot Adaptation to Robot Structural Damage via Natural Language-Informed Kinodynamics Modeling",
      "url": "https://arxiv.org/abs/2602.12385v1",
      "type": "paper",
      "source": "arXiv",
      "summary": "面对机器人结构损伤导致运动学行为变化的挑战，研究提出零样本语言引导运动动力学建模（ZLIK）方法，利用自然语言描述损伤类型并通过自监督学习将其与运动动力学行为对齐；在BeamNG.tech仿真器中，该模型实现对未见损伤类型的零样本适应，运动动力学误差最高降低81%，并成功泛化至模拟到现实及全尺寸到1/10比例的场景。",
      "summaryZh": "面对机器人结构损伤导致运动学行为变化的挑战，研究提出零样本语言引导运动动力学建模（ZLIK）方法，利用自然语言描述损伤类型并通过自监督学习将其与运动动力学行为对齐；在BeamNG.tech仿真器中，该模型实现对未见损伤类型的零样本适应，运动动力学误差最高降低81%，并成功泛化至模拟到现实及全尺寸到1/10比例的场景。",
      "summaryEn": "To address kinodynamic shifts caused by structural damage in robots, the paper proposes Zero-shot Language-Informed Kinodynamics (ZLIK), which uses natural language descriptions of damage and self-supervised learning to ground semantic damage information in kinodynamic behavior. Trained on diverse damaged vehicles in BeamNG.tech, the model achieves zero-shot adaptation to unseen damage types with up to 81% reduction in kinodynamic error and generalizes across sim-to-real and full-to-1/10th scale gaps.",
      "fullText": "",
      "imageUrl": "",
      "tags": [
        "Robotics"
      ],
      "paperCategory": "Robotics",
      "signalReasons": [
        "来源：arXiv",
        "跨源重复：1 个来源",
        "模型评分：9/10，理由：实现零样本适应机器人结构损伤的自然语言驱动动力学建模，突破传统故障容错局限，具备跨平台、跨场景部署潜力，属具身AI关键突破。",
        "热度：10 / 评论 0"
      ],
      "score": 9.0,
      "publishedAt": "2026-02-12T20:28:45+00:00",
      "authors": [
        "Anuj Pokhrel",
        "Aniket Datar",
        "Mohammad Nazeri"
      ]
    },
    {
      "id": "arxiv_2602_12508v1",
      "title": "Monocular Reconstruction of Neural Tactile Fields",
      "titleZh": "Monocular Reconstruction of Neural Tactile Fields",
      "titleEn": "Monocular Reconstruction of Neural Tactile Fields",
      "url": "https://arxiv.org/abs/2602.12508v1",
      "type": "paper",
      "source": "arXiv",
      "summary": "研究首次提出“神经触觉场”——一种从单张RGB图像预测空间位置接触时预期触觉响应的3D表示方法；该方法在体积重建和表面重建上分别比当前最优单目3D重建模型（LRM、Direct3D）提升85.8%和26.7%，并可集成至路径规划器，使机器人主动避开高阻力物体、穿越低阻力区域（如植被），突破传统静态几何占用假设的局限。",
      "summaryZh": "研究首次提出“神经触觉场”——一种从单张RGB图像预测空间位置接触时预期触觉响应的3D表示方法；该方法在体积重建和表面重建上分别比当前最优单目3D重建模型（LRM、Direct3D）提升85.8%和26.7%，并可集成至路径规划器，使机器人主动避开高阻力物体、穿越低阻力区域（如植被），突破传统静态几何占用假设的局限。",
      "summaryEn": "The paper introduces neural tactile fields—a novel 3D representation that maps spatial locations to expected tactile responses upon contact, predicted from a single monocular RGB image, the first method to do so. It improves volumetric and surface 3D reconstruction by 85.8% and 26.7%, respectively, over state-of-the-art monocular methods (LRM, Direct3D). When integrated with path planners, it enables robots to route through low-resistance regions (e.g., foliage) while avoiding high-resistance objects, moving beyond static geometric occupancy assumptions.",
      "fullText": "",
      "imageUrl": "",
      "tags": [
        "Vision",
        "Robotics",
        "3D"
      ],
      "paperCategory": "Robotics",
      "signalReasons": [
        "来源：arXiv",
        "跨源重复：1 个来源",
        "模型评分：8/10，理由：提出神经触觉场用于单目重建变形环境，突破机器人交互感知的静态表征局限，推动具身智能向真实世界适应迈进，具有广泛应用前景。",
        "热度：5 / 评论 0"
      ],
      "score": 8.0,
      "publishedAt": "2026-02-13T01:25:19+00:00",
      "authors": [
        "Pavan Mantripragada",
        "Siddhanth Deshmukh",
        "Eadom Dessalene"
      ]
    },
    {
      "id": "arxiv_2602_13013v1",
      "title": "Towards Universal Video MLLMs with Attribute-Structured and Quality-Verified Instructions",
      "titleZh": "Towards Universal Video MLLMs with Attribute-Structured and Quality-Verified Instructions",
      "titleEn": "Towards Universal Video MLLMs with Attribute-Structured and Quality-Verified Instructions",
      "url": "https://arxiv.org/abs/2602.13013v1",
      "type": "paper",
      "source": "arXiv",
      "summary": "为提升通用视频理解能力，研究构建了ASID-1M——一个包含百万级结构化音视频指令的数据集，每个样本具备单属性或多属性细粒度标注，并配套ASID-Verify自动验证管道确保语义与时序一致性；基于此训练的ASID-Captioner模型在七项基准测试中显著减少幻觉、提升指令遵循能力，在开源模型中达到SOTA水平，性能媲美Gemini-3-Pro。",
      "summaryZh": "为提升通用视频理解能力，研究构建了ASID-1M——一个包含百万级结构化音视频指令的数据集，每个样本具备单属性或多属性细粒度标注，并配套ASID-Verify自动验证管道确保语义与时序一致性；基于此训练的ASID-Captioner模型在七项基准测试中显著减少幻觉、提升指令遵循能力，在开源模型中达到SOTA水平，性能媲美Gemini-3-Pro。",
      "summaryEn": "To advance universal video understanding, the work introduces ASID-1M—a million-scale dataset of structured, fine-grained audiovisual instructions with single- and multi-attribute supervision—and ASID-Verify, a scalable curation pipeline ensuring semantic and temporal consistency. The resulting ASID-Captioner model, trained via supervised fine-tuning, reduces hallucinations, improves instruction following, and achieves state-of-the-art performance among open-source models across seven benchmarks, rivaling Gemini-3-Pro.",
      "fullText": "",
      "imageUrl": "",
      "tags": [
        "LLM",
        "Vision",
        "Audio",
        "Training"
      ],
      "paperCategory": "Computer Vision",
      "signalReasons": [
        "来源：arXiv",
        "跨源重复：1 个来源",
        "模型评分：8/10，理由：推动通用视频多模态大模型发展，通过结构化指令提升理解能力，契合当前多模态融合核心趋势。",
        "热度：20 / 评论 0"
      ],
      "score": 8.0,
      "publishedAt": "2026-02-13T15:20:54+00:00",
      "authors": [
        "Yunheng Li",
        "Hengrui Zhang",
        "Meng-Hao Guo"
      ]
    },
    {
      "id": "arxiv_2602_12407v1",
      "title": "MiDAS: A Multimodal Data Acquisition System and Dataset for Robot-Assisted Minimally Invasive Surgery",
      "titleZh": "MiDAS: A Multimodal Data Acquisition System and Dataset for Robot-Assisted Minimally Invasive Surgery",
      "titleEn": "MiDAS: A Multimodal Data Acquisition System and Dataset for Robot-Assisted Minimally Invasive Surgery",
      "url": "https://arxiv.org/abs/2602.12407v1",
      "type": "paper",
      "source": "arXiv",
      "summary": "针对手术机器人多模态数据获取受限于封闭接口的问题，研究开发了MiDAS——一个开源、平台无关的非侵入式数据采集系统，通过电磁与RGB-D手部追踪、脚踏板传感和手术视频同步记录，无需访问机器人内部接口；在Raven-II和da Vinci Xi平台上验证显示，其外部信号可准确近似内部运动学数据，手势识别性能媲美原厂遥测，并发布了首个包含腹股沟疝修补缝合任务的多模态公开数据集。",
      "summaryZh": "针对手术机器人多模态数据获取受限于封闭接口的问题，研究开发了MiDAS——一个开源、平台无关的非侵入式数据采集系统，通过电磁与RGB-D手部追踪、脚踏板传感和手术视频同步记录，无需访问机器人内部接口；在Raven-II和da Vinci Xi平台上验证显示，其外部信号可准确近似内部运动学数据，手势识别性能媲美原厂遥测，并发布了首个包含腹股沟疝修补缝合任务的多模态公开数据集。",
      "summaryEn": "To overcome proprietary barriers in robot-assisted surgery data collection, the paper presents MiDAS—an open-source, platform-agnostic system that non-invasively synchronizes electromagnetic and RGB-D hand tracking, foot pedal sensing, and surgical video without requiring robot API access. Validated on Raven-II and da Vinci Xi, external signals closely approximate internal kinematics, achieving gesture recognition performance comparable to proprietary telemetry. The release includes the first multimodal dataset of hernia repair suturing on high-fidelity simulators.",
      "fullText": "",
      "imageUrl": "",
      "tags": [
        "Multimodal",
        "Robotics",
        "Research"
      ],
      "paperCategory": "Robotics",
      "signalReasons": [
        "来源：arXiv",
        "跨源重复：1 个来源",
        "模型评分：8/10，理由：发布MiDAS开源多模态手术数据系统，打破机器人手术数据壁垒，极大促进该领域研究生态发展，具备行业变革潜力。",
        "热度：7 / 评论 0"
      ],
      "score": 8.0,
      "publishedAt": "2026-02-12T20:56:15+00:00",
      "authors": [
        "Keshara Weerasinghe",
        "Seyed Hamid Reza Roodabeh",
        "Andrew Hawkins"
      ]
    },
    {
      "id": "arxiv_2602_13193v1",
      "title": "Steerable Vision-Language-Action Policies for Embodied Reasoning and Hierarchical Control",
      "titleZh": "Steerable Vision-Language-Action Policies for Embodied Reasoning and Hierarchical Control",
      "titleEn": "Steerable Vision-Language-Action Policies for Embodied Reasoning and Hierarchical Control",
      "url": "https://arxiv.org/abs/2602.13193v1",
      "type": "paper",
      "source": "arXiv",
      "summary": "该研究提出“可操控策略”（Steerable Policies），通过在多种抽象层级（如子任务、动作和像素坐标）上训练视觉-语言-动作模型（VLA），使其能更精细地响应高层指令，从而更好地利用预训练视觉语言模型（VLM）中的常识知识。实验表明，结合学习型高层推理器或现成VLM进行上下文推理后，该方法在真实世界操作任务中显著优于现有分层控制基线，尤其在长时程和泛化任务上表现突出。",
      "summaryZh": "该研究提出“可操控策略”（Steerable Policies），通过在多种抽象层级（如子任务、动作和像素坐标）上训练视觉-语言-动作模型（VLA），使其能更精细地响应高层指令，从而更好地利用预训练视觉语言模型（VLM）中的常识知识。实验表明，结合学习型高层推理器或现成VLM进行上下文推理后，该方法在真实世界操作任务中显著优于现有分层控制基线，尤其在长时程和泛化任务上表现突出。",
      "summaryEn": "This work introduces Steerable Policies—vision-language-action models (VLAs) trained on rich synthetic commands across multiple abstraction levels (e.g., subtasks, motions, and grounded pixel coordinates)—to enable finer control and better leverage pretrained vision-language models (VLMs). Experiments show that when paired with either a learned high-level embodied reasoner or an off-the-shelf VLM using in-context learning, these policies significantly outperform prior hierarchical baselines in real-world manipulation, especially on long-horizon and generalization tasks.",
      "fullText": "",
      "imageUrl": "",
      "tags": [
        "LLM",
        "Vision",
        "Multimodal",
        "Robotics"
      ],
      "paperCategory": "Robotics",
      "signalReasons": [
        "来源：arXiv",
        "跨源重复：1 个来源",
        "模型评分：8/10，理由：提出可导向的视觉-语言-动作策略框架，推动具身智能与机器人推理的融合，是迈向通用机器人控制的重要一步。",
        "热度：14 / 评论 0"
      ],
      "score": 8.0,
      "publishedAt": "2026-02-13T18:57:56+00:00",
      "authors": [
        "William Chen",
        "Jagdeep Singh Bhatia",
        "Catherine Glossop"
      ]
    },
    {
      "id": "arxiv_2602_12633v1",
      "title": "Real-to-Sim for Highly Cluttered Environments via Physics-Consistent Inter-Object Reasoning",
      "titleZh": "Real-to-Sim for Highly Cluttered Environments via Physics-Consistent Inter-Object Reasoning",
      "titleEn": "Real-to-Sim for Highly Cluttered Environments via Physics-Consistent Inter-Object Reasoning",
      "url": "https://arxiv.org/abs/2602.12633v1",
      "type": "paper",
      "source": "arXiv",
      "summary": "针对高度杂乱环境中机器人操作所需的物理一致性场景重建问题，该论文提出一种基于物理约束的Real-to-Sim流程，利用可微优化与接触图建模，从单视角RGB-D数据联合优化物体位姿与物理属性，并通过可微刚体仿真确保无穿透、无悬浮等物理合理性。实验证明，该方法重建的3D场景能准确复现真实世界的接触动力学，显著提升后续仿真的可靠性与操作稳定性。",
      "summaryZh": "针对高度杂乱环境中机器人操作所需的物理一致性场景重建问题，该论文提出一种基于物理约束的Real-to-Sim流程，利用可微优化与接触图建模，从单视角RGB-D数据联合优化物体位姿与物理属性，并通过可微刚体仿真确保无穿透、无悬浮等物理合理性。实验证明，该方法重建的3D场景能准确复现真实世界的接触动力学，显著提升后续仿真的可靠性与操作稳定性。",
      "summaryEn": "To address physically consistent 3D scene reconstruction for robotic manipulation in highly cluttered environments, this paper proposes a physics-constrained Real-to-Sim pipeline that uses differentiable optimization and a contact graph to jointly refine object poses and physical properties from single-view RGB-D data, ensuring no inter-penetration or floating objects via differentiable rigid-body simulation. Experiments demonstrate that the reconstructed scenes faithfully replicate real-world contact dynamics, enabling stable and reliable downstream manipulation.",
      "fullText": "",
      "imageUrl": "",
      "tags": [
        "Robotics",
        "3D",
        "Reasoning",
        "Benchmark"
      ],
      "paperCategory": "Robotics",
      "signalReasons": [
        "来源：arXiv",
        "跨源重复：1 个来源",
        "模型评分：8/10，理由：在复杂环境中的真实到仿真重建问题上引入物理一致性交互推理，对机器人感知与操作的泛化能力具有重要提升，具备行业应用潜力。",
        "热度：7 / 评论 0"
      ],
      "score": 8.0,
      "publishedAt": "2026-02-13T05:24:58+00:00",
      "authors": [
        "Tianyi Xiang",
        "Jiahang Cao",
        "Sikai Guo"
      ]
    }
  ],
  "news": [
    {
      "id": "rss_5902681501",
      "title": "NVIDIA Blackwell Ultra 推出：AI代理推理成本降35倍",
      "titleZh": "NVIDIA Blackwell Ultra 推出：AI代理推理成本降35倍",
      "titleEn": "NVIDIA Blackwell Ultra Slashes Agentic AI Inference Costs by Up to 35x",
      "url": "https://blogs.nvidia.com/blog/data-blackwell-ultra-performance-lower-cost-agentic-ai/",
      "type": "news",
      "source": "NVIDIA Blog",
      "summary": "NVIDIA最新发布的Blackwell Ultra平台（GB300 NVL72）结合软硬件协同设计，在低延迟、长上下文的AI代理任务中实现高达50倍的每兆瓦吞吐量提升，相比Hopper平台降低最多35倍的每token成本；这一突破使AI编码助手等需实时多步推理的应用得以大规模部署，普通开发者将能以更低价格使用高性能推理服务，推动交互式AI体验普及。",
      "summaryZh": "NVIDIA最新发布的Blackwell Ultra平台（GB300 NVL72）结合软硬件协同设计，在低延迟、长上下文的AI代理任务中实现高达50倍的每兆瓦吞吐量提升，相比Hopper平台降低最多35倍的每token成本；这一突破使AI编码助手等需实时多步推理的应用得以大规模部署，普通开发者将能以更低价格使用高性能推理服务，推动交互式AI体验普及。",
      "summaryEn": "NVIDIA’s new Blackwell Ultra platform (GB300 NVL72), through extreme hardware-software co-design, delivers up to 50x higher throughput per megawatt and up to 35x lower cost per token compared to the Hopper platform for low-latency, long-context agentic AI workloads like coding assistants. This breakthrough enables scalable real-time multi-step AI applications, allowing developers and users to access high-performance inference at dramatically reduced costs.",
      "fullText": "The NVIDIA Blackwell platform has been widely adopted by leading inference providers such as Baseten, DeepInfra, Fireworks AI and Together AI to reduce cost per token by up to 10x. Now, the NVIDIA Blackwell Ultra platform is taking this momentum further for agentic AI. AI agents and coding assistants are driving explosive growth in software-programming-related AI queries: from 11% to about 50% last year, according to OpenRouter’s State of Inference report. These applications require low latency to maintain real-time responsiveness across multistep workflows and long context when reasoning across entire codebases. New SemiAnalysis InferenceX performance data shows that the combination of NVIDIA’s software optimizations and the next-generation NVIDIA Blackwell Ultra platform has delivered breakthrough advances on both fronts. NVIDIA GB300 NVL72 systems now deliver up to 50x higher throughput per megawatt, resulting in 35x lower cost per token compared with the NVIDIA Hopper platform. By innovating across chips, system architecture and software, NVIDIA’s extreme codesign accelerates performance across AI workloads — from agentic coding to interactive coding assistants — while driving down costs at scale. GB300 NVL72 Delivers up to 50x Better Performance for Low-Latency Workloads Recent analysis from Signal65 shows that NVIDIA GB200 NVL72 with extreme hardware and software codesign delivers more than 10x more tokens per watt, resulting in one-tenth the cost per token compared with the NVIDIA Hopper platform. These massive performance gains continue to expand as the underlying stack improves. Continuous optimizations from the NVIDIA TensorRT-LLM, NVIDIA Dynamo, Mooncake and SGLang teams continue to significantly boost Blackwell NVL72 throughput for mixture-of-experts (MoE) inference across all latency targets. For instance, NVIDIA TensorRT-LLM library improvements have delivered up to 5x better performance on GB200 for low-latency workloads compared with just four months ago. Higher-performance GPU kernels optimized for efficiency and low latency help make the most of Blackwell’s immense compute capabilities and boost throughput. NVIDIA NVLink Symmetric Memory enables direct GPU-to-GPU memory access for more efficient communication. Programmatic dependent launch minimizes idle time by launching the next kernel’s setup phase before the previous one completes. Building on these software advances, GB300 NVL72 — which features the Blackwell Ultra GPU — pushes the throughput-per-megawatt frontier to 50x compared with the Hopper platform. This performance gain translates into superior economics, with NVIDIA GB300 lowering costs compared with the Hopper platform across the entire latency spectrum. The most dramatic reduction occurs at low latency, where agentic applications operate: up to 35x lower cost per million tokens compared with the Hopper platform. NVIDIA GB300 NVL72 and the codesigned software stack including NVIDIA Dynamo and TensorRT-LLM deliver 35x lower cost per token compared with NVIDIA Hopper platform. For agentic coding and interactive assistants workloads where every millisecond compounds across multistep workflows, this combination of relentless software optimization and next-generation hardware enables AI platforms to scale real-time interactive experiences to significantly more users. GB300 NVL72 Delivers Superior Economics for Long-Context Workloads While both GB200 NVL72 and GB300 NVL72 efficiently deliver ultralow latency, the distinct advantages of GB300 NVL72 become most apparent in long-context scenarios. For workloads with 128,000-token inputs and 8,000-token outputs — such as AI coding assistants reasoning across codebases — GB300 NVL72 delivers up to 1.5x lower cost per token compared with GB200 NVL72. NVIDIA GB300 NVL72 is ideal for low-latency, long-context workloads. Context grows as the agent reads in more of the code. This allows it to better understand the code base but also requires much more compete. Blackwell Ultra has 1.5x higher NVFP4 compute performance and 2x faster attention processing, enabling the agent to efficiently understand entire code bases. Infrastructure for Agentic AI Leading cloud providers and AI innovators have already deployed NVIDIA GB200 NVL72 at scale, and are also deploying GB300 NVL72 in production. Microsoft, CoreWeave and OCI are deploying GB300 NVL72 for low-latency and long-context use cases such as agentic coding and coding assistants. By reducing token costs, GB300 NVL72 enables a new class of applications that can reason across massive codebases in real time. “As inference moves to the center of AI production, long-context performance and token efficiency become critical,” said Chen Goldberg, senior vice president of engineering at CoreWeave. “Grace Blackwell NVL72 addresses that challenge directly, and CoreWeave’s AI cloud, including CKS and SUNK, is designed to translate GB300 systems’ gains, building on the success of GB200, into predictable performance and cost efficiency. The result is better token economics and more usable inference for customers running workloads at scale.” NVIDIA Vera Rubin NVL72 to Bring Next-Generation Performance With NVIDIA Blackwell systems deployed at scale, continuous software optimizations will keep unlocking additional performance and cost improvements across the installed base. Looking ahead, the NVIDIA Rubin platform — which combines six new chips to create one AI supercomputer — is set to deliver another round of massive performance leaps. For MoE inference, it delivers up to 10x higher throughput per megawatt compared with Blackwell, translating into one-tenth the cost per million tokens. And for the next wave of frontier AI models, Rubin can train large MoE models using just one-fourth the number of GPUs compared with Blackwell. Learn more about the NVIDIA Rubin platform and the Vera Rubin NVL72 system.",
      "imageUrl": "https://blogs.nvidia.com/wp-content/uploads/2026/02/inference-charts-inferencemax-v1.5-perf-charts-4753570-r12_1-alt-scaled.png",
      "tags": [
        "Agent",
        "Inference"
      ],
      "paperCategory": "",
      "signalReasons": [
        "来源权威：官方/白名单",
        "跨源重复：1 个来源",
        "模型评分：9/10，理由：NVIDIA Blackwell Ultra在推理性能与成本上的显著提升，直接推动AI代理和代码助手等关键应用的发展，具有全球产业级战略意义。",
        "热度：0 / 评论 0"
      ],
      "score": 9.5,
      "publishedAt": "2026-02-16T17:00:40+00:00",
      "authors": [
        "Ashraf Eassa"
      ]
    },
    {
      "id": "rss_0951620054",
      "title": "Ricursive Intelligence 四月融3.35亿，估值40亿美元",
      "titleZh": "Ricursive Intelligence 四月融3.35亿，估值40亿美元",
      "titleEn": "Ricursive Intelligence Raises $335M at $4B Valuation in Four Months",
      "url": "https://techcrunch.com/2026/02/16/how-ricursive-intelligence-raised-335m-at-a-4b-valuation-in-4-months/",
      "type": "news",
      "source": "TechCrunch AI",
      "summary": "由前Google Brain与Anthropic核心成员Anna Goldie和Azalia Mirhoseini创立的Ricursive Intelligence在四个月内完成3.35亿美元融资，估值达40亿美元；该公司不造芯片，而是用AI自动化设计各类芯片，目标客户包括NVIDIA、AMD等厂商，其技术有望将芯片设计周期从一年缩短至数小时，并通过持续学习提升设计效率，加速AI硬件与模型的协同进化，最终可能推动AGI发展并大幅降低算力能耗成本。",
      "summaryZh": "由前Google Brain与Anthropic核心成员Anna Goldie和Azalia Mirhoseini创立的Ricursive Intelligence在四个月内完成3.35亿美元融资，估值达40亿美元；该公司不造芯片，而是用AI自动化设计各类芯片，目标客户包括NVIDIA、AMD等厂商，其技术有望将芯片设计周期从一年缩短至数小时，并通过持续学习提升设计效率，加速AI硬件与模型的协同进化，最终可能推动AGI发展并大幅降低算力能耗成本。",
      "summaryEn": "Ricursive Intelligence, founded by ex-Google Brain and Anthropic engineers Anna Goldie and Azalia Mirhoseini, raised $335M at a $4B valuation in just four months. Unlike AI chipmakers, Ricursive uses AI to automate chip design for clients like NVIDIA and AMD, cutting design time from over a year to hours. Its self-improving platform could accelerate co-evolution of AI models and hardware, reduce energy costs, and contribute to AGI development—making powerful, efficient chips more accessible.",
      "fullText": "How Ricursive Intelligence raised $335M at a $4B valuation in 4 months | TechCrunch TechCrunch Desktop Logo TechCrunch Mobile Logo Latest Startups Venture Apple Security AI Apps Events Podcasts Newsletters Search Submit Site Search Toggle Mega Menu Toggle Topics Latest AI Amazon Apps Biotech & Health Climate Cloud Computing Commerce Crypto Enterprise EVs Fintech Fundraising Gadgets Gaming Google Government & Policy Hardware Instagram Layoffs Media & Entertainment Meta Microsoft Privacy Robotics Security Social Space Startups TikTok Transportation Venture More from TechCrunch Staff Events Startup Battlefield StrictlyVC Newsletters Podcasts Videos Partner Content TechCrunch Brand Studio Crunchboard Contact Us Image Credits: Ricursive Intelligence AI How Ricursive Intelligence raised $335M at a $4B valuation in 4 months Julie Bort 9:00 AM PST · February 16, 2026 The co-founders of startup Ricursive Intelligence seemed destined to be co-founders. Anna Goldie, CEO, and Azalia Mirhoseini, CTO, are so well-known in the AI community that they were among those AI engineers who “got those weird emails from Zuckerberg making crazy offers to us,” Goldie told TechCrunch, chuckling. (They didn’t take the offers.) The pair worked at Google Brain together and were early employees at Anthropic. They earned acclaim at Google by creating the Alpha Chip — an AI tool that could generate solid chip layouts in hours — a process that normally takes human designers a year or more. The tool helped design three generations of Google’s Tensor Processing Units. That pedigree explains why, just four months after launching Ricursive, they last month announced a $300 million Series A round at a $4 billion valuation led by Lightspeed, just a couple of months after raising a $35 million seed round led by Sequoia. Ricursive is building AI tools that design chips, not the chips themselves. That makes them fundamentally different from nearly every other AI chip startup: they’re not a wannabe Nvidia competitor. In fact, Nvidia is an investor. The GPU giant, along with AMD, Intel, and every other chip maker, are the startup’s target customers. “We want to enable any chip, like a custom chip or a more traditional chip, any kind of chip, to be built in an automated and very accelerated way. We’re using AI to do that,” Mirhoseini told TechCrunch. Their paths first crossed at Stanford, where Goldie earned her PhD as Mirhoseini taught computer science classes. Since then, their careers have been in lockstep. “We started at Google Brain on the same day. We left Google Brain on the same day. We joined Anthropic on the same day. We left Anthropic on the same day. We rejoined Google on the same day, and then we left Google again on the same day. Then we started this company together on the same day,” Goldie recounted. Techcrunch event TechCrunch Founder Summit 2026: Tickets Live On June 23 in Boston , more than 1,100 founders come together at TechCrunch Founder Summit 2026 for a full day focused on growth, execution, and real-world scaling. Learn from founders and investors who have shaped the industry. Connect with peers navigating similar growth stages. Walk away with tactics you can apply immediately Save up to $300 on your pass or save up to 30% with group tickets for teams of four or more. TechCrunch Founder Summit: Tickets Live On June 23 in Boston , more than 1,100 founders come together at TechCrunch Founder Summit 2026 for a full day focused on growth, execution, and real-world scaling. Learn from founders and investors who have shaped the industry. Connect with peers navigating similar growth stages. Walk away with tactics you can apply immediately Save up to $300 on your pass or save up to 30% with group tickets for teams of four or more. Boston, MA | June 23, 2026 REGISTER NOW During their time at Google, the colleagues were so close they even worked out together, both enjoying circuit training. The pun wasn’t lost on Jeff Dean, the famed Google engineer who was their collaborator. He nicknamed their Alpha Chip project “chip circuit training” — a play on their shared workout routine. Internally, the pair also got a nickname: A&A. The Alpha Chip earned them industry notice, but it also attracted controversy. In 2022, one of their colleagues at Google was fired, Wired reported , after he spent years trying to discredit A&A and their chip work, even though that work was used to help produce some of Google’s most important, bet-the-business AI chips . Their Alpha Chip project at Google Brain proved the concept that would become Ricursive — using AI to dramatically accelerate chip design. Designing chips is hard The issue is, computer chips have millions to billions of logic gate components integrated on their silicon wafer. Human designers can spend a year or more placing those components on the chip to ensure performance, good power utilization and any other design needs. Digitally determining the placement of such infinitesimally small components with precision is, as you might expect, hard. Alpha Chip “could generate a very high-quality layout in, like, six hours. And the cool thing about this approach was that it actually learns from experience,” Goldie said. The premise of their AI chip design work is to use “a reward signal” that rates how good the design is. The agent then takes that rating to “update the parameters of its deep neural network to get better,” Goldie said. After completing thousands of designs, the agent got really good. It also got faster as it learned, the founders say. Ricursive’s platform will take the concept further. The AI chip designer they are building will “learn across different chips,” Goldie said. So each chip it designs should help it become a better designer for every next chip. Ricursive’s platform also makes use of LLMs and will handle everything from component placement through design verification. Any company that makes electronics and needs chips is their target customer. If their platform proves itself, as it seems likely to do, Ricursive could play a role in the moonshot goal of achieving artificial general intelligence (AGI). Indeed, their ultimate vision is designing AI chips, meaning the AI will essentially design its own computer brains. “Chips are the fuel for AI,” Goldie said. “I think by building more powerful chips, that’s the best way to advance that frontier.” Mirhoseini adds that the lengthy chip-design process is constraining how quickly AI can advance. “We think we can also enable this fast co-evolution of the models and the chips that basically power them,” she said. So AI can grow smarter faster. If the thought of AI designing its own brains at ever increasing speeds brings visions of Skynet and the Terminator to mind, the founders point out that there’s a more positive, immediate and, they think, more likely benefit: hardware efficiency. When AI Labs can design far more efficient chips (and, eventually all the underlying hardware), their growth won’t have to consume so much of the world’s resources. “We could design a computer architecture that’s uniquely suited to that model, and we could achieve almost a 10x improvement in performance per total cost of ownership,” Goldie said. While the young startup won’t name its early customers, the founders say that they’ve heard from every big chip making name you can imagine. Unsurprisingly, they have their pick of their first development partners, too. Topics AI , AI chips , Hardware , Ricursive Intelligence , TC Julie Bort Venture Editor Julie Bort is the Startups/Venture Desk editor for TechCrunch. You can contact or verify outreach from Julie by emailing julie.bort@techcrunch.com or via @Julie188 on X. View Bio October 13-15 San Francisco, CA Tickets are live at the lowest rates of the year. Save up to $680 on your pass now. Meet investors. Discover your next portfolio company. Hear from 250+ tech leaders , dive into 200+ sessions , and explore 300+ startups building what’s next. Don’t miss these one-time savings. REGISTER NOW Most Popular OpenClaw creator Peter Steinberger joins OpenAI Anthony Ha Hollywood isn’t happy about the new Seedance 2.0 video generator Anthony Ha The great computer science exodus (and where students are going instead) Connie Loizos A Stanford grad student created an algorithm to help his classmates find love; now, Date Drop is the basis of his new startup Amanda Silberling Spotify says its best developers haven’t written a line of code since December, thanks to AI Sarah Perez With co-founders leaving and an IPO looming, Elon Musk turns talk to the moon Connie Loizos Former GitHub CEO raises record $60M dev tool seed round at $300M valuation Julie Bort Loading the next article Error loading the next article X LinkedIn Facebook Instagram youTube Mastodon Threads Bluesky TechCrunch Staff Contact Us Advertise Crunchboard Jobs Site Map Terms of Service Privacy Policy RSS Terms of Use Code of Conduct Epstein Kindle Scribe Reddit TikTok GPT-4o Tech Layoffs ChatGPT © 2025 TechCrunch Media LLC.",
      "imageUrl": "https://techcrunch.com/wp-content/uploads/2026/02/Ricursive-Intelligence-founders.png?resize=1200%2C630",
      "tags": [],
      "paperCategory": "",
      "signalReasons": [
        "来源：TechCrunch AI",
        "跨源重复：1 个来源",
        "模型评分：10/10，理由：Ricursive Intelligence以顶尖人才为核心，在4个月内完成3.35亿美元融资并估值40亿美元，是AI初创领域里程碑式融资事件。",
        "热度：0 / 评论 0"
      ],
      "score": 6.0,
      "publishedAt": "2026-02-16T17:00:00+00:00",
      "authors": [
        "Julie Bort"
      ]
    },
    {
      "id": "rss_7175387020",
      "title": "字节跳动将加强Seedance 2.0版权保护",
      "titleZh": "字节跳动将加强Seedance 2.0版权保护",
      "titleEn": "ByteDance to Bolster Copyright Safeguards on Seedance 2.0 After Hollywood Backlash",
      "url": "https://www.theverge.com/ai-artificial-intelligence/879644/bytedance-seedance-safeguards-ai-video-copyright-infringement",
      "type": "news",
      "source": "The Verge AI",
      "summary": "字节跳动旗下TikTok开发的Seedance 2.0 AI视频生成模型因生成汤姆·克鲁斯与布拉德·皮特打斗等超写实侵权内容引发迪士尼、派拉蒙及好莱坞工会强烈抗议，公司已宣布将加强版权保护机制；该事件凸显生成式AI在肖像权与IP合规方面的风险，普通用户应警惕滥用此类工具可能带来的法律后果，行业亟需建立内容溯源与授权框架。",
      "summaryZh": "字节跳动旗下TikTok开发的Seedance 2.0 AI视频生成模型因生成汤姆·克鲁斯与布拉德·皮特打斗等超写实侵权内容引发迪士尼、派拉蒙及好莱坞工会强烈抗议，公司已宣布将加强版权保护机制；该事件凸显生成式AI在肖像权与IP合规方面的风险，普通用户应警惕滥用此类工具可能带来的法律后果，行业亟需建立内容溯源与授权框架。",
      "summaryEn": "ByteDance is strengthening safeguards on its Seedance 2.0 AI video generator after Disney, Paramount, and Hollywood unions accused it of copyright violations due to hyperrealistic deepfakes featuring Tom Cruise, Brad Pitt, and animated characters. The backlash highlights urgent legal risks around likeness rights and IP in generative AI, urging users to avoid unauthorized use and pushing the industry toward robust content attribution and licensing frameworks.",
      "fullText": "Hollywood groups have spoken out against Seedance 2.0 since generated videos like fight scenes between Tom Cruise and Brad Pitt went viral. | Image by Dave Benett / WireImage via Getty TikTok creator ByteDance says that it is working to improve safeguards on its new AI video generator after Disney, Paramount, and Hollywood trade groups accused the tool of violating copyright protections. Concerns were raised after hyperrealistic videos generated by the Seedance 2.0 model went viral last week, with the likeness of actors, such as Tom Cruise and Brad Pitt, and characters from Dragon Ball Z, Family Guy, and Pok&eacute;mon. \"ByteDance respects intellectual property rights and we have heard the concerns regarding Seedance 2.0,\" a ByteDance spokesperson said in a statement shared by CNBC. \"We are taking steps to strengthen current saf … Read the full story at The Verge.",
      "imageUrl": "https://platform.theverge.com/wp-content/uploads/sites/2/2026/02/gettyimages-2221026706.jpg?quality=90&strip=all&crop=0%2C10.705478127883%2C100%2C78.589043744234&w=1200",
      "tags": [],
      "paperCategory": "",
      "signalReasons": [
        "来源：The Verge AI",
        "跨源重复：1 个来源",
        "模型评分：9/10，理由：ByteDance AI视频生成模型引发好莱坞版权争议，涉及全球内容产业核心利益，推动AI伦理与法律边界讨论，具备战略级影响。",
        "热度：0 / 评论 0"
      ],
      "score": 5.4,
      "publishedAt": "2026-02-16T11:29:24+00:00",
      "authors": [
        "Jess Weatherbed"
      ]
    },
    {
      "id": "rss_5617371312",
      "title": "印度AI峰会：11亿美元基金启动，巨头齐聚共建生态",
      "titleZh": "印度AI峰会：11亿美元基金启动，巨头齐聚共建生态",
      "titleEn": "India Launches $1.1B AI Fund at Inaugural AI Impact Summit with Global Tech Leaders",
      "url": "https://techcrunch.com/2026/02/16/all-the-important-news-from-the-ongoing-india-ai-summit/",
      "type": "news",
      "source": "TechCrunch AI",
      "summary": "印度举办首届AI Impact Summit，吸引OpenAI、Anthropic、NVIDIA、Google等科技巨头高管及多国政要参与；会上宣布设立11亿美元国家风投基金支持AI与先进制造，并披露ChatGPT在印周活用户超1亿、仅次于美国，同时Anthropic将在班加罗尔设首个印度办公室；此举标志着印度正系统性构建本土AI生态，普通开发者与企业将获得更多本地化算力与政策支持。",
      "summaryZh": "印度举办首届AI Impact Summit，吸引OpenAI、Anthropic、NVIDIA、Google等科技巨头高管及多国政要参与；会上宣布设立11亿美元国家风投基金支持AI与先进制造，并披露ChatGPT在印周活用户超1亿、仅次于美国，同时Anthropic将在班加罗尔设首个印度办公室；此举标志着印度正系统性构建本土AI生态，普通开发者与企业将获得更多本地化算力与政策支持。",
      "summaryEn": "India’s inaugural AI Impact Summit drew leaders from OpenAI, Anthropic, NVIDIA, Google, and heads of state, unveiling a $1.1B national VC fund for AI and advanced manufacturing. With over 100 million weekly ChatGPT users in India—second only to the U.S.—and Anthropic opening its first Indian office in Bengaluru, the event signals India’s strategic push to build a domestic AI ecosystem, offering local developers and enterprises enhanced infrastructure and policy backing.",
      "fullText": "All the important news from the ongoing India AI Impact Summit | TechCrunch TechCrunch Desktop Logo TechCrunch Mobile Logo Latest Startups Venture Apple Security AI Apps Events Podcasts Newsletters Search Submit Site Search Toggle Mega Menu Toggle Topics Latest AI Amazon Apps Biotech & Health Climate Cloud Computing Commerce Crypto Enterprise EVs Fintech Fundraising Gadgets Gaming Google Government & Policy Hardware Instagram Layoffs Media & Entertainment Meta Microsoft Privacy Robotics Security Social Space Startups TikTok Transportation Venture More from TechCrunch Staff Events Startup Battlefield StrictlyVC Newsletters Podcasts Videos Partner Content TechCrunch Brand Studio Crunchboard Contact Us Image Credits: Jagmeet Singh / TechCrunch AI All the important news from the ongoing India AI Impact Summit Ivan Mehta 3:20 AM PST · February 16, 2026 With an eye towards luring more AI investment to the country, India is hosting a four-day AI Impact Summit this week that will be attended by executives from major AI labs and Big Tech, including OpenAI, Anthropic, Nvidia, Microsoft, Google, and Cloudflare, as well as heads of state. The event, which expects 250,000 visitors, will see Alphabet CEO Sundar Pichai, OpenAI CEO Sam Altman, Anthropic CEO Dario Amodei, Reliance Chairman Mukesh Ambani, and Google DeepMind CEO Demis Hassabis in attendance. India’s prime minister, Narendra Modi, is scheduled to deliver a speech with French President Emmanuel Macron on Thursday. Here are all the key updates from the event: India earmarks $1.1 billion for its state-backed venture capital fund . The fund will invest in artificial intelligence and advanced manufacturing startups across the country. OpenAI CEO Sam Altman said India accounts for more than 100 million weekly active ChatGPT users , second only to the U.S. He also said Indians also account for the most students using ChatGPT. Blackstone has picked up a majority stake in Indian AI startup Neysa as part of a $600 million equity fundraise . Teachers’ Venture Growth, TVS Capital, 360 ONE Asset, and Nexus Venture Partners also invested. The company now plans to raise another $600 million in debt, and deploy more than 20,000 GPUs. Bengaluru-based C2i, which is building a power solution for data centers, raised $15 million in a Series A round from Peak XV , with participation from Yali Deeptech and TDK Ventures. HCL CEO Vineet Nayyar said Indian IT companies will focus on turning profits and not being job creators . These comments come as Indian IT stocks dip as fears of AI disrupting the IT services sector burgeon. Vinod Khosla, founder of Khosla Ventures, said that industries like IT services and BPOs (Business Process Outsourcing) can “almost completely disappear” within five years because of AI. He told Hindustan Times that 250 million young people in India should be selling AI-based products and services to the rest of the world. AMD is teaming up with Tata Consultancy Services (TCS) to develop rack-scale AI infrastructure based on AMD’s “Helios” platform. Anthropic said that it is opening its first office in India in the city of Bengaluru. The company said that the country is the second biggest user of Claude afte the U.S. Topics AI , Anthropic , Google , Government & Policy , India , India AI summit , Microsoft , nvidia , OpenAI Ivan Mehta Ivan covers global consumer tech developments at TechCrunch. He is based out of India and has previously worked at publications including Huffington Post and The Next Web. You can contact or verify outreach from Ivan by emailing im@ivanmehta.com or via encrypted message at ivan.42 on Signal. View Bio October 13-15 San Francisco, CA Tickets are live at the lowest rates of the year. Save up to $680 on your pass now. Meet investors. Discover your next portfolio company. Hear from 250+ tech leaders , dive into 200+ sessions , and explore 300+ startups building what’s next. Don’t miss these one-time savings. REGISTER NOW Most Popular OpenClaw creator Peter Steinberger joins OpenAI Anthony Ha Hollywood isn’t happy about the new Seedance 2.0 video generator Anthony Ha The great computer science exodus (and where students are going instead) Connie Loizos A Stanford grad student created an algorithm to help his classmates find love; now, Date Drop is the basis of his new startup Amanda Silberling Spotify says its best developers haven’t written a line of code since December, thanks to AI Sarah Perez With co-founders leaving and an IPO looming, Elon Musk turns talk to the moon Connie Loizos Former GitHub CEO raises record $60M dev tool seed round at $300M valuation Julie Bort Loading the next article Error loading the next article X LinkedIn Facebook Instagram youTube Mastodon Threads Bluesky TechCrunch Staff Contact Us Advertise Crunchboard Jobs Site Map Terms of Service Privacy Policy RSS Terms of Use Code of Conduct Epstein Kindle Scribe Reddit TikTok GPT-4o Tech Layoffs ChatGPT © 2025 TechCrunch Media LLC.",
      "imageUrl": "https://techcrunch.com/wp-content/uploads/2024/06/india-ai.jpg?w=1200",
      "tags": [
        "Industry"
      ],
      "paperCategory": "",
      "signalReasons": [
        "来源：TechCrunch AI",
        "跨源重复：1 个来源",
        "模型评分：9/10，理由：印度AI影响力峰会汇聚全球科技巨头与政要，标志新兴经济体在AI战略布局中的崛起，具有全球产业格局重塑意义。",
        "热度：0 / 评论 0"
      ],
      "score": 5.4,
      "publishedAt": "2026-02-16T11:20:27+00:00",
      "authors": [
        "Ivan Mehta"
      ]
    },
    {
      "id": "rss_8782276460",
      "title": "黑石12亿美元押注印度AI基建，Neysa扩产至2万GPU",
      "titleZh": "黑石12亿美元押注印度AI基建，Neysa扩产至2万GPU",
      "titleEn": "Blackstone Bets $1.2B on Indian AI Infrastructure as Neysa Scales to 20,000 GPUs",
      "url": "https://techcrunch.com/2026/02/15/blackstone-backs-neysa-in-up-to-1-2b-financing-as-india-pushes-to-build-domestic-ai-compute/",
      "type": "news",
      "source": "TechCrunch AI",
      "summary": "印度AI基础设施初创公司Neysa获黑石领投的12亿美元融资（含6亿股权+6亿债权），计划将GPU部署规模从当前1200台扩展至超2万台，以满足本地AI算力需求；随着印度GPU总量预计从6万增至200万，Neysa专注为政府与企业提供定制化、高响应支持的本地化AI基础设施，普通企业将能更快获得合规、低延迟的AI训练与部署服务。",
      "summaryZh": "印度AI基础设施初创公司Neysa获黑石领投的12亿美元融资（含6亿股权+6亿债权），计划将GPU部署规模从当前1200台扩展至超2万台，以满足本地AI算力需求；随着印度GPU总量预计从6万增至200万，Neysa专注为政府与企业提供定制化、高响应支持的本地化AI基础设施，普通企业将能更快获得合规、低延迟的AI训练与部署服务。",
      "summaryEn": "Indian AI infrastructure startup Neysa secured up to $1.2B in financing led by Blackstone—including $600M equity and $600M debt—to scale GPU deployments from 1,200 to over 20,000 units. As India’s total GPU count is projected to surge from 60,000 to over 2 million, Neysa aims to provide localized, high-support AI infrastructure for enterprises and government agencies, enabling faster, compliant, and low-latency model training and deployment for Indian businesses.",
      "fullText": "Blackstone backs Neysa in up to $1.2B financing as India pushes to build domestic AI infrastructure | TechCrunch TechCrunch Desktop Logo TechCrunch Mobile Logo Latest Startups Venture Apple Security AI Apps Events Podcasts Newsletters Search Submit Site Search Toggle Mega Menu Toggle Topics Latest AI Amazon Apps Biotech & Health Climate Cloud Computing Commerce Crypto Enterprise EVs Fintech Fundraising Gadgets Gaming Google Government & Policy Hardware Instagram Layoffs Media & Entertainment Meta Microsoft Privacy Robotics Security Social Space Startups TikTok Transportation Venture More from TechCrunch Staff Events Startup Battlefield StrictlyVC Newsletters Podcasts Videos Partner Content TechCrunch Brand Studio Crunchboard Contact Us Image Credits: Mark Abramson / Bloomberg / Getty Images AI Blackstone backs Neysa in up to $1.2B financing as India pushes to build domestic AI infrastructure Jagmeet Singh 4:30 PM PST · February 15, 2026 Neysa , an Indian AI infrastructure startup, has secured backing from U.S. private equity firm Blackstone as it scales domestic compute capacity amid India’s push to build homegrown AI capabilities . Blackstone and co-investors, including Teachers’ Venture Growth, TVS Capital, 360 ONE Asset, and Nexus Venture Partners, have agreed to invest up to $600 million of primary equity in Neysa, giving Blackstone a majority stake, Blackstone and Neysa told TechCrunch. The Mumbai-headquartered startup also plans to raise an additional $600 million in debt financing as it expands GPU capacity, a sharp increase from the $50 million it had raised previously. The deal comes as demand for AI computing surges globally , creating supply constraints for specialized chips and data center capacity needed to train and run large models. Newer AI-focused infrastructure providers — often referred to as “neo-clouds” — have emerged to bridge that gap by offering dedicated GPU capacity and faster deployment than traditional hyperscalers, particularly for enterprises and AI labs with specific regulatory, latency, or customisation requirements. Neysa operates in this emerging segment , positioning itself as a provider of customized, GPU-first infrastructure for enterprises, government agencies, and AI developers in India, where demand for local compute is still at an early but rapidly expanding stage. “A lot of customers want hand-holding, and a lot of them want round-the-clock support with a 15-minute response and a couple of our resolutions. And so those are the kinds of things that we provide that some of the hyperscalers don’t,” said Neysa co-founder and CEO Sharad Sanghi. Nesya co-founder and CEO Sharad Sanghi Image Credits: Neysa Ganesh Mani, a senior managing director at Blackstone Private Equity, said his firm estimates that India currently has fewer than 60,000 GPUs deployed — and it expects the figure to scale up nearly 30 times to more than two million in the coming years. That expansion is being driven by a combination of government demand, enterprises in regulated sectors such as financial services and healthcare that need to keep data local, and AI developers building models within India, Mani told TechCrunch. Global AI labs, many of which count India among their largest user bases , are also increasingly looking to deploy computing capacity closer to users to reduce latency and meet data requirements. Techcrunch event TechCrunch Founder Summit 2026: Tickets Live On June 23 in Boston , more than 1,100 founders come together at TechCrunch Founder Summit 2026 for a full day focused on growth, execution, and real-world scaling. Learn from founders and investors who have shaped the industry. Connect with peers navigating similar growth stages. Walk away with tactics you can apply immediately Save up to $300 on your pass or save up to 30% with group tickets for teams of four or more. TechCrunch Founder Summit: Tickets Live On June 23 in Boston , more than 1,100 founders come together at TechCrunch Founder Summit 2026 for a full day focused on growth, execution, and real-world scaling. Learn from founders and investors who have shaped the industry. Connect with peers navigating similar growth stages. Walk away with tactics you can apply immediately Save up to $300 on your pass or save up to 30% with group tickets for teams of four or more. Boston, MA | June 23, 2026 REGISTER NOW The investment also builds on Blackstone’s broader push into data center and AI infrastructure globally. The firm has previously backed large-scale data centre platforms such as QTS and AirTrunk, as well as specialized AI infrastructure providers including CoreWeave in the U.S. and Firmus in Australia. Neysa develops and operates GPU-based AI infrastructure that enables enterprises, researchers, and public sector clients to train, fine-tune, and deploy AI models locally. The startup currently has about 1,200 GPUs live and plans to sharply scale that capacity, targeting deployments of more than 20,000 GPUs over time as customer demand accelerates. “We are seeing a demand that we are going to more than triple our capacity next year,” Sanghi said. “Some of the conversations we are having are at a fairly advanced stage; if they go through, then we could see it sooner rather than later. We could see in the next nine months.” Sanghi told TechCrunch that the bulk of the new capital will be used to deploy large-scale GPU clusters, including compute, networking and storage, while a smaller portion will go toward research and development and building out Neysa’s software platforms for orchestration, observability, and security. Neysa aims to more than triple its revenue next year as demand for AI workloads accelerates, with ambitions to expand beyond India over time, Sanghi said. Founded in 2023, the startup employs 110 people across offices in Mumbai, Bengaluru, and Chennai. Topics AI , Blackstone , India , Neysa , Startups Jagmeet Singh Reporter Jagmeet covers startups, tech policy-related updates, and all other major tech-centric developments from India for TechCrunch. He previously worked as a principal correspondent at NDTV. You can contact or verify outreach from Jagmeet by emailing mail@journalistjagmeet.com . View Bio October 13-15 San Francisco, CA Tickets are live at the lowest rates of the year. Save up to $680 on your pass now. Meet investors. Discover your next portfolio company. Hear from 250+ tech leaders , dive into 200+ sessions , and explore 300+ startups building what’s next. Don’t miss these one-time savings. REGISTER NOW Most Popular OpenClaw creator Peter Steinberger joins OpenAI Anthony Ha Hollywood isn’t happy about the new Seedance 2.0 video generator Anthony Ha The great computer science exodus (and where students are going instead) Connie Loizos A Stanford grad student created an algorithm to help his classmates find love; now, Date Drop is the basis of his new startup Amanda Silberling Spotify says its best developers haven’t written a line of code since December, thanks to AI Sarah Perez With co-founders leaving and an IPO looming, Elon Musk turns talk to the moon Connie Loizos Former GitHub CEO raises record $60M dev tool seed round at $300M valuation Julie Bort Loading the next article Error loading the next article X LinkedIn Facebook Instagram youTube Mastodon Threads Bluesky TechCrunch Staff Contact Us Advertise Crunchboard Jobs Site Map Terms of Service Privacy Policy RSS Terms of Use Code of Conduct Epstein Kindle Scribe Reddit TikTok GPT-4o Tech Layoffs ChatGPT © 2025 TechCrunch Media LLC.",
      "imageUrl": "https://techcrunch.com/wp-content/uploads/2021/07/GettyImages-1156074432.jpg?resize=1200%2C630",
      "tags": [],
      "paperCategory": "",
      "signalReasons": [
        "来源：TechCrunch AI",
        "跨源重复：1 个来源",
        "模型评分：9/10，理由：Blackstone领投12亿美元支持印度本土AI算力基建，标志着全球AI基础设施布局向新兴市场转移，具有重大战略意义。",
        "热度：0 / 评论 0"
      ],
      "score": 5.4,
      "publishedAt": "2026-02-16T00:30:00+00:00",
      "authors": [
        "Jagmeet Singh"
      ]
    },
    {
      "id": "rss_6320730280",
      "title": "C2i获1500万美元融资，破解AI数据中心供电瓶颈",
      "titleZh": "C2i获1500万美元融资，破解AI数据中心供电瓶颈",
      "titleEn": "C2i Raises $15M to Tackle AI Data Center Power Loss with Grid-to-GPU Solution",
      "url": "https://techcrunch.com/2026/02/15/as-ai-data-centers-hit-power-limits-peak-xv-backs-indian-startup-c2i-to-fix-the-bottleneck/",
      "type": "news",
      "source": "TechCrunch AI",
      "summary": "印度初创公司C2i获Peak XV等1500万美元投资，开发“电网到GPU”一体化电源方案，通过系统级优化将数据中心电力转换损耗从15–20%降至约10%，每兆瓦节省100千瓦；随着AI数据中心能耗激增，该技术有望显著降低运营成本，普通用户未来可能受益于更便宜、更绿色的AI服务，而印度半导体设计生态也借此加速走向全球竞争。",
      "summaryZh": "印度初创公司C2i获Peak XV等1500万美元投资，开发“电网到GPU”一体化电源方案，通过系统级优化将数据中心电力转换损耗从15–20%降至约10%，每兆瓦节省100千瓦；随着AI数据中心能耗激增，该技术有望显著降低运营成本，普通用户未来可能受益于更便宜、更绿色的AI服务，而印度半导体设计生态也借此加速走向全球竞争。",
      "summaryEn": "Indian startup C2i raised $15M from Peak XV Partners to develop an integrated “grid-to-GPU” power delivery system that reduces energy loss in AI data centers from 15–20% to ~10%, saving 100 kW per megawatt. As data center power demand surges globally, this efficiency gain could lower AI service costs and carbon footprints for end users, while signaling India’s growing capability to produce globally competitive semiconductor solutions beyond captive design centers.",
      "fullText": "As AI data centers hit power limits, Peak XV backs Indian startup C2i to fix the bottleneck | TechCrunch TechCrunch Desktop Logo TechCrunch Mobile Logo Latest Startups Venture Apple Security AI Apps Events Podcasts Newsletters Search Submit Site Search Toggle Mega Menu Toggle Topics Latest AI Amazon Apps Biotech & Health Climate Cloud Computing Commerce Crypto Enterprise EVs Fintech Fundraising Gadgets Gaming Google Government & Policy Hardware Instagram Layoffs Media & Entertainment Meta Microsoft Privacy Robotics Security Social Space Startups TikTok Transportation Venture More from TechCrunch Staff Events Startup Battlefield StrictlyVC Newsletters Podcasts Videos Partner Content TechCrunch Brand Studio Crunchboard Contact Us Image Credits: sdecoret / Getty Images AI As AI data centers hit power limits, Peak XV backs Indian startup C2i to fix the bottleneck Jagmeet Singh 5:00 PM PST · February 15, 2026 Power, rather than compute, is fast becoming the limiting factor in scaling AI data centers. That shift has prompted Peak XV Partners to back C2i Semiconductors , an Indian startup building plug-and-play, system-level power solutions designed to cut energy losses and improve the economics of large-scale AI infrastructure. C2i (which stands for control conversion and intelligence) has raised $15 million in a Series A round led by Peak XV Partners, with participation from Yali Deeptech and TDK Ventures, bringing the two-year-old startup’s total funding to $19 million. The investment comes as data-center energy demand accelerates worldwide. Electricity consumption from data centers is projected to nearly triple by 2035 , per a December 2025 report from BloombergNEF, while Goldman Sachs Research estimates data-center power demand could surge 175% by 2030 from 2023 levels — the equivalent of adding another top-10 power-consuming country. Much of that strain comes not from generating electricity but from converting it efficiently inside data centers, where high-voltage power must be stepped down thousands of times before it reaches GPUs. This process currently wastes about 15% to 20% of energy, C2i’s co-founder and CTO Preetam Tadeparthy said in an interview. “What used to be 400 volts has already moved to 800 volts, and will likely go higher,” Tadeparthy told TechCrunch. Founded in 2024 by former Texas Instruments power executives Ram Anant, Vikram Gakhar, Preetam Tadeparthy, and Dattatreya Suryanarayana, along with Harsha S. B and Muthusubramanian N. V, C2i is redesigning power delivery as a single, plug-and-play “grid-to-GPU” system spanning the data-center bus to the processor itself. C2i co-founders Vikram Gakhar, Preetam Tadeparthy, Ram Anant, and Dattatreya Suryanarayana (Left to right) Image Credits: C2i By treating power conversion, control and packaging as an integrated platform, C2i estimates it can cut end-to-end losses by around 10% — roughly 100 kilowatts saved for every megawatt consumed — with knock-on effects for cooling costs, GPU utilisation and overall data-center economics. Techcrunch event TechCrunch Founder Summit 2026: Tickets Live On June 23 in Boston , more than 1,100 founders come together at TechCrunch Founder Summit 2026 for a full day focused on growth, execution, and real-world scaling. Learn from founders and investors who have shaped the industry. Connect with peers navigating similar growth stages. Walk away with tactics you can apply immediately Save up to $300 on your pass or save up to 30% with group tickets for teams of four or more. TechCrunch Founder Summit: Tickets Live On June 23 in Boston , more than 1,100 founders come together at TechCrunch Founder Summit 2026 for a full day focused on growth, execution, and real-world scaling. Learn from founders and investors who have shaped the industry. Connect with peers navigating similar growth stages. Walk away with tactics you can apply immediately Save up to $300 on your pass or save up to 30% with group tickets for teams of four or more. Boston, MA | June 23, 2026 REGISTER NOW “All that translates directly to total cost of ownership, revenue, and profitability,” Tadeparthy said. For Peak XV Partners (which split from Sequoia Capital in 2023), the attraction lies in how power costs shape the economics of AI infrastructure at scale. Rajan Anandan, the venture firm’s managing director, told TechCrunch that after the upfront capital investment in servers and facilities, energy costs become the dominant ongoing expense for data centers, making even incremental efficiency gains highly valuable. “If you can reduce energy costs by, call it, 10 to 30%, that’s like a huge number,” Anandan said. “You’re talking about tens of billions of dollars.” The claims will be tested quickly. C2i expects its first two silicon designs to return from fabrication between April and June, after which the startup plans to validate performance with data-center operators and hyperscalers that have asked to review the data, according to Tadeparthy. The Bengaluru-based startup has built a team of about 65 engineers and is setting up customer-facing operations in the U.S. and Taiwan as it prepares for early deployments. Power delivery is one of the most entrenched parts of the data-center stack, long dominated by large incumbents with deep balance sheets and years-long qualification cycles. While many newer companies focus on improving individual components, redesigning power delivery end-to-end requires coordinating silicon, packaging, and system architecture simultaneously — a capital-intensive approach that few startups attempt and one that can take years to prove in production environments. Anandan said the real question now is execution, noting that all startups face technology, market, and team risks when betting on how industries evolve. In C2i’s case, he said, the feedback loop should be relatively short. “We’ll know in the next six months,” said Anandan, pointing to upcoming silicon and early customer validation as the moment when the thesis will be tested. The bet also reflects how India’s semiconductor design ecosystem has matured in recent years. “The way you should look at semiconductors in India is, this is like 2008 e-commerce,” said Anandan. “It’s just getting started.” He pointed to the depth of engineering talent — with a growing share of global chip designers based in the country — alongside government-backed design-linked incentives that have lowered the cost and risk of tape-outs, making it increasingly viable for startups to build globally competitive semiconductor products from India rather than operate only as captive design centers. Whether those conditions translate into a globally competitive product will become clearer over the coming months, as C2i begins validating its system-level power solutions with customers. Topics AI , ai data centers , C2i , C2i Semiconductors , data centers , Exclusive , India , Peak XV Partners , Startups , TDK Ventures Jagmeet Singh Reporter Jagmeet covers startups, tech policy-related updates, and all other major tech-centric developments from India for TechCrunch. He previously worked as a principal correspondent at NDTV. You can contact or verify outreach from Jagmeet by emailing mail@journalistjagmeet.com . View Bio October 13-15 San Francisco, CA Tickets are live at the lowest rates of the year. Save up to $680 on your pass now. Meet investors. Discover your next portfolio company. Hear from 250+ tech leaders , dive into 200+ sessions , and explore 300+ startups building what’s next. Don’t miss these one-time savings. REGISTER NOW Most Popular OpenClaw creator Peter Steinberger joins OpenAI Anthony Ha Hollywood isn’t happy about the new Seedance 2.0 video generator Anthony Ha The great computer science exodus (and where students are going instead) Connie Loizos A Stanford grad student created an algorithm to help his classmates find love; now, Date Drop is the basis of his new startup Amanda Silberling Spotify says its best developers haven’t written a line of code since December, thanks to AI Sarah Perez With co-founders leaving and an IPO looming, Elon Musk turns talk to the moon Connie Loizos Former GitHub CEO raises record $60M dev tool seed round at $300M valuation Julie Bort Loading the next article Error loading the next article X LinkedIn Facebook Instagram youTube Mastodon Threads Bluesky TechCrunch Staff Contact Us Advertise Crunchboard Jobs Site Map Terms of Service Privacy Policy RSS Terms of Use Code of Conduct Epstein Kindle Scribe Reddit TikTok GPT-4o Tech Layoffs ChatGPT © 2025 TechCrunch Media LLC.",
      "imageUrl": "https://techcrunch.com/wp-content/uploads/2024/02/GettyImages-1195233690.jpg?resize=1200%2C630",
      "tags": [],
      "paperCategory": "",
      "signalReasons": [
        "来源：TechCrunch AI",
        "跨源重复：1 个来源",
        "模型评分：8/10，理由：C2i获Peak XV投资解决AI数据中心电力瓶颈，直面算力扩展的核心物理限制，推动基础设施关键突破。",
        "热度：0 / 评论 0"
      ],
      "score": 4.8,
      "publishedAt": "2026-02-16T01:00:00+00:00",
      "authors": [
        "Jagmeet Singh"
      ]
    },
    {
      "id": "hn_47031268",
      "title": "三人AI小队协作开发类SQLite数据库引擎",
      "titleZh": "三人AI小队协作开发类SQLite数据库引擎",
      "titleEn": "A Small Swarm of AI Agents Builds a SQLite-Like Database Engine",
      "url": "https://kiankyars.github.io/machine_learning/2026/02/12/sqlite.html",
      "type": "news",
      "source": "Hacker News",
      "summary": "开发者Kian Kyars协调Claude、Codex和Gemini三个AI代理并行开发了一个类SQLite的Rust数据库引擎，实现了解析器、查询规划器、Volcano执行器、B+树存储、WAL日志等核心组件，共约1.9万行代码，并通过64个SQLLogicTest用例验证其基础功能；该实验展示了多智能体协同编写系统级软件的可行性，但也暴露出任务协调开销大（54.5%提交用于锁管理）、缺乏并发支持及代码冗余等问题，对普通开发者而言，它提供了一套可复现的开源框架（parallel-ralph），揭示了未来AI编程需依赖高频率测试、清晰模块边界与共享状态文档才能提升产出质量。",
      "summaryZh": "开发者Kian Kyars协调Claude、Codex和Gemini三个AI代理并行开发了一个类SQLite的Rust数据库引擎，实现了解析器、查询规划器、Volcano执行器、B+树存储、WAL日志等核心组件，共约1.9万行代码，并通过64个SQLLogicTest用例验证其基础功能；该实验展示了多智能体协同编写系统级软件的可行性，但也暴露出任务协调开销大（54.5%提交用于锁管理）、缺乏并发支持及代码冗余等问题，对普通开发者而言，它提供了一套可复现的开源框架（parallel-ralph），揭示了未来AI编程需依赖高频率测试、清晰模块边界与共享状态文档才能提升产出质量。",
      "summaryEn": "Developer Kian Kyars orchestrated a small swarm of AI agents—Claude, Codex, and Gemini—to collaboratively build a SQLite-like database engine in Rust, implementing core components including a parser, query planner, Volcano-style executor, B+ tree storage, WAL logging, and transaction semantics across ~19k lines of code; the system passed 64 SQLLogicTest queries but failed on unsupported syntax like subqueries. The experiment demonstrates the potential—and pitfalls—of multi-agent software development: 54.5% of commits were spent on coordination (locking/claiming tasks), concurrency was absent, and code duplication persisted despite a coalescer agent. For practitioners, the open-source framework (parallel-ralph) offers a replicable model showing that effective AI-driven systems programming requires high test cadence, strong module boundaries, and shared progress tracking.",
      "fullText": "building sqlite with a small swarm | Kian Kyars Kian Kyars About Blog Build Doomscrolling Now Publications Weekly Victories building sqlite with a small swarm February 12, 2026 tl;dr I tasked Claude, Codex, and Gemini to build a SQLite-like engine in Rust. ~19k lines of code. Parser, planner, volcano executor, pager, b+trees, wal, recovery, joins, aggregates, indexing, transaction semantics, grouped aggregates, and stats-aware planning all implemented. 282 passing ai-written unit tests (this project certainly fails against the official sqlite test suite, so I did not bother testing). response to hacker news post does it actually work? Yes, for a supported subset of SQL. To establish a baseline, I integrated the SQLLogicTest suite. Pass Rate: The engine passes 64 queries (including core CRUD, JOINs, and GROUP BY). Failures: Most failures in the 1,000+ query select1.slt suite are due to unsupported syntax (Subqueries, CASE , EXISTS ). code quality There are several slop patterns in the codebase: Freelist linear search: The agents initially implemented a linear scan of the freelist to check for duplicates. Buffer Clones: The pager was performing redundant .clone() calls on large page buffers during WAL flushes. Concurrency: There is currently no concurrency . is this “sqlite”? No. It is a “simulacrum” of SQLite’s architecture. It is an experiment in agent orchestration , not a production database. background Treat software engineering like distributed systems, and force coordination with: git, lock files, tests, and merge discipline. harness ├── AGENT_PROMPT.md // main agent task prompt ├── BOOTSTRAP_PROMPT.md // bootstrap (initialization) prompt ├── COALESCE_PROMPT.md // deduplication prompt for coalescer agent ├── launch_agents.sh // launches all agents and sets up isolated workspaces ├── agent_loop.sh // per-agent loop/worker script ├── restart_agents.sh // restarts agents └── coalesce.sh // invokes the coalescing script workflow bootstrap phase : one Claude run generates baseline docs, crate skeleton, and test harness. ├── Cargo.toml // crate manifest ├── DESIGN.md // architecture design notes ├── PROGRESS.md // test & build progress ├── README.md // project overview ├── agent_logs // per-agent log files ├── crates // workspace subcrates ├── current_tasks // lock files ├── notes // inter-agent notes ├── target // build artifacts └── test.sh // test harness script worker phase : six workers loop forever ( 2x Claude , 2x Codex , 2x Gemini ). The choice of two workers per model is purely pragmatic: I can’t afford more. I chose heterogeneous agents because it has not been done yet. There is no performance justification for this choice. loop Each agent pulls latest main. Claims one scoped task. Implements + tests against sqlite3 as oracle. Updates shared progress/notes. Push. analysis coordination tax 84 / 154 commits (54.5%) were lock/claim/stale-lock/release coordination. Demonstrates parallel-agent throughput depends heavily on lock hygiene and stale-lock cleanup discipline. what helped most Two things looked decisive: oracle-style validation + high test cadence ( cargo test ... and ./test.sh --fast /full runs captured in PROGRESS.md ). strong module boundaries ( parser -> planner -> executor <-> storage ) so agents could work on orthogonal slices with fewer merge collisions. redundancy I implemented a coalescer with gemini to clean duplication/drift, since that is the largest problem with parallel agents. However, it only ran once at the end of the project, so it was never actually used during the run itself. I have a cron job which runs it daily, but gemini couldn’t complete the entire de-deuplication when I ran it during the expirement itself, which is to say it stopped mid-way through. takeaways Parallelism is great, but only with strict task boundaries. Shared state docs (PROGRESS.md, design notes) are part of the runtime, not “documentation.” Tests are the anti-entropy force. Give agents a narrow interface, a common truth source, and fast feedback, and you get compounding throughput on real systems code. replication To replicate this setup: git clone git@github.com:kiankyars/parallel-ralph.git mv parallel-ralph/sqlite . chmod 700 sqlite/ * .sh ./sqlite/launch_agents.sh restart agents: ./sqlite/restart_agents.sh claude/codex/gemini coalesce agent: ./sqlite/coalesce.sh Assumes you have the relevant CLIs installed ( claude , codex , gemini ), plus screen , git , Rust toolchain, and sqlite3 . limitations The documentation in the repo became enormous, PROGRESS.md became 490 lines and look at the sheer amount of notes ; all this to say that the coalesce agent must be run as often as the other agents. There isn’t a great way to record token usage with cli coding agents (as opposed to API use which is trivial), so I don’t have a grasp on which agent pulled the most weight. future work Track “substantive run rate”, since many are rate-limited/nothing happened. Only Claude adds itself as a co-author to each commit and I did not do that for Codex and Gemini, so I need to add a commit message for Gemini and Codex. Adding more strict observability because probably a lot of errors were due to rate-limiting. inspiration https://cursor.com/blog/scaling-agents https://www.anthropic.com/engineering/building-c-compiler appendix code size snapshot Language Files Lines Non-blank/Non-comment Rust 14 18,650 ~16,155 Shell 1 199 ~139 Total 15 18,849 ~16,294 154 commits between 2026-02-10 and 2026-02-12. usage Gemini does not offer a way to monitor usage with their CLI. It’s also not on a weekly usage basis, but rather a 24-hour usage basis. For codex, I used 100% of the Pro Plan weekly usage, which is currently on a 2x promotion. I used 70% of the Claude Pro weekly usage. codex claude disclaimer codex wrote the first draft for this post. citation @misc { kyars2026sqlite , author = {Kian Kyars} , title = {building sqlite with a small swarm} , year = {2026} , month = feb , day = {12} , howpublished = {\\url{https://kiankyars.github.io/machine_learning/2026/02/16/sqlite.html}} , note = {Blog post} } © 2026 Kian Kyars",
      "imageUrl": "https://kiankyars.github.io/imgs/2026-02-12-sqlite/sqlite1.png",
      "tags": [],
      "paperCategory": "",
      "signalReasons": [
        "来源：Hacker News",
        "跨源重复：1 个来源",
        "模型评分：7/10，理由：AI生成完整SQL引擎的实证尝试，展示多模型协同开发能力，具备显著技术示范意义，属影响开发者生态的实践突破。",
        "热度：104 / 评论 111"
      ],
      "score": 4.03,
      "publishedAt": "2026-02-16T05:38:51+00:00",
      "authors": [
        "kyars"
      ]
    },
    {
      "id": "github_letta-ai_letta-code",
      "title": "letta-ai/letta-code",
      "titleZh": "letta-ai/letta-code",
      "titleEn": "letta-ai/letta-code",
      "url": "https://github.com/letta-ai/letta-code",
      "type": "news",
      "source": "GitHub Trending",
      "summary": "Letta Code 是一个以记忆为中心的编程智能体框架，强调在长期上下文和跨会话知识积累中进行代码生成与推理，其核心设计将记忆作为首要资源而非附属功能，使智能体能在复杂软件任务中保持上下文连贯性并减少重复劳动，这对构建能持续演进的自主编程系统具有方法论意义，普通开发者可将其用于需要长期项目理解和增量式开发的场景。",
      "summaryZh": "Letta Code 是一个以记忆为中心的编程智能体框架，强调在长期上下文和跨会话知识积累中进行代码生成与推理，其核心设计将记忆作为首要资源而非附属功能，使智能体能在复杂软件任务中保持上下文连贯性并减少重复劳动，这对构建能持续演进的自主编程系统具有方法论意义，普通开发者可将其用于需要长期项目理解和增量式开发的场景。",
      "summaryEn": "Letta Code is a memory-first coding agent framework that treats persistent memory as a primary resource rather than an afterthought, enabling agents to maintain contextual coherence and avoid redundant work across sessions during complex software development tasks. This approach offers a methodological shift toward building autonomous programming systems capable of long-term project understanding, which developers can leverage for incremental, context-aware coding workflows.",
      "fullText": "",
      "imageUrl": "https://opengraph.githubassets.com/9232f2412311403809d01c6b132b4d034cb09117afee77d7d40361b53a17e09f/letta-ai/letta-code",
      "tags": [
        "Agent"
      ],
      "paperCategory": "",
      "signalReasons": [
        "来源：GitHub Trending",
        "跨源重复：1 个来源",
        "模型评分：8/10，理由：内存优先型编程代理，强调长期记忆与上下文连贯性，代表AI编程助手向更智能、可复用方向演进的重要进展。",
        "热度：1438 / 评论 0"
      ],
      "score": 7.8,
      "publishedAt": "2026-02-16T23:37:43.562592+00:00",
      "authors": []
    },
    {
      "id": "github_SynkraAI_aios-core",
      "title": "SynkraAI/aios-core",
      "titleZh": "SynkraAI/aios-core",
      "titleEn": "SynkraAI/aios-core",
      "url": "https://github.com/SynkraAI/aios-core",
      "type": "news",
      "source": "GitHub Trending",
      "summary": "Synkra AIOS Core v4.0 是一个面向全栈开发的AI编排系统核心框架，旨在通过统一调度语言模型、工具调用与运行时环境，实现从前端到后端、从设计到部署的端到端自动化开发流程，其关键创新在于将AI视为操作系统级资源进行管理，为构建可扩展、可组合的AI原生开发平台奠定基础，开发者可基于此快速搭建定制化AI工程流水线。",
      "summaryZh": "Synkra AIOS Core v4.0 是一个面向全栈开发的AI编排系统核心框架，旨在通过统一调度语言模型、工具调用与运行时环境，实现从前端到后端、从设计到部署的端到端自动化开发流程，其关键创新在于将AI视为操作系统级资源进行管理，为构建可扩展、可组合的AI原生开发平台奠定基础，开发者可基于此快速搭建定制化AI工程流水线。",
      "summaryEn": "Synkra AIOS Core v4.0 is an AI-orchestrated system framework designed for full-stack development, unifying the scheduling of language models, tool invocation, and runtime environments to enable end-to-end automation from frontend to backend and from design to deployment. Its key innovation lies in treating AI as an operating-system-level resource, establishing a foundation for scalable, composable AI-native development platforms that developers can use to rapidly build customized AI engineering pipelines.",
      "fullText": "",
      "imageUrl": "https://opengraph.githubassets.com/1162dd81f1ac06026d347dc8ad81b970bb517a7546b84980e67da048da23fc13/SynkraAI/aios-core",
      "tags": [],
      "paperCategory": "",
      "signalReasons": [
        "来源：GitHub Trending",
        "跨源重复：1 个来源",
        "模型评分：7/10，理由：AI驱动全栈开发框架核心版本发布，聚焦系统级自动化，具备推动开发范式变革潜力，属高相关性技术进展。",
        "热度：978 / 评论 0"
      ],
      "score": 7.2,
      "publishedAt": "2026-02-16T23:37:42.312856+00:00",
      "authors": []
    },
    {
      "id": "hn_47034192",
      "title": "AI抢购致西部数据2026年硬盘全线售罄",
      "titleZh": "AI抢购致西部数据2026年硬盘全线售罄",
      "titleEn": "AI Demand Sells Out Western Digital’s 2026 Hard Drive Supply by February",
      "url": "https://mashable.com/article/ai-hard-drive-hdd-shortages-western-digital-sold-out",
      "type": "news",
      "source": "Hacker News",
      "summary": "**西部数据**（Western Digital）因AI公司提前抢购全部产能而宣布2026年硬盘已售罄，其CEO透露全球前七大客户（主要为AI企业）不仅包揽全年供应，部分合同甚至延续至2028年，导致消费级市场仅占公司营收5%；这一现象凸显AI基础设施扩张正严重挤压普通消费者的硬件供应并推高价格，用户若需购买存储设备应尽快行动或准备承担更高成本，而行业可能被迫推迟如PlayStation等消费电子产品发布以应对短缺。",
      "summaryZh": "**西部数据**（Western Digital）因AI公司提前抢购全部产能而宣布2026年硬盘已售罄，其CEO透露全球前七大客户（主要为AI企业）不仅包揽全年供应，部分合同甚至延续至2028年，导致消费级市场仅占公司营收5%；这一现象凸显AI基础设施扩张正严重挤压普通消费者的硬件供应并推高价格，用户若需购买存储设备应尽快行动或准备承担更高成本，而行业可能被迫推迟如PlayStation等消费电子产品发布以应对短缺。",
      "summaryEn": "Western Digital has sold out its entire 2026 hard drive capacity by mid-February due to overwhelming demand from AI companies, with its CEO revealing that the top seven enterprise customers—primarily AI firms—have secured supply through 2027 and even 2028, reducing the consumer market to just 5% of company revenue. This underscores how AI’s infrastructure boom is severely constraining hardware availability for ordinary users and driving up prices; consumers should act quickly or expect higher costs, while industries may delay product launches like the next PlayStation to navigate ongoing shortages.",
      "fullText": "Thanks a lot, AI: Hard drives are already sold out for the entire year, says Western Digital | Mashable The Mashable 101 Creator Hub Tech Science Life Social Good Entertainment Deals Shopping Games Search Cancel The Mashable 101 Creator Hub Tech Apps & Software Artificial Intelligence Cybersecurity Cryptocurrency Mobile Smart Home Social Media Tech Industry Transportation All Tech Science Space Climate Change Environment All Science Life Family & Parenting Health & Wellness Sex, Dating & Relationships Sleep Careers Mental Health All Life Social Good Activism Gender LGBTQ Racial Justice Sustainability Politics All Social Good Entertainment Games Movies Podcasts TV Shows Watch Guides All Entertainment SHOP THE BEST Laptops Budget Laptops Dating Apps Sexting Apps Hookup Apps VPNs Robot Vaccuums Robot Vaccum & Mop Headphones Speakers Kindles Gift Guides Mashable Choice Mashable Selects All Sex, Dating & Relationships All Laptops All Headphones All Robot Vacuums All VPN All Shopping Games Product Reviews Adult Friend Finder Bumble Premium Tinder Platinum Kindle Paperwhite PS5 vs PS5 Slim All Reviews All Shopping Deals Newsletters VIDEOS Mashable Shows All Videos Home > Tech Thanks a lot, AI: Hard drives are already sold out for the entire year, says Western Digital AI companies have bought out Western Digital's storage capacity for 2026. It's only February. By Matt Binder on February 15, 2026 Share on Facebook Share on Twitter Share on Flipboard Western Digital says its all sold out of hard drives for 2026, less than two months into the year. Credit: Alex Kraus/Bloomberg via Getty Images Looking to buy a new hard drive? Get ready to pay even more this year. According to Western Digital, one of the world's biggest hard drive manufacturers, the company has already sold out of its storage capacity for 2026 with more than 10 months still left in the year. \"We're pretty much sold out for calendar 2026,\" said Western Digital CEO Irving Tan on the company's recent quarterly earnings call . You May Also Like Tan shared that most of the storage space has been allocated to its \"top seven customers.\" Three of these companies already have agreements with Western Digital for 2027 and even 2028. Mashable Light Speed Want more out-of-this world tech, space and science stories? Sign up for Mashable's weekly Light Speed newsletter. Loading... Sign Me Up Use this instead By clicking Sign Me Up, you confirm you are 16+ and agree to our Terms of Use and Privacy Policy . Thanks for signing up! SEE ALSO: This is your last chance to get super cheap SSDs and hard drives at Amazon Furthermore, the incentive for these hardware companies to prioritize the average consumer is also dwindling. According to Western Digital, thanks to a surge in demand from its enterprise customers, the consumer market now accounts for just 5 percent of the company's revenue. AI companies have been eating up computer hardware as industry growth accelerates. Prices for products ranging from computer processors to video game consoles have skyrocketed due to these AI companies cannibalizing supply chains. The tech industry has already been experiencing a shortage of memory due to demand from AI companies. PC makers have been forced to raise RAM prices on a near-regular basis as shortages persist. Video game console makers, like Sony, have even reportedly considered pushing the next PlayStation launch beyond the planned 2027 release in hopes that AI-related hardware shortages would be resolved by then. With this latest news from Western Digital, it appears the ever-increasing demands from AI companies for memory and storage will continue to grow, with no end in sight. Unless, of course, investors decide to pull back from AI over fears that AI's promises may not come to fruition. But, for now at least, the shortages – and price hikes for consumers – will continue. Topics Artificial Intelligence Recommended For You How to watch Western Michigan vs. Kennesaw State online for free Live stream all the action from the Myrtle Beach Bowl from anywhere in the world. 12/19/2025 By Matt Ford Apple's iPhone Pocket sock is fully sold out The people, they yearn for the sock! 11/26/2025 By Stan Schroeder Creator Khaby Lame just sold a stake in his brand for $975 million A mouth-watering amount of money. 01/28/2026 By Chance Townsend How 'A Knight of the Seven Kingdoms' perfected its Western, whimsical score Composer Dan Romer and showrunner Ira Parker break down the score of \"A Knight of the Seven Kingdoms.\" 02/08/2026 By Belen Edwards Driverless Waymo seemingly drives straight into oncoming traffic in viral video It needs to be...Waymo...careful. 12/19/2025 By Tim Marcin Trending on Mashable NYT Connections hints today: Clues, answers for February 16, 2026 Everything you need to solve 'Connections' #981. 11 hours ago By Mashable Team Wordle today: Answer, hints for February 16, 2026 Here are some tips and tricks to help you find the answer to \"Wordle\" #1703. 11 hours ago By Mashable Team NYT Strands hints, answers for February 16, 2026 Every hint, nudge and outright answer you need to complete today's NYT Strands puzzle. 11 hours ago By Mashable Team Who is the Black Dragon in 'A Knight of the Seven Kingdoms'? It's time for a Westerosi history lesson! 19 hours ago By Belen Edwards Solar eclipse 2026: An eclipse will happen Tuesday, but it's only viewable from a remote spot When and where it will happen. 5 hours ago By Lois Mackenzie The biggest stories of the day delivered to your inbox. Loading... Sign Me Up Use this instead These newsletters may contain advertising, deals, or affiliate links. By clicking Subscribe, you confirm you are 16+ and agree to our Terms of Use and Privacy Policy . Thanks for signing up. See you at your inbox! TECH Android Antivirus Apple Artificial Intelligence Cameras Creative Software Cryptocurrency Desktops Gaming Consoles Headphones Laptops Microsoft Mobile Productivity Smart Home Social Media Speakers Tablets Tech Industry TikTok Transportation TVs VPN Wifi/Routers SCIENCE Animals Climate Change Environment NASA Rocket Launches Space GAMES Arcade Card Casino Connections Daily Puzzles Strands Word Wordle SOCIAL GOOD DEALS How we Select Deals ENTERTAINMENT DC Comics Disney HBO Hulu Marvel Movies Music Peacock Pixar Podcasts Streaming TV Shows True Crime LIFE Careers Digital Culture Education Family & Parenting Health & Wellness House & Home Mental Health Sex, Dating & Relationships Sleep SHOP THE BEST All Beauty All Earbuds All Kitchen All Robot Vacuums Bluetooth Speakers Budget Laptops Dating Apps E-Readers Gift Guides Headphones Hookup Apps iPads Kindles Laptops Mashable Choice Mashable Selects Speakers Sexting Apps Macbooks Robot Vaccuums Robot Vaccum & Mop VPNs Windows Laptops REVIEWS Adult Friend Finder Bumble Premium Kindle Paperwhite PS5 vs PS5 Slim Tinder Platinum EVENTS Black Friday CES Cyber Monday Prime Day Samsung Galaxy Unpacked WWDC VIDEOS About Mashable Editorial Standards Contact Us We're Hiring Newsletters Sitemap ©2005–2026 Mashable, Inc., a Ziff Davis company. All Rights Reserved. Mashable is a registered trademark of Ziff Davis and may not be used by third parties without express written permission. About Ziff Davis Privacy Policy Terms of Use Advertise Accessibility Do Not Sell My Personal Information AdChoices",
      "imageUrl": "https://helios-i.mashable.com/imagery/articles/03BMp5tylVs9DJJavYCVFKV/hero-image.fill.size_1200x675.v1771180235.jpg",
      "tags": [],
      "paperCategory": "",
      "signalReasons": [
        "来源：Hacker News",
        "跨源重复：1 个来源",
        "模型评分：9/10，理由：AI驱动硬件需求激增导致存储设备断货，揭示真实供应链压力，反映AI规模化落地的全球性经济影响。",
        "热度：331 / 评论 278"
      ],
      "score": 7.05,
      "publishedAt": "2026-02-16T12:28:31+00:00",
      "authors": [
        "dClauzel"
      ]
    },
    {
      "id": "hn_47032876",
      "title": "Qwen3.5: Towards Native Multimodal Agents",
      "titleZh": "Qwen3.5: Towards Native Multimodal Agents",
      "titleEn": "Qwen3.5: Towards Native Multimodal Agents",
      "url": "https://qwen.ai/blog?id=qwen3.5",
      "type": "news",
      "source": "Hacker News",
      "summary": "通义千问Qwen3.5聚焦原生多模态智能体能力，通过统一架构同时处理文本、图像等多源输入并生成跨模态响应，其关键进展在于将感知、推理与行动模块深度集成，使智能体能在真实环境中理解复杂指令并执行具身任务，这标志着大模型正从单模态问答向可交互、可操作的通用智能体演进，为开发者构建视觉导航、机器人控制等应用提供新基座。",
      "summaryZh": "通义千问Qwen3.5聚焦原生多模态智能体能力，通过统一架构同时处理文本、图像等多源输入并生成跨模态响应，其关键进展在于将感知、推理与行动模块深度集成，使智能体能在真实环境中理解复杂指令并执行具身任务，这标志着大模型正从单模态问答向可交互、可操作的通用智能体演进，为开发者构建视觉导航、机器人控制等应用提供新基座。",
      "summaryEn": "Qwen3.5 advances native multimodal agent capabilities by integrating perception, reasoning, and action modules into a unified architecture that processes text, images, and other modalities simultaneously to generate cross-modal responses and execute embodied tasks. This marks a shift from single-modality question-answering toward interactive, actionable general-purpose agents, offering developers a foundation for applications like visual navigation and robotic control.",
      "fullText": "Qwen",
      "imageUrl": "https://tse4.mm.bing.net/th/id/OIP.msWhAnZhpROGKQfGM_0z6gHaD4?w=1200&h=630&c=7&r=0&o=5&cb=defcachec2&pid=1.7",
      "tags": [
        "Multimodal",
        "Agent"
      ],
      "paperCategory": "",
      "signalReasons": [
        "来源：Hacker News",
        "跨源重复：1 个来源",
        "模型评分：8/10，理由：Qwen3.5在多模态代理方向的进展具有显著技术前瞻性，若属实则可能推动通用AI代理发展，具备行业影响力。",
        "热度：358 / 评论 174"
      ],
      "score": 6.66,
      "publishedAt": "2026-02-16T09:32:21+00:00",
      "authors": [
        "danielhanchen"
      ]
    }
  ],
  "stats": {
    "total_papers_ingested": 243,
    "total_news_ingested": 40,
    "l1_papers_passed": 109,
    "l1_news_passed": 32,
    "l2_papers_scored": 54,
    "l2_news_scored": 18,
    "l3_papers_selected": 18,
    "l3_news_selected": 11,
    "news_source_counts": {
      "Hacker News": 24,
      "TechCrunch AI": 8,
      "GitHub Trending": 5,
      "The Verge AI": 2,
      "NVIDIA Blog": 1
    },
    "rss_source_counts": {
      "TechCrunch AI": 8,
      "The Verge AI": 2,
      "NVIDIA Blog": 1
    },
    "news_title_source_counts": {
      "show hn wildex we built pok mon go for real wildlife": 1,
      "show hn jemini gemini for the epstein files": 1,
      "show hn maths cs and ai compendium": 1,
      "show hn 2d coulomb gas simulator": 1,
      "neurons outside the brain": 1,
      "qwen3 5 towards native multimodal agents": 1,
      "the long tail of llm assisted decompilation": 1,
      "privilege is bad grammar": 1,
      "robert duvall has died": 1,
      "anthropic tries to hide claude s ai actions devs hate it": 1,
      "should drug companies be advertising to consumers": 1,
      "castlevania and bloodstained developer shutaro ida dies aged 52": 1,
      "ai optimism is a class privilege": 1,
      "big tech stocks lose billions as ai spending fears hit valuations": 1,
      "thanks a lot ai hard drives are sold out for the year says wd": 1,
      "pentagon threatens anthropic punishment": 1,
      "ask hn what happens after the ai bubble bursts": 1,
      "show hn jefftube": 1,
      "building sqlite with a small swarm": 1,
      "plan 9 desktop guide": 1,
      "btrfs disk errors to fall asleep to": 1,
      "ai fails at 96 of general work jobs new study": 1,
      "canada gives u s arms makers the cold shoulder on military spending": 1,
      "i guess i kinda get why people hate ai": 1,
      "rowboatlabs rowboat": 1,
      "steipete gogcli": 1,
      "openclaw openclaw": 1,
      "synkraai aios core": 1,
      "letta ai letta code": 1,
      "new semianalysis inferencex data shows nvidia blackwell ultra delivers up to 50x better performance and 35x lower costs for agentic ai": 1,
      "let 8217 s talk about ring lost dogs and the surveillance state": 1,
      "after spooking hollywood bytedance will tweak safeguards on new ai model": 1,
      "have money will travel a16z s hunt for the next european unicorn": 1,
      "how ricursive intelligence raised 335m at a 4b valuation in 4 months": 1,
      "flapping airplanes on the future of ai we want to try really radically different things": 1,
      "after all the hype some ai experts don t think openclaw is all that exciting": 1,
      "fractal analytics muted ipo debut signals persistent ai fears in india": 1,
      "all the important news from the ongoing india ai impact summit": 1,
      "as ai data centers hit power limits peak xv backs indian startup c2i to fix the bottleneck": 1,
      "blackstone backs neysa in up to 1 2b financing as india pushes to build domestic ai infrastructure": 1
    },
    "total_papers_deduped": 243,
    "total_news_deduped": 40,
    "news_recent_filtered": 40
  }
}