"""
AI Tides - ä¸»ç¨‹åºå…¥å£
æ•´åˆæ‰€æœ‰ Pipeline é˜¶æ®µ
"""
import logging
import sys
import os
from collections import defaultdict
import re
from datetime import datetime, timedelta, timezone

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from pipeline.ingestion import fetch_all_papers, fetch_all_news
from pipeline.filters import HeuristicFilter, AIScorer, Refiner, Deduplicator
from pipeline.enrichment.fulltext import enrich_news_full_text
from pipeline.output import OutputGenerator
from pipeline.config import config

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s [%(levelname)s] %(message)s",
    datefmt="%Y-%m-%d %H:%M:%S"
)
logger = logging.getLogger(__name__)


def run_pipeline(dry_run: bool = False):
    """è¿è¡Œå®Œæ•´çš„ AI Tides Pipeline"""
    
    start_time = datetime.now()
    logger.info("=" * 60)
    logger.info("ğŸŒŠ AI Tides Pipeline å¯åŠ¨")
    logger.info(f"ğŸ“… æ—¥æœŸ: {start_time.strftime('%Y-%m-%d %H:%M:%S')}")
    logger.info("=" * 60)
    
    stats = {}
    
    # ========================================
    # Phase 1: æ•°æ®æ‘„å–
    # ========================================
    logger.info("\nğŸ“¥ Phase 1: æ•°æ®æ‘„å–...")
    
    # è·å–è®ºæ–‡
    logger.info("æ­£åœ¨è·å–è®ºæ–‡...")
    papers = fetch_all_papers()
    stats["total_papers_ingested"] = len(papers)
    
    # è·å–æ–°é—»
    logger.info("æ­£åœ¨è·å–æ–°é—»...")
    news = fetch_all_news()
    stats["total_news_ingested"] = len(news)

    def _normalize_title(title: str) -> str:
        text = (title or "").lower().strip()
        text = re.sub(r"[^a-z0-9\u4e00-\u9fff]+", " ", text)
        return re.sub(r"\s+", " ", text).strip()

    source_counts = defaultdict(int)
    rss_counts = defaultdict(int)
    title_sources = defaultdict(set)
    for item in news:
        source_name = item.source_name or "Unknown"
        source_counts[source_name] += 1
        if getattr(item, "source_type", None) and item.source_type.value == "rss":
            rss_counts[source_name] += 1
        title_key = _normalize_title(item.title or "")
        if title_key:
            title_sources[title_key].add(source_name)

    stats["news_source_counts"] = dict(
        sorted(source_counts.items(), key=lambda x: x[1], reverse=True)
    )
    stats["rss_source_counts"] = dict(
        sorted(rss_counts.items(), key=lambda x: x[1], reverse=True)
    )
    stats["news_title_source_counts"] = {
        k: len(v) for k, v in title_sources.items()
    }

    # å»é‡ï¼ˆURLï¼‰
    deduplicator = Deduplicator()
    papers = deduplicator.deduplicate_by_url(papers)
    news = deduplicator.deduplicate_by_url(news)
    stats["total_papers_deduped"] = len(papers)
    stats["total_news_deduped"] = len(news)
    
    logger.info(f"âœ… æ‘„å–å®Œæˆ - è®ºæ–‡: {len(papers)}, æ–°é—»: {len(news)}")

    # æœ€ç»ˆå…œåº•ï¼šä¸¥æ ¼ä¿è¯â€œè¿‡å» 24 å°æ—¶å†…â€çš„æ–°é—»æ‰è¿›å…¥åç»­é˜¶æ®µ
    # è¯´æ˜ï¼šå„ ingestion å·²åš cutoffï¼Œä½†ä¸åŒæºå¯èƒ½æ—¶é—´å­—æ®µç¼ºå¤±/è§£æå¼‚å¸¸ï¼Œè¿™é‡Œåšç»Ÿä¸€ç¡¬çº¦æŸã€‚
    now_utc = datetime.now(timezone.utc)
    cutoff = now_utc - timedelta(hours=config.hours_lookback)
    dropped_no_time = 0
    dropped_old = 0
    recent_news = []
    for item in news:
        published_at = getattr(item, "published_at", None)
        if not published_at:
            dropped_no_time += 1
            continue
        if published_at.tzinfo is None:
            # é¿å… naive datetime è¯¯åˆ¤ï¼šé»˜è®¤æŒ‰ UTC å¤„ç†
            published_at = published_at.replace(tzinfo=timezone.utc)
        if published_at < cutoff:
            dropped_old += 1
            continue
        recent_news.append(item)
    if dropped_no_time or dropped_old:
        logger.info(
            f"[Recency] æ–°é—»æ—¶é—´çª—å£è¿‡æ»¤({config.hours_lookback}h): "
            f"{len(news)} -> {len(recent_news)} (æ— æ—¶é—´: {dropped_no_time}, è¿‡æœŸ: {dropped_old})"
        )
    news = recent_news
    stats["news_recent_filtered"] = len(news)

    # ========================================
    # Phase 1.5: æ–°é—»å…¨æ–‡æŠ“å–ï¼ˆä¾› L2/L3 ä½¿ç”¨ï¼‰
    # ========================================
    logger.info("\nğŸ§¾ Phase 1.5: æ–°é—»å…¨æ–‡æŠ“å–...")
    news = enrich_news_full_text(news)
    
    # ========================================
    # Phase 2: ä¸‰çº§è¿‡æ»¤æ¼æ–—
    # ========================================
    logger.info("\nğŸ” Phase 2: ä¸‰çº§è¿‡æ»¤æ¼æ–—...")
    
    # L1: å¯å‘å¼è¿‡æ»¤
    logger.info("\n[L1] å¯å‘å¼è¿‡æ»¤...")
    heuristic_filter = HeuristicFilter()
    l1_result = heuristic_filter.run(papers, news)
    
    stats["l1_papers_passed"] = len(l1_result["papers_l2"]) + len(l1_result["papers_whitelist"])
    stats["l1_news_passed"] = len(l1_result["news_l2"]) + len(l1_result["news_whitelist"])
    
    # L2: AI æ‰“åˆ†
    logger.info("\n[L2] AI æ™ºèƒ½æ‰“åˆ†...")
    ai_scorer = AIScorer()
    l2_result = ai_scorer.run(
        papers_l2=l1_result["papers_l2"],
        papers_whitelist=l1_result["papers_whitelist"],
        news_l2=l1_result["news_l2"],
        news_whitelist=l1_result["news_whitelist"]
    )

    # L2 ååšæ–°é—»è¯­ä¹‰å»é‡ï¼ˆä½¿ç”¨ L2 ç»¼åˆåˆ†è¿›è¡Œ rankï¼‰ï¼Œå†å– Top news è¿›å…¥ L3
    logger.info("\n[Dedup] L2 åæ–°é—»è¯­ä¹‰å»é‡...")
    news_l3 = deduplicator.deduplicate_semantic(l2_result["news_l3_all"])
    news_l3.sort(key=lambda x: x.l2_combined_score, reverse=True)
    news_l3 = news_l3[: config.l2_news_limit]
    
    # L3: æ·±åº¦ç²¾ç‚¼
    logger.info("\n[L3] æ·±åº¦ç²¾ç‚¼...")
    refiner = Refiner()
    report = refiner.run(
        papers_l3=l2_result["papers_l3"],
        news_l3=news_l3
    )
    
    # æ›´æ–°ç»Ÿè®¡ä¿¡æ¯
    report.stats.update(stats)
    
    # ========================================
    # Phase 3: è¾“å‡ºç”Ÿæˆ
    # ========================================
    logger.info("\nğŸ“¤ Phase 3: ç”Ÿæˆè¾“å‡º...")
    
    output_generator = OutputGenerator()
    
    if not dry_run:
        output_paths = output_generator.save_report(report)
    else:
        logger.info("ğŸ” å¹²è¿è¡Œæ¨¡å¼ï¼šè·³è¿‡æ–‡ä»¶ä¿å­˜")
        output_paths = {}
    
    # ========================================
    # å®Œæˆ
    # ========================================
    end_time = datetime.now()
    duration = (end_time - start_time).total_seconds()
    
    logger.info("\n" + "=" * 60)
    logger.info("ğŸ‰ Pipeline æ‰§è¡Œå®Œæˆ!")
    logger.info(f"â±ï¸  è€—æ—¶: {duration:.2f} ç§’")
    logger.info(f"ğŸ“Š æœ€ç»ˆç»“æœ: {len(report.papers)} ç¯‡è®ºæ–‡, {len(report.news)} æ¡æ–°é—»")
    logger.info("=" * 60)
    
    # æ‰“å°è¾“å‡ºè·¯å¾„
    if output_paths:
        logger.info("\nğŸ“ è¾“å‡ºæ–‡ä»¶:")
        for key, path in output_paths.items():
            if path:
                logger.info(f"   {key}: {path}")
    
    # æ‰“å°ç²¾é€‰å†…å®¹é¢„è§ˆ
    if report.papers:
        logger.info("\nğŸ“š ç²¾é€‰è®ºæ–‡é¢„è§ˆ:")
        for i, paper in enumerate(report.papers[:3], 1):
            logger.info(f"   {i}. {paper.title[:60]}...")
    
    if report.news:
        logger.info("\nğŸ“° ç²¾é€‰æ–°é—»é¢„è§ˆ:")
        for i, news_item in enumerate(report.news[:3], 1):
            logger.info(f"   {i}. {news_item.title[:60]}...")
    
    return report


def main():
    """ä¸»å‡½æ•°"""
    import argparse
    
    parser = argparse.ArgumentParser(description="AI Tides Pipeline")
    parser.add_argument(
        "--debug",
        action="store_true",
        help="å¼€å¯è°ƒè¯•æ¨¡å¼ï¼ˆæ›´è¯¦ç»†çš„æ—¥å¿—ï¼‰"
    )
    parser.add_argument(
        "--dry-run",
        action="store_true",
        help="å¹²è¿è¡Œæ¨¡å¼ï¼ˆä¸ä¿å­˜è¾“å‡ºï¼‰"
    )
    
    args = parser.parse_args()
    
    if args.debug:
        logging.getLogger().setLevel(logging.DEBUG)
        logger.debug("è°ƒè¯•æ¨¡å¼å·²å¼€å¯")
    
    try:
        report = run_pipeline(dry_run=args.dry_run)
        return 0
    except Exception as e:
        logger.error(f"âŒ Pipeline æ‰§è¡Œå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return 1


if __name__ == "__main__":
    sys.exit(main())
