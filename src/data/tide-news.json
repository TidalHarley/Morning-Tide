{
  "date": "2026-02-13",
  "generatedAt": "2026-02-13T02:13:11.190304",
  "introduction": "今日AI领域迎来多项关键进展：Anthropic承诺承担数据中心带来的电价上涨，凸显AI基础设施扩张与社会成本的张力；NVIDIA与开源社区推动推理成本下降10倍，显著改善AI经济性；DeepMind发布Gemini 3 Deep Think，专攻科研与工程复杂推理。论文方面，Control Reinforcement Learning实现对语言模型输出的精准干预，RADAR构建首个真实世界具身智能评测基准，OmniVL-Guard统一多模态伪造检测，而AI-rithmetic探索AI在数学证明中的新边界。这些突破不仅推动技术前沿，更直接影响AI部署成本、可信度与实际应用。",
  "introductionZh": "今日AI领域迎来多项关键进展：Anthropic承诺承担数据中心带来的电价上涨，凸显AI基础设施扩张与社会成本的张力；NVIDIA与开源社区推动推理成本下降10倍，显著改善AI经济性；DeepMind发布Gemini 3 Deep Think，专攻科研与工程复杂推理。论文方面，Control Reinforcement Learning实现对语言模型输出的精准干预，RADAR构建首个真实世界具身智能评测基准，OmniVL-Guard统一多模态伪造检测，而AI-rithmetic探索AI在数学证明中的新边界。这些突破不仅推动技术前沿，更直接影响AI部署成本、可信度与实际应用。",
  "introductionEn": "Today’s AI landscape features pivotal developments: Anthropic pledges to absorb electricity cost increases from its data centers, highlighting the tension between AI growth and public infrastructure. NVIDIA and open-source tools drive inference costs down by up to 10x, reshaping AI economics. DeepMind launches Gemini 3 Deep Think for advanced scientific reasoning. Key papers include Control Reinforcement Learning for precise LLM output steering, RADAR—a real-world benchmark for embodied AI—and OmniVL-Guard for unified multimodal forgery detection. These advances directly impact AI’s cost, reliability, and real-world deployment.",
  "longformScript": "今天AI领域发生的事，可以用三个关键词来概括：成本、责任和专业。一边是技术效率的飞速提升，让AI推理越来越便宜；另一边是社会成本的显性化，迫使科技公司开始为能源消耗买单；而更深层的变化，则是AI正从“什么都能聊两句”的通用助手，转向真正能解决科研与工程难题的专业协作者。\n\n先说成本。过去几个月，AI推理的经济性正在发生质变。NVIDIA最新发布的Blackwell平台，配合开源大模型，已经让多家服务商把推理成本压低了最高10倍。比如医疗领域的Baseten，把医学编码任务的成本砍掉九成；游戏公司Latitude用DeepInfra部署AI角色，token成本直接降了四分之三。这些不是实验室数据，而是真实上线的业务。背后的关键，除了硬件本身的算力密度提升，还有像NVFP4这样的低精度格式、以及整套优化过的推理栈。更值得关注的是，这波降本潮的主角是开源模型——它们不再是闭源大模型的廉价替代品，而是通过软硬协同，真正打开了AI普惠的大门。普通用户可能不会注意到技术细节，但会感受到：AI服务变得更流畅、更便宜，甚至免费。\n\n与此同时，另一股力量在推动AI公司承担更多外部成本。Anthropic最近做出一个罕见承诺：它将全额承担自家数据中心接入电网所需的全部升级费用，并主动采购额外电力、部署削峰系统，确保当地居民的电费不会因为AI算力激增而上涨。这个动作看似是企业公关，实则回应了一个日益尖锐的现实矛盾——单个前沿模型训练动辄消耗吉瓦级电力，相当于一座小城市的用电量。在美国多个州，新建数据中心已引发社区抗议和电价担忧。Anthropic的做法，等于把原本可能转嫁给公众的隐性成本内部化了。它还呼吁联邦政府加快能源审批流程，试图把AI基建变成国家能源转型的推手，而非负担。这或许预示着，未来AI公司的扩张，不仅要拼算力，还要拼“能源社会责任”。\n\n而在技术纵深方向，AI正加速向专业领域扎根。Google刚刚升级的Gemini 3 Deep Think模式，就是典型例子。它不再追求泛泛的对话能力，而是专攻科研与工程中的复杂推理问题。在数学、物理、材料科学等场景中，它已经能发现经同行评审论文中的逻辑漏洞，甚至设计出可制造的半导体薄膜方案。虽然普通用户暂时用不到这个模式，但它通过API开放给研究人员后，很快会渗透进各类专业工具——比如把实验草图自动转成3D打印模型，或解析高维物理数据。这种转变意味着，AI的价值重心正在从“用户交互”转向“问题求解”。类似趋势也出现在开源工具上：Google开源的langextract库，能让LLM从杂乱文本中精准提取结构化信息，并附带来源追溯；Unsloth则让微调大模型的显存占用减少70%，速度翻倍。这些工具不炫技，但实实在在降低了专业AI应用的门槛。\n\n有意思的是，有时候突破并不来自模型本身，而在于怎么“用”模型。一位开发者Can Bölük最近做了一个实验：他只改了一行代码——替换了AI编程代理的工具接口（harness），就让15个大模型的代码修复成功率集体跃升，其中Grok Code Fast 1从6.7%飙升到68.3%。他提出的新格式叫“Hashline”，给每行代码加一个短哈希作为锚点，避免传统diff机制对输出精度的苛刻要求。这个案例揭示了一个常被忽视的事实：当前AI编程的瓶颈，往往不在模型能力，而在工具链设计。对开发者来说，与其盲目追新模型，不如花时间优化调用接口——这可能是性价比更高的提效路径。\n\n当然，也不是所有宏大叙事都经得起推敲。比如最近热议的“轨道AI数据中心”——把算力送上太空，靠空间太阳能供电。听起来很科幻，但仔细算账就会发现，1GW规模的轨道设施成本高达424亿美元，是地面的三倍。即便SpaceX实现超低成本发射，太空散热难、辐射损伤芯片、通信带宽受限等问题依然无解。专家普遍认为，除非火箭复用和卫星量产带来数量级的成本下降，否则这类项目短期内难有商业可行性。这提醒我们，在AI热潮中，既要看到真实进展，也要警惕那些用未来愿景掩盖当下缺陷的过度炒作。\n\n那么，作为听众，该怎么理解今天这些变化？首先，AI的“经济性拐点”可能真的来了——推理成本大幅下降，意味着更多企业能负担规模化部署，而开源生态的成熟也让创新不再局限于巨头。其次，社会对AI的约束正在制度化，能源公平、社区影响将成为公司扩张的硬性考量，这未必是阻力，反而可能倒逼更可持续的技术路径。最后，如果你从事科研、工程或开发工作，不妨关注那些专注垂直场景的工具和模型，它们带来的效率提升，可能远超通用聊天机器人。\n\n今天的AI世界，既在向下扎根——变得更便宜、更可控、更负责任；也在向上突破——向科学前沿发起挑战。技术演进从来不是单线程的故事，而是在效率、伦理与专业深度之间不断寻找平衡。我们下次再聊。",
  "longformScriptZh": "今天AI领域发生的事，可以用三个关键词来概括：成本、责任和专业。一边是技术效率的飞速提升，让AI推理越来越便宜；另一边是社会成本的显性化，迫使科技公司开始为能源消耗买单；而更深层的变化，则是AI正从“什么都能聊两句”的通用助手，转向真正能解决科研与工程难题的专业协作者。\n\n先说成本。过去几个月，AI推理的经济性正在发生质变。NVIDIA最新发布的Blackwell平台，配合开源大模型，已经让多家服务商把推理成本压低了最高10倍。比如医疗领域的Baseten，把医学编码任务的成本砍掉九成；游戏公司Latitude用DeepInfra部署AI角色，token成本直接降了四分之三。这些不是实验室数据，而是真实上线的业务。背后的关键，除了硬件本身的算力密度提升，还有像NVFP4这样的低精度格式、以及整套优化过的推理栈。更值得关注的是，这波降本潮的主角是开源模型——它们不再是闭源大模型的廉价替代品，而是通过软硬协同，真正打开了AI普惠的大门。普通用户可能不会注意到技术细节，但会感受到：AI服务变得更流畅、更便宜，甚至免费。\n\n与此同时，另一股力量在推动AI公司承担更多外部成本。Anthropic最近做出一个罕见承诺：它将全额承担自家数据中心接入电网所需的全部升级费用，并主动采购额外电力、部署削峰系统，确保当地居民的电费不会因为AI算力激增而上涨。这个动作看似是企业公关，实则回应了一个日益尖锐的现实矛盾——单个前沿模型训练动辄消耗吉瓦级电力，相当于一座小城市的用电量。在美国多个州，新建数据中心已引发社区抗议和电价担忧。Anthropic的做法，等于把原本可能转嫁给公众的隐性成本内部化了。它还呼吁联邦政府加快能源审批流程，试图把AI基建变成国家能源转型的推手，而非负担。这或许预示着，未来AI公司的扩张，不仅要拼算力，还要拼“能源社会责任”。\n\n而在技术纵深方向，AI正加速向专业领域扎根。Google刚刚升级的Gemini 3 Deep Think模式，就是典型例子。它不再追求泛泛的对话能力，而是专攻科研与工程中的复杂推理问题。在数学、物理、材料科学等场景中，它已经能发现经同行评审论文中的逻辑漏洞，甚至设计出可制造的半导体薄膜方案。虽然普通用户暂时用不到这个模式，但它通过API开放给研究人员后，很快会渗透进各类专业工具——比如把实验草图自动转成3D打印模型，或解析高维物理数据。这种转变意味着，AI的价值重心正在从“用户交互”转向“问题求解”。类似趋势也出现在开源工具上：Google开源的langextract库，能让LLM从杂乱文本中精准提取结构化信息，并附带来源追溯；Unsloth则让微调大模型的显存占用减少70%，速度翻倍。这些工具不炫技，但实实在在降低了专业AI应用的门槛。\n\n有意思的是，有时候突破并不来自模型本身，而在于怎么“用”模型。一位开发者Can Bölük最近做了一个实验：他只改了一行代码——替换了AI编程代理的工具接口（harness），就让15个大模型的代码修复成功率集体跃升，其中Grok Code Fast 1从6.7%飙升到68.3%。他提出的新格式叫“Hashline”，给每行代码加一个短哈希作为锚点，避免传统diff机制对输出精度的苛刻要求。这个案例揭示了一个常被忽视的事实：当前AI编程的瓶颈，往往不在模型能力，而在工具链设计。对开发者来说，与其盲目追新模型，不如花时间优化调用接口——这可能是性价比更高的提效路径。\n\n当然，也不是所有宏大叙事都经得起推敲。比如最近热议的“轨道AI数据中心”——把算力送上太空，靠空间太阳能供电。听起来很科幻，但仔细算账就会发现，1GW规模的轨道设施成本高达424亿美元，是地面的三倍。即便SpaceX实现超低成本发射，太空散热难、辐射损伤芯片、通信带宽受限等问题依然无解。专家普遍认为，除非火箭复用和卫星量产带来数量级的成本下降，否则这类项目短期内难有商业可行性。这提醒我们，在AI热潮中，既要看到真实进展，也要警惕那些用未来愿景掩盖当下缺陷的过度炒作。\n\n那么，作为听众，该怎么理解今天这些变化？首先，AI的“经济性拐点”可能真的来了——推理成本大幅下降，意味着更多企业能负担规模化部署，而开源生态的成熟也让创新不再局限于巨头。其次，社会对AI的约束正在制度化，能源公平、社区影响将成为公司扩张的硬性考量，这未必是阻力，反而可能倒逼更可持续的技术路径。最后，如果你从事科研、工程或开发工作，不妨关注那些专注垂直场景的工具和模型，它们带来的效率提升，可能远超通用聊天机器人。\n\n今天的AI世界，既在向下扎根——变得更便宜、更可控、更负责任；也在向上突破——向科学前沿发起挑战。技术演进从来不是单线程的故事，而是在效率、伦理与专业深度之间不断寻找平衡。我们下次再聊。",
  "longformScriptEn": "Today’s AI landscape is defined by a critical inflection point: the race to scale artificial intelligence is colliding with real-world constraints—energy, cost, and infrastructure. As companies push the boundaries of what models can do, they’re also being forced to reckon with the physical and economic realities of deploying them at scale. From Anthropic’s unprecedented pledge to absorb grid upgrade costs, to NVIDIA-powered inference breakthroughs slashing token prices by 90%, the field is shifting from pure capability to responsible, efficient deployment. Meanwhile, specialized reasoning systems like Gemini 3 Deep Think are unlocking new frontiers in science and engineering, proving that the next leap in AI won’t just be about size—but precision, reliability, and integration into human workflows.\n\nOne of the most consequential developments this week comes not from a model benchmark, but from a corporate commitment: Anthropic has pledged to cover 100% of electricity price hikes caused by its data centers. This includes paying for grid infrastructure upgrades, procuring new clean power generation, and deploying curtailment systems to reduce strain during peak demand. The move directly addresses growing public backlash over AI’s soaring energy appetite—some frontier training runs now consume gigawatts, equivalent to small cities. By shielding local ratepayers and investing in water-efficient cooling and community jobs, Anthropic is attempting to reframe AI infrastructure not as a burden, but as a catalyst for clean energy investment. Crucially, the company is also lobbying for federal reforms to speed up energy permitting, signaling that AI firms may soon need regulatory partnerships as much as compute clusters to sustain growth.\n\nAt the same time, the economics of running AI are undergoing a dramatic transformation. Inference—the process of using trained models to generate responses—is becoming radically cheaper, thanks to a powerful synergy between open-source models and NVIDIA’s new Blackwell architecture. Companies like Baseten, Fireworks AI, and Together AI are reporting up to 10x reductions in cost per token. Real-world examples abound: Sully.ai cut medical coding expenses by 90%; gaming studio Latitude achieved 4x lower token costs; and voice-agent platform Decagon slashed query costs by sixfold. These gains stem from hardware-software co-design innovations like NVFP4, a low-precision format that maintains accuracy while boosting throughput. The ripple effect is profound: businesses can now afford to embed AI deeply into customer-facing products, and consumers may soon see faster, free, or ad-supported AI services become the norm. This trend is also fueling investor frenzy—Modal Labs, an inference optimization startup, is reportedly raising at a $2.5 billion valuation, more than double its worth just five months ago.\n\nBeyond cost and energy, the frontier of AI capability is narrowing its focus. Google’s upgraded Gemini 3 Deep Think exemplifies this shift. Unlike general-purpose chatbots, Deep Think is a specialized reasoning engine built for scientific and engineering challenges. It’s already setting records on advanced benchmarks like Humanity’s Last Exam and ARC-AGI-2, and has demonstrated tangible utility—spotting subtle errors in peer-reviewed mathematics and designing semiconductor crystal recipes over 100 micrometers in size. Now available via early API access and to Google AI Ultra subscribers, it enables researchers to turn hand-drawn sketches into 3D-printable models or analyze complex experimental data. This marks a pivot toward AI as a domain-specific collaborator, not just a general assistant. Complementing this, Google also open-sourced *langextract*, a Python library that uses LLMs to pull structured data from unstructured text with precise source grounding and interactive visualization—making AI outputs more interpretable and trustworthy for developers building enterprise applications.\n\nMeanwhile, innovation isn’t just happening at the model level—it’s accelerating in the tooling layer. A striking example comes from developer Can Bölük, who boosted coding performance across 15 different LLMs—not by retraining them, but by redesigning the “edit harness,” the interface that handles code modifications. His new “Hashline” format tags each line with a content hash, allowing models to reference code reliably without worrying about whitespace or context drift. The result? Grok Code Fast 1’s success rate jumped from 6.7% to 68.3% in a single afternoon. Similarly, the open-source library *Unsloth* now enables 2x faster fine-tuning with 70% less VRAM, making it feasible for small teams to customize models like Llama or Gemma without expensive hardware. These advances underscore a crucial insight: sometimes, the biggest gains come not from bigger models, but from smarter plumbing.\n\nLooking ahead, the path forward hinges on balancing ambition with realism. While orbital AI data centers—championed by figures like Elon Musk—capture headlines, a sobering TechCrunch analysis shows they’d cost nearly three times as much as ground-based equivalents, with unresolved issues like heat dissipation in vacuum and cosmic radiation damage. For now, the real action remains earthbound: in university labs powered by NVIDIA’s DGX Spark desktop supercomputers, where researchers at Harvard, Stanford, and NYU are prototyping everything from epilepsy diagnostics to Antarctic neutrino detectors. As you track developments, watch how corporate energy pledges translate into enforceable utility agreements, whether open-source tooling continues to democratize fine-tuning, and if specialized reasoning engines like Deep Think begin appearing in commercial R&D pipelines.\n\nIn sum, today’s AI story isn’t just about who has the biggest model—it’s about who can deploy it responsibly, affordably, and effectively within the limits of physics, policy, and public trust. The era of unchecked scaling is giving way to one of optimization, specialization, and accountability. And that, perhaps, is the most promising development of all.",
  "audioUrl": "",
  "papers": [
    {
      "id": "arxiv_2602_10437v1",
      "title": "Control Reinforcement Learning: Token-Level Mechanistic Analysis via Learned SAE Feature Steering",
      "titleZh": "Control Reinforcement Learning: Token-Level Mechanistic Analysis via Learned SAE Feature Steering",
      "titleEn": "Control Reinforcement Learning: Token-Level Mechanistic Analysis via Learned SAE Feature Steering",
      "url": "https://arxiv.org/abs/2602.10437v1",
      "type": "paper",
      "source": "arXiv",
      "summary": "该研究提出Control Reinforcement Learning（CRL）框架，通过强化学习策略在每个token级别动态选择稀疏自编码器（SAE）特征进行干预，从而识别出真正影响语言模型输出的可解释特征；其引入的自适应特征掩码机制促进多样性发现，同时保留单特征可解释性，并支持分支点追踪、评论家轨迹分析和层间比较等新分析能力，在Gemma-2 2B模型上于MMLU、BBQ、GSM8K等多个基准中实现性能提升并生成逐token干预日志，确立了学习型特征引导作为动态机制可解释性工具的价值。",
      "summaryZh": "该研究提出Control Reinforcement Learning（CRL）框架，通过强化学习策略在每个token级别动态选择稀疏自编码器（SAE）特征进行干预，从而识别出真正影响语言模型输出的可解释特征；其引入的自适应特征掩码机制促进多样性发现，同时保留单特征可解释性，并支持分支点追踪、评论家轨迹分析和层间比较等新分析能力，在Gemma-2 2B模型上于MMLU、BBQ、GSM8K等多个基准中实现性能提升并生成逐token干预日志，确立了学习型特征引导作为动态机制可解释性工具的价值。",
      "summaryEn": "This work introduces Control Reinforcement Learning (CRL), a framework that trains a policy to dynamically select Sparse Autoencoder (SAE) features for token-level steering, identifying which interpretable features actually alter language model outputs. Adaptive Feature Masking encourages diverse feature discovery while preserving single-feature interpretability. The approach enables new analytical capabilities—including branch point tracking, critic trajectory analysis, and layer-wise comparisons revealing syntactic features in early layers and semantic ones later—and improves performance on Gemma-2 2B across MMLU, BBQ, GSM8K, HarmBench, and XSTest while producing per-token intervention logs, establishing learned feature steering as a dynamic mechanistic interpretability tool.",
      "fullText": "",
      "imageUrl": "",
      "tags": [
        "LLM",
        "RAG"
      ],
      "paperCategory": "General AI",
      "signalReasons": [
        "来源：arXiv",
        "跨源重复：1 个来源",
        "模型评分：10/10，理由：首次实现基于SAE特征的可解释控制与机制分析，为大模型可解释性提供全新工具，具有里程碑意义。",
        "热度：15 / 评论 0"
      ],
      "score": 10.0,
      "publishedAt": "2026-02-11T02:28:49+00:00",
      "authors": [
        "Seonglae Cho",
        "Zekun Wu",
        "Adriano Koshiyama"
      ]
    },
    {
      "id": "arxiv_2602_10980v1",
      "title": "RADAR: Benchmarking Vision-Language-Action Generalization via Real-World Dynamics, Spatial-Physical Intelligence, and Autonomous Evaluation",
      "titleZh": "RADAR: Benchmarking Vision-Language-Action Generalization via Real-World Dynamics, Spatial-Physical Intelligence, and Autonomous Evaluation",
      "titleEn": "RADAR: Benchmarking Vision-Language-Action Generalization via Real-World Dynamics, Spatial-Physical Intelligence, and Autonomous Evaluation",
      "url": "https://arxiv.org/abs/2602.10980v1",
      "type": "paper",
      "source": "arXiv",
      "summary": "针对当前视觉-语言-动作（VLA）模型评估局限于仿真或高度受限环境的问题，该研究提出RADAR基准，通过整合真实物理动态、空间-物理智能任务和基于3D指标的全自动评估管道，系统性检验模型在现实条件下的泛化能力；对多个前沿VLA模型的审计显示，其在轻微传感器噪声下3D IoU从0.261骤降至0.068，且空间推理能力有限，揭示了现有模型在真实世界中的脆弱性，为可靠、可扩展的具身智能评估提供了必要工具。",
      "summaryZh": "针对当前视觉-语言-动作（VLA）模型评估局限于仿真或高度受限环境的问题，该研究提出RADAR基准，通过整合真实物理动态、空间-物理智能任务和基于3D指标的全自动评估管道，系统性检验模型在现实条件下的泛化能力；对多个前沿VLA模型的审计显示，其在轻微传感器噪声下3D IoU从0.261骤降至0.068，且空间推理能力有限，揭示了现有模型在真实世界中的脆弱性，为可靠、可扩展的具身智能评估提供了必要工具。",
      "summaryEn": "Addressing the gap between simulated evaluations and real-world performance of Vision-Language-Action (VLA) models, this paper introduces RADAR—a benchmark that systematically evaluates generalization under realistic conditions through three components: principled physical dynamics, tasks probing spatial-physical intelligence, and a fully autonomous 3D-metric-based evaluation pipeline. Auditing state-of-the-art VLA models reveals severe fragility: 3D IoU drops from 0.261 to 0.068 under modest sensor noise, and models show limited spatial reasoning. RADAR thus provides a necessary foundation for reliable, scalable real-world VLA evaluation.",
      "fullText": "",
      "imageUrl": "",
      "tags": [
        "Vision",
        "Multimodal",
        "Robotics",
        "3D"
      ],
      "paperCategory": "Robotics",
      "signalReasons": [
        "来源：arXiv",
        "跨源重复：1 个来源",
        "模型评分：10/10，理由：发布首个面向真实世界动态、物理智能与自主评估的VLA模型基准，填补评估空白，将重塑机器人与AI系统评价体系，具有历史级影响。",
        "热度：13 / 评论 0"
      ],
      "score": 10.0,
      "publishedAt": "2026-02-11T16:08:30+00:00",
      "authors": [
        "Yuhao Chen",
        "Zhihao Zhan",
        "Xiaoxin Lin"
      ]
    },
    {
      "id": "arxiv_2602_10687v1",
      "title": "OmniVL-Guard: Towards Unified Vision-Language Forgery Detection and Grounding via Balanced RL",
      "titleZh": "OmniVL-Guard: Towards Unified Vision-Language Forgery Detection and Grounding via Balanced RL",
      "titleEn": "OmniVL-Guard: Towards Unified Vision-Language Forgery Detection and Grounding via Balanced RL",
      "url": "https://arxiv.org/abs/2602.10687v1",
      "type": "paper",
      "source": "arXiv",
      "summary": "为应对现实世界中交织文本、图像与视频的多模态虚假信息检测挑战，该研究提出OmniVL-Guard统一框架，通过平衡强化学习解决多任务优化中“难度偏差”问题——即真实性分类主导梯度导致细粒度定位性能下降；其核心包括自进化思维链生成以克服冷启动问题，以及自适应奖励缩放策略优化（ARSPO）动态调节任务权重，实验证明该方法显著优于现有技术，并在域外场景中展现零样本鲁棒泛化能力。",
      "summaryZh": "为应对现实世界中交织文本、图像与视频的多模态虚假信息检测挑战，该研究提出OmniVL-Guard统一框架，通过平衡强化学习解决多任务优化中“难度偏差”问题——即真实性分类主导梯度导致细粒度定位性能下降；其核心包括自进化思维链生成以克服冷启动问题，以及自适应奖励缩放策略优化（ARSPO）动态调节任务权重，实验证明该方法显著优于现有技术，并在域外场景中展现零样本鲁棒泛化能力。",
      "summaryEn": "To tackle omnibus vision-language forgery detection and grounding in real-world multimodal misinformation involving interleaved text, images, and videos, this paper proposes OmniVL-Guard, a balanced reinforcement learning framework that addresses the 'difficulty bias'—where easier veracity classification dominates gradients and harms fine-grained grounding. Its core components include Self-Evolving CoT Generation to overcome cold-start issues and Adaptive Reward Scaling Policy Optimization (ARSPO) to dynamically balance task weights. Experiments show OmniVL-Guard significantly outperforms state-of-the-art methods and exhibits robust zero-shot generalization across out-of-domain scenarios.",
      "fullText": "",
      "imageUrl": "",
      "tags": [
        "Vision",
        "Multimodal",
        "Reasoning",
        "Research"
      ],
      "paperCategory": "Computer Vision",
      "signalReasons": [
        "来源：arXiv",
        "跨源重复：1 个来源",
        "模型评分：9/10，理由：提出统一多模态伪造检测与定位框架，响应全球虚假信息治理需求，具备战略级产业影响力。",
        "热度：15 / 评论 0"
      ],
      "score": 9.0,
      "publishedAt": "2026-02-11T09:41:36+00:00",
      "authors": [
        "Jinjie Shen",
        "Jing Wu",
        "Yaxiong Wang"
      ]
    },
    {
      "id": "arxiv_2602_10329v1",
      "title": "Are More Tokens Rational? Inference-Time Scaling in Language Models as Adaptive Resource Rationality",
      "titleZh": "Are More Tokens Rational? Inference-Time Scaling in Language Models as Adaptive Resource Rationality",
      "titleEn": "Are More Tokens Rational? Inference-Time Scaling in Language Models as Adaptive Resource Rationality",
      "url": "https://arxiv.org/abs/2602.10329v1",
      "type": "paper",
      "source": "arXiv",
      "summary": "该研究探讨大语言模型在推理时扩展计算资源是否能自发形成资源理性行为，设计变量归因任务系统操控复杂度后发现：指令微调模型和大型推理模型均能随任务复杂度增加从暴力搜索转向分析策略，其中后者在XOR/XNOR等非线性函数上保持稳健；结果表明，即使无显式计算成本奖励，推理时扩展本身也能促使模型自适应调整推理策略，为资源理性作为涌现属性提供实证支持。",
      "summaryZh": "该研究探讨大语言模型在推理时扩展计算资源是否能自发形成资源理性行为，设计变量归因任务系统操控复杂度后发现：指令微调模型和大型推理模型均能随任务复杂度增加从暴力搜索转向分析策略，其中后者在XOR/XNOR等非线性函数上保持稳健；结果表明，即使无显式计算成本奖励，推理时扩展本身也能促使模型自适应调整推理策略，为资源理性作为涌现属性提供实证支持。",
      "summaryEn": "This study investigates whether inference-time scaling in large language models can lead to emergent resource-rational behavior—optimizing performance under computational constraints—without explicit cost-based rewards. Using a Variable Attribution Task with systematically varied complexity, both instruction-tuned (IT) and Large Reasoning Models (LRMs) shift from brute-force to analytic strategies as complexity increases. LRMs remain robust on XOR/XNOR functions where IT models degrade, suggesting that adaptive reasoning behavior emerges inherently from inference-time scaling, providing compelling evidence for resource rationality as an emergent property.",
      "fullText": "",
      "imageUrl": "",
      "tags": [
        "LLM",
        "Inference",
        "Reasoning"
      ],
      "paperCategory": "General AI",
      "signalReasons": [
        "来源：arXiv",
        "跨源重复：1 个来源",
        "模型评分：9/10，理由：系统探讨推理时扩展作为资源理性策略，深刻揭示LLM推理机制本质，可能影响下一代大模型设计范式。",
        "热度：10 / 评论 0"
      ],
      "score": 9.0,
      "publishedAt": "2026-02-10T22:07:05+00:00",
      "authors": [
        "Zhimin Hu",
        "Riya Roshan",
        "Sashank Varma"
      ]
    },
    {
      "id": "arxiv_2602_10450v1",
      "title": "Constructing Industrial-Scale Optimization Modeling Benchmark",
      "titleZh": "Constructing Industrial-Scale Optimization Modeling Benchmark",
      "titleEn": "Constructing Industrial-Scale Optimization Modeling Benchmark",
      "url": "https://arxiv.org/abs/2602.10450v1",
      "type": "paper",
      "source": "arXiv",
      "summary": "为填补工业级优化建模缺乏真实自然语言到求解器代码映射基准的空白，该研究构建MIPLIB-NL数据集，通过结构感知逆向构建方法从MIPLIB 2017真实混合整数线性规划实例中恢复紧凑模型结构，生成与其严格对齐的自然语言描述，并经专家评审与人机协同验证；包含223个一对一重建实例的基准暴露了现有系统在工业规模问题上的严重性能退化，揭示了玩具级评估无法捕捉的失败模式。",
      "summaryZh": "为填补工业级优化建模缺乏真实自然语言到求解器代码映射基准的空白，该研究构建MIPLIB-NL数据集，通过结构感知逆向构建方法从MIPLIB 2017真实混合整数线性规划实例中恢复紧凑模型结构，生成与其严格对齐的自然语言描述，并经专家评审与人机协同验证；包含223个一对一重建实例的基准暴露了现有系统在工业规模问题上的严重性能退化，揭示了玩具级评估无法捕捉的失败模式。",
      "summaryEn": "To address the lack of realistic benchmarks for translating natural-language requirements into industrial-scale optimization code, this work introduces MIPLIB-NL—a dataset built via structure-aware reverse construction from real mixed-integer linear programs in MIPLIB 2017. The pipeline recovers compact model structures, generates aligned natural-language specifications under a unified model–data separation format, and validates semantics through expert review and human–LLM reconstruction checks. The resulting 223 one-to-one instances expose severe performance degradation in current systems that excel on toy benchmarks, revealing failure modes invisible at small scale.",
      "fullText": "",
      "imageUrl": "",
      "tags": [
        "LLM",
        "Benchmark"
      ],
      "paperCategory": "General AI",
      "signalReasons": [
        "来源：arXiv",
        "跨源重复：1 个来源",
        "模型评分：9/10，理由：构建工业级优化建模基准，推动自然语言到可执行代码的自动化转型，将深刻影响制造业、金融等核心产业。",
        "热度：8 / 评论 0"
      ],
      "score": 9.0,
      "publishedAt": "2026-02-11T02:45:31+00:00",
      "authors": [
        "Zhong Li",
        "Hongliang Lu",
        "Tao Wei"
      ]
    },
    {
      "id": "arxiv_2602_10416v1",
      "title": "AI-rithmetic",
      "titleZh": "AI-rithmetic",
      "titleEn": "AI-rithmetic",
      "url": "https://arxiv.org/abs/2602.10416v1",
      "type": "paper",
      "source": "arXiv",
      "summary": "尽管前沿AI模型能在国际数学竞赛中获奖，但该研究系统揭示其在基础整数加法上存在严重缺陷：随着位数增加，所有主流模型准确率显著下降；错误主要源于操作数错位（与分词相关）和进位失败（表现为独立随机错误），这两类错误分别解释了Claude Opus 4.1、GPT-5和Gemini 2.5 Pro 87.9%、62.9%和92.4%的失败案例，凸显当前大模型在基本算术上的根本性局限。",
      "summaryZh": "尽管前沿AI模型能在国际数学竞赛中获奖，但该研究系统揭示其在基础整数加法上存在严重缺陷：随着位数增加，所有主流模型准确率显著下降；错误主要源于操作数错位（与分词相关）和进位失败（表现为独立随机错误），这两类错误分别解释了Claude Opus 4.1、GPT-5和Gemini 2.5 Pro 87.9%、62.9%和92.4%的失败案例，凸显当前大模型在基本算术上的根本性局限。",
      "summaryEn": "Despite excelling in advanced mathematics, frontier AI models exhibit severe deficiencies in basic integer addition, with accuracy sharply declining as digit count increases. This study shows that errors primarily stem from operand misalignment (linked to tokenization) and carry failures (appearing as independent random errors), collectively explaining 87.9%, 62.9%, and 92.4% of mistakes by Claude Opus 4.1, GPT-5, and Gemini 2.5 Pro, respectively—highlighting a fundamental limitation in current large language models’ arithmetic reasoning.",
      "fullText": "",
      "imageUrl": "",
      "tags": [
        "LLM",
        "Research"
      ],
      "paperCategory": "General AI",
      "signalReasons": [
        "来源：arXiv",
        "跨源重复：1 个来源",
        "模型评分：9/10，理由：直击AI在基础算术能力上的致命短板，提出可解释、可验证的解决方案，对AI可信性与通用智能发展具有战略级意义。",
        "热度：14 / 评论 0"
      ],
      "score": 9.0,
      "publishedAt": "2026-02-11T01:56:39+00:00",
      "authors": [
        "Alex Bie",
        "Travis Dick",
        "Alex Kulesza"
      ]
    },
    {
      "id": "arxiv_2602_10295v1",
      "title": "ECHO: An Open Research Platform for Evaluation of Chat, Human Behavior, and Outcomes",
      "titleZh": "ECHO: An Open Research Platform for Evaluation of Chat, Human Behavior, and Outcomes",
      "titleEn": "ECHO: An Open Research Platform for Evaluation of Chat, Human Behavior, and Outcomes",
      "url": "https://arxiv.org/abs/2602.10295v1",
      "type": "paper",
      "source": "arXiv",
      "summary": "ECHO是一个开源研究平台，支持跨学科研究者以低代码方式开展关于人类与对话AI及搜索引擎交互的混合方法研究，整合知情同意、背景调查、信息检索会话、写作/判断任务及前后测评估于一体，记录细粒度交互轨迹并导出结构化数据集；通过同时支持聊天与搜索范式及灵活评估工具，显著降低信息检索、人机交互和社会科学领域开展可复现、规模化以人为中心AI评估的技术门槛。",
      "summaryZh": "ECHO是一个开源研究平台，支持跨学科研究者以低代码方式开展关于人类与对话AI及搜索引擎交互的混合方法研究，整合知情同意、背景调查、信息检索会话、写作/判断任务及前后测评估于一体，记录细粒度交互轨迹并导出结构化数据集；通过同时支持聊天与搜索范式及灵活评估工具，显著降低信息检索、人机交互和社会科学领域开展可复现、规模化以人为中心AI评估的技术门槛。",
      "summaryEn": "ECHO is an open research platform enabling interdisciplinary researchers to conduct reproducible, mixed-method studies on human interaction with conversational AI and web search through a low-code framework. It integrates consent, background surveys, chat- or search-based information-seeking sessions, writing/judgment tasks, and pre/post evaluations, logging fine-grained interaction traces and exporting structured datasets. By supporting both paradigms alongside flexible evaluation instruments, ECHO lowers technical barriers for scalable, human-centered AI research in information retrieval, HCI, and social sciences.",
      "fullText": "",
      "imageUrl": "",
      "tags": [
        "RAG",
        "Research",
        "Benchmark"
      ],
      "paperCategory": "General AI",
      "signalReasons": [
        "来源：arXiv",
        "跨源重复：1 个来源",
        "模型评分：9/10，理由：构建开放研究平台ECHO，支持跨学科人机交互研究，推动AI评估标准化与可复现性，对全球AI伦理与社会影响研究具有里程碑意义。",
        "热度：6 / 评论 0"
      ],
      "score": 9.0,
      "publishedAt": "2026-02-10T21:10:38+00:00",
      "authors": [
        "Jiqun Liu",
        "Nischal Dinesh",
        "Ran Yu"
      ]
    },
    {
      "id": "arxiv_2602_10429v1",
      "title": "AIvilization v0: Toward Large-Scale Artificial Social Simulation with a Unified Agent Architecture and Adaptive Agent Profiles",
      "titleZh": "AIvilization v0: Toward Large-Scale Artificial Social Simulation with a Unified Agent Architecture and Adaptive Agent Profiles",
      "titleEn": "AIvilization v0: Toward Large-Scale Artificial Social Simulation with a Unified Agent Architecture and Adaptive Agent Profiles",
      "url": "https://arxiv.org/abs/2602.10429v1",
      "type": "paper",
      "source": "arXiv",
      "summary": "AIvilization v0是一个公开部署的大规模人工社会模拟平台，结合资源受限的沙盒经济与统一LLM智能体架构，通过分层分支思维规划器、双过程记忆的自适应智能体画像及人在环路引导接口，实现长期自主性与环境适应性的平衡；其经济系统包含生存成本、多层级生产、AMM定价机制和教育-职业通道，高频交易数据显示市场重现重尾收益与波动聚集等典型事实，并形成由教育与资源获取差异驱动的财富分层，消融实验表明完整架构在多目标长期任务中更具鲁棒性。",
      "summaryZh": "AIvilization v0是一个公开部署的大规模人工社会模拟平台，结合资源受限的沙盒经济与统一LLM智能体架构，通过分层分支思维规划器、双过程记忆的自适应智能体画像及人在环路引导接口，实现长期自主性与环境适应性的平衡；其经济系统包含生存成本、多层级生产、AMM定价机制和教育-职业通道，高频交易数据显示市场重现重尾收益与波动聚集等典型事实，并形成由教育与资源获取差异驱动的财富分层，消融实验表明完整架构在多目标长期任务中更具鲁棒性。",
      "summaryEn": "AIvilization v0 is a publicly deployed large-scale artificial society combining a resource-constrained sandbox economy with a unified LLM-agent architecture to balance long-horizon autonomy and environmental adaptability. It features a hierarchical branch-thinking planner, an adaptive agent profile with dual-process memory, and a human-in-the-loop steering interface. The economy incorporates survival costs, multi-tier production, an AMM-based pricing mechanism, and an education-occupation system. High-frequency transaction data reveal stylized market facts (heavy-tailed returns, volatility clustering) and structured wealth stratification driven by education and access constraints. Ablations confirm the full architecture’s superior robustness in multi-objective, long-horizon settings.",
      "fullText": "",
      "imageUrl": "",
      "tags": [
        "LLM",
        "Agent"
      ],
      "paperCategory": "General AI",
      "signalReasons": [
        "来源：arXiv",
        "跨源重复：1 个来源",
        "模型评分：9/10，理由：发布首个公开可运行的大规模人工社会仿真系统AIvilization v0，以统一智能体架构实现长期自治模拟，为社会科学研究提供全新范式。",
        "热度：14 / 评论 0"
      ],
      "score": 9.0,
      "publishedAt": "2026-02-11T02:18:15+00:00",
      "authors": [
        "Wenkai Fan",
        "Shurui Zhang",
        "Xiaolong Wang"
      ]
    },
    {
      "id": "arxiv_2602_10639v1",
      "title": "VideoSTF: Stress-Testing Output Repetition in Video Large Language Models",
      "titleZh": "VideoSTF: Stress-Testing Output Repetition in Video Large Language Models",
      "titleEn": "VideoSTF: Stress-Testing Output Repetition in Video Large Language Models",
      "url": "https://arxiv.org/abs/2602.10639v1",
      "type": "paper",
      "source": "arXiv",
      "summary": "主要内容：Video Large Language Models (VideoLLMs) have recently achieved strong performance in video understanding tasks. However,...；关键点：VideoSTF: Stress-Testing Output Repetition in Video Large La；为什么重要：这是近期 AI 领域值得关注的进展。",
      "summaryZh": "主要内容：Video Large Language Models (VideoLLMs) have recently achieved strong performance in video understanding tasks. However,...；关键点：VideoSTF: Stress-Testing Output Repetition in Video Large La；为什么重要：这是近期 AI 领域值得关注的进展。",
      "summaryEn": "Main point: Video Large Language Models (VideoLLMs) have recently achieved strong performance in video understanding tasks. However, we identify a previously unde.... Key takeaway: VideoSTF: Stress-Testing Output Repetition in Video Large Language Models. Why it matters: this is a notable AI development to track.",
      "fullText": "",
      "imageUrl": "",
      "tags": [
        "LLM",
        "Open Source",
        "Benchmark"
      ],
      "paperCategory": "Computer Vision",
      "signalReasons": [
        "来源：arXiv",
        "跨源重复：1 个来源",
        "模型评分：9/10，理由：首次系统揭示视频大模型中的输出重复缺陷，提出VideoSTF评测框架，对视频LLM可靠性评估形成行业标准影响。",
        "热度：11 / 评论 0"
      ],
      "score": 9.0,
      "publishedAt": "2026-02-11T08:40:48+00:00",
      "authors": [
        "Yuxin Cao",
        "Wei Song",
        "Shangzhi Xu"
      ]
    },
    {
      "id": "arxiv_2602_11130v1",
      "title": "From Circuits to Dynamics: Understanding and Stabilizing Failure in 3D Diffusion Transformers",
      "titleZh": "From Circuits to Dynamics: Understanding and Stabilizing Failure in 3D Diffusion Transformers",
      "titleEn": "From Circuits to Dynamics: Understanding and Stabilizing Failure in 3D Diffusion Transformers",
      "url": "https://arxiv.org/abs/2602.11130v1",
      "type": "paper",
      "source": "arXiv",
      "summary": "主要内容：Reliable surface completion from sparse point clouds underpins many applications spanning content creation and robotics....；关键点：From Circuits to Dynamics: Understanding and Stabilizing Fai；为什么重要：这是近期 AI 领域值得关注的进展。",
      "summaryZh": "主要内容：Reliable surface completion from sparse point clouds underpins many applications spanning content creation and robotics....；关键点：From Circuits to Dynamics: Understanding and Stabilizing Fai；为什么重要：这是近期 AI 领域值得关注的进展。",
      "summaryEn": "Main point: Reliable surface completion from sparse point clouds underpins many applications spanning content creation and robotics. While 3D diffusion transforme.... Key takeaway: From Circuits to Dynamics: Understanding and Stabilizing Failure in 3D Diffusion. Why it matters: this is a notable AI development to track.",
      "fullText": "",
      "imageUrl": "",
      "tags": [
        "Robotics",
        "Diffusion",
        "3D",
        "RAG"
      ],
      "paperCategory": "Computer Vision",
      "signalReasons": [
        "来源：arXiv",
        "跨源重复：1 个来源",
        "模型评分：9/10，理由：揭示3D扩散Transformer的灾难性失败机制并提出稳定方案，直击生成模型可靠性核心痛点，对AI安全与工业落地有战略意义。",
        "热度：21 / 评论 0"
      ],
      "score": 9.0,
      "publishedAt": "2026-02-11T18:42:05+00:00",
      "authors": [
        "Maximilian Plattner",
        "Fabian Paischer",
        "Johannes Brandstetter"
      ]
    },
    {
      "id": "arxiv_2602_10983v1",
      "title": "Scaling World Model for Hierarchical Manipulation Policies",
      "titleZh": "Scaling World Model for Hierarchical Manipulation Policies",
      "titleEn": "Scaling World Model for Hierarchical Manipulation Policies",
      "url": "https://arxiv.org/abs/2602.10983v1",
      "type": "paper",
      "source": "arXiv",
      "summary": "主要内容：Vision-Language-Action (VLA) models are promising for generalist robot manipulation but remain brittle in out-of-distrib...；关键点：Scaling World Model for Hierarchical Manipulation Policies；为什么重要：这是近期 AI 领域值得关注的进展。",
      "summaryZh": "主要内容：Vision-Language-Action (VLA) models are promising for generalist robot manipulation but remain brittle in out-of-distrib...；关键点：Scaling World Model for Hierarchical Manipulation Policies；为什么重要：这是近期 AI 领域值得关注的进展。",
      "summaryEn": "Main point: Vision-Language-Action (VLA) models are promising for generalist robot manipulation but remain brittle in out-of-distribution (OOD) settings, especial.... Key takeaway: Scaling World Model for Hierarchical Manipulation Policies. Why it matters: this is a notable AI development to track.",
      "fullText": "",
      "imageUrl": "",
      "tags": [
        "Vision",
        "Multimodal",
        "Robotics",
        "RAG"
      ],
      "paperCategory": "Robotics",
      "signalReasons": [
        "来源：arXiv",
        "跨源重复：1 个来源",
        "模型评分：9/10，理由：构建分层视觉-语言-动作框架以提升机器人在分布外场景的泛化能力，是通用机器人控制的关键突破，具有全球产业变革潜力。",
        "热度：16 / 评论 0"
      ],
      "score": 9.0,
      "publishedAt": "2026-02-11T16:12:33+00:00",
      "authors": [
        "Qian Long",
        "Yueze Wang",
        "Jiaxi Song"
      ]
    },
    {
      "id": "arxiv_2602_11021v1",
      "title": "ContactGaussian-WM: Learning Physics-Grounded World Model from Videos",
      "titleZh": "ContactGaussian-WM: Learning Physics-Grounded World Model from Videos",
      "titleEn": "ContactGaussian-WM: Learning Physics-Grounded World Model from Videos",
      "url": "https://arxiv.org/abs/2602.11021v1",
      "type": "paper",
      "source": "arXiv",
      "summary": "主要内容：Developing world models that understand complex physical interactions is essential for advancing robotic planning and si...；关键点：ContactGaussian-WM: Learning Physics-Grounded World Model fr；为什么重要：这是近期 AI 领域值得关注的进展。",
      "summaryZh": "主要内容：Developing world models that understand complex physical interactions is essential for advancing robotic planning and si...；关键点：ContactGaussian-WM: Learning Physics-Grounded World Model fr；为什么重要：这是近期 AI 领域值得关注的进展。",
      "summaryEn": "Main point: Developing world models that understand complex physical interactions is essential for advancing robotic planning and simulation.However, existing met.... Key takeaway: ContactGaussian-WM: Learning Physics-Grounded World Model from Videos. Why it matters: this is a notable AI development to track.",
      "fullText": "",
      "imageUrl": "",
      "tags": [
        "Robotics",
        "3D",
        "Benchmark"
      ],
      "paperCategory": "Robotics",
      "signalReasons": [
        "来源：arXiv",
        "跨源重复：1 个来源",
        "模型评分：8/10，理由：基于物理约束的世界模型从视频中学习接触动力学，对机器人规划与仿真具有重要意义，是具身智能领域的关键进展。",
        "热度：10 / 评论 0"
      ],
      "score": 8.0,
      "publishedAt": "2026-02-11T16:48:13+00:00",
      "authors": [
        "Meizhong Wang",
        "Wanxin Jin",
        "Kun Cao"
      ]
    },
    {
      "id": "arxiv_2602_11146v1",
      "title": "Beyond VLM-Based Rewards: Diffusion-Native Latent Reward Modeling",
      "titleZh": "Beyond VLM-Based Rewards: Diffusion-Native Latent Reward Modeling",
      "titleEn": "Beyond VLM-Based Rewards: Diffusion-Native Latent Reward Modeling",
      "url": "https://arxiv.org/abs/2602.11146v1",
      "type": "paper",
      "source": "arXiv",
      "summary": "主要内容：Preference optimization for diffusion and flow-matching models relies on reward functions that are both discriminatively...；关键点：Beyond VLM-Based Rewards: Diffusion-Native Latent Reward Mod；为什么重要：这是近期 AI 领域值得关注的进展。",
      "summaryZh": "主要内容：Preference optimization for diffusion and flow-matching models relies on reward functions that are both discriminatively...；关键点：Beyond VLM-Based Rewards: Diffusion-Native Latent Reward Mod；为什么重要：这是近期 AI 领域值得关注的进展。",
      "summaryEn": "Main point: Preference optimization for diffusion and flow-matching models relies on reward functions that are both discriminatively robust and computationally ef.... Key takeaway: Beyond VLM-Based Rewards: Diffusion-Native Latent Reward Modeling. Why it matters: this is a notable AI development to track.",
      "fullText": "",
      "imageUrl": "",
      "tags": [
        "LLM",
        "Vision",
        "Multimodal",
        "Diffusion"
      ],
      "paperCategory": "Computer Vision",
      "signalReasons": [
        "来源：arXiv",
        "跨源重复：1 个来源",
        "模型评分：8/10，理由：提出扩散模型原生的潜在奖励建模方法，突破依赖VLM的奖励瓶颈，有望提升生成模型对齐效率，是训练范式的重要演进。",
        "热度：19 / 评论 0"
      ],
      "score": 8.0,
      "publishedAt": "2026-02-11T18:57:29+00:00",
      "authors": [
        "Gongye Liu",
        "Bo Yang",
        "Yida Zhi"
      ]
    },
    {
      "id": "arxiv_2602_10722v1",
      "title": "A Diffusion-Based Generative Prior Approach to Sparse-view Computed Tomography",
      "titleZh": "A Diffusion-Based Generative Prior Approach to Sparse-view Computed Tomography",
      "titleEn": "A Diffusion-Based Generative Prior Approach to Sparse-view Computed Tomography",
      "url": "https://arxiv.org/abs/2602.10722v1",
      "type": "paper",
      "source": "arXiv",
      "summary": "主要内容：The reconstruction of X-rays CT images from sparse or limited-angle geometries is a highly challenging task. The lack of...；关键点：A Diffusion-Based Generative Prior Approach to Sparse-view C；为什么重要：这是近期 AI 领域值得关注的进展。",
      "summaryZh": "主要内容：The reconstruction of X-rays CT images from sparse or limited-angle geometries is a highly challenging task. The lack of...；关键点：A Diffusion-Based Generative Prior Approach to Sparse-view C；为什么重要：这是近期 AI 领域值得关注的进展。",
      "summaryEn": "Main point: The reconstruction of X-rays CT images from sparse or limited-angle geometries is a highly challenging task. The lack of data typically results in art.... Key takeaway: A Diffusion-Based Generative Prior Approach to Sparse-view Computed Tomography. Why it matters: this is a notable AI development to track.",
      "fullText": "",
      "imageUrl": "",
      "tags": [
        "Vision",
        "Diffusion",
        "Research"
      ],
      "paperCategory": "Computer Vision",
      "signalReasons": [
        "来源：arXiv",
        "跨源重复：1 个来源",
        "模型评分：8/10，理由：采用扩散模型作为先验解决稀疏视角CT重建难题，显著提升图像质量与临床可用性，对医学影像AI具有重大推动作用。",
        "热度：6 / 评论 0"
      ],
      "score": 8.0,
      "publishedAt": "2026-02-11T10:27:41+00:00",
      "authors": [
        "Davide Evangelista",
        "Pasquale Cascarano",
        "Elena Loli Piccolomini"
      ]
    },
    {
      "id": "arxiv_2602_10698v1",
      "title": "AugVLA-3D: Depth-Driven Feature Augmentation for Vision-Language-Action Models",
      "titleZh": "AugVLA-3D: Depth-Driven Feature Augmentation for Vision-Language-Action Models",
      "titleEn": "AugVLA-3D: Depth-Driven Feature Augmentation for Vision-Language-Action Models",
      "url": "https://arxiv.org/abs/2602.10698v1",
      "type": "paper",
      "source": "arXiv",
      "summary": "主要内容：Vision-Language-Action (VLA) models have recently achieved remarkable progress in robotic perception and control, yet mo...；关键点：AugVLA-3D: Depth-Driven Feature Augmentation for Vision-Lang；为什么重要：这是近期 AI 领域值得关注的进展。",
      "summaryZh": "主要内容：Vision-Language-Action (VLA) models have recently achieved remarkable progress in robotic perception and control, yet mo...；关键点：AugVLA-3D: Depth-Driven Feature Augmentation for Vision-Lang；为什么重要：这是近期 AI 领域值得关注的进展。",
      "summaryEn": "Main point: Vision-Language-Action (VLA) models have recently achieved remarkable progress in robotic perception and control, yet most existing approaches primari.... Key takeaway: AugVLA-3D: Depth-Driven Feature Augmentation for Vision-Language-Action Models. Why it matters: this is a notable AI development to track.",
      "fullText": "",
      "imageUrl": "",
      "tags": [
        "Vision",
        "Multimodal",
        "Robotics",
        "3D"
      ],
      "paperCategory": "Computer Vision",
      "signalReasons": [
        "来源：arXiv",
        "跨源重复：1 个来源",
        "模型评分：8/10，理由：聚焦3D视觉-语言-动作模型的深度特征增强，对机器人感知与控制有实质性推动，符合当前AI+机器人关键方向。",
        "热度：10 / 评论 0"
      ],
      "score": 8.0,
      "publishedAt": "2026-02-11T09:57:32+00:00",
      "authors": [
        "Zhifeng Rao",
        "Wenlong Chen",
        "Lei Xie"
      ]
    },
    {
      "id": "arxiv_2602_10719v1",
      "title": "From Representational Complementarity to Dual Systems: Synergizing VLM and Vision-Only Backbones for End-to-End Driving",
      "titleZh": "From Representational Complementarity to Dual Systems: Synergizing VLM and Vision-Only Backbones for End-to-End Driving",
      "titleEn": "From Representational Complementarity to Dual Systems: Synergizing VLM and Vision-Only Backbones for End-to-End Driving",
      "url": "https://arxiv.org/abs/2602.10719v1",
      "type": "paper",
      "source": "arXiv",
      "summary": "主要内容：Vision-Language-Action (VLA) driving augments end-to-end (E2E) planning with language-enabled backbones, yet it remains ...；关键点：From Representational Complementarity to Dual Systems: Syner；为什么重要：这是近期 AI 领域值得关注的进展。",
      "summaryZh": "主要内容：Vision-Language-Action (VLA) driving augments end-to-end (E2E) planning with language-enabled backbones, yet it remains ...；关键点：From Representational Complementarity to Dual Systems: Syner；为什么重要：这是近期 AI 领域值得关注的进展。",
      "summaryEn": "Main point: Vision-Language-Action (VLA) driving augments end-to-end (E2E) planning with language-enabled backbones, yet it remains unclear what changes beyond th.... Key takeaway: From Representational Complementarity to Dual Systems: Synergizing VLM and Visio. Why it matters: this is a notable AI development to track.",
      "fullText": "",
      "imageUrl": "",
      "tags": [
        "Vision",
        "Multimodal",
        "Diffusion",
        "Training"
      ],
      "paperCategory": "Robotics",
      "signalReasons": [
        "来源：arXiv",
        "跨源重复：1 个来源",
        "模型评分：8/10，理由：探索视觉-语言-动作协同机制在端到端驾驶中的互补性，推动智能驾驶系统从感知向认知演进，具备行业引领潜力。",
        "热度：13 / 评论 0"
      ],
      "score": 8.0,
      "publishedAt": "2026-02-11T10:25:05+00:00",
      "authors": [
        "Sining Ang",
        "Yuguang Yang",
        "Chenxu Dang"
      ]
    },
    {
      "id": "arxiv_2602_11113v1",
      "title": "A receding-horizon multi-contact motion planner for legged robots in challenging environments",
      "titleZh": "A receding-horizon multi-contact motion planner for legged robots in challenging environments",
      "titleEn": "A receding-horizon multi-contact motion planner for legged robots in challenging environments",
      "url": "https://arxiv.org/abs/2602.11113v1",
      "type": "paper",
      "source": "arXiv",
      "summary": "该论文提出一种新型滚动时域多接触运动规划器，使足式机器人能在烟囱攀爬、穿越狭窄通道或跨越大间隙等复杂环境中自主规划全身轨迹与接触点。其方法支持实时重规划，避免了传统多阶段流程和后处理需求，并在局部极小值问题上优于基于势场的方法；实验表明，在短规划视距（如一步）下，规划速度比现有技术快45%至98%，尽管动作效率略低；而在四步视距下虽耗时增加（最慢达4倍），却显著减少支撑切换次数（最多降低47%），提升动作质量。",
      "summaryZh": "该论文提出一种新型滚动时域多接触运动规划器，使足式机器人能在烟囱攀爬、穿越狭窄通道或跨越大间隙等复杂环境中自主规划全身轨迹与接触点。其方法支持实时重规划，避免了传统多阶段流程和后处理需求，并在局部极小值问题上优于基于势场的方法；实验表明，在短规划视距（如一步）下，规划速度比现有技术快45%至98%，尽管动作效率略低；而在四步视距下虽耗时增加（最慢达4倍），却显著减少支撑切换次数（最多降低47%），提升动作质量。",
      "summaryEn": "This paper presents a novel receding-horizon multi-contact motion planner enabling legged robots to autonomously navigate challenging environments like chimney climbing, narrow passages, or large gaps by simultaneously planning whole-body trajectories and contact locations. The approach supports reactive re-planning, eliminates post-processing or multi-stage pipelines, and is more robust to local minima than potential-field methods. Experiments show that with short horizons (e.g., one step), it plans 45–98% faster than state-of-the-art—albeit with less efficient motions—while longer horizons (e.g., four steps) increase planning time (up to 4× slower) but yield higher-quality plans with up to 47% fewer stance changes.",
      "fullText": "",
      "imageUrl": "",
      "tags": [
        "Robotics",
        "RAG"
      ],
      "paperCategory": "Robotics",
      "signalReasons": [
        "来源：arXiv",
        "跨源重复：1 个来源",
        "模型评分：8/10，理由：提出用于复杂环境下的足式机器人多接触运动规划方法，突破现有机器人自主能力边界，推动具身智能发展。",
        "热度：9 / 评论 0"
      ],
      "score": 8.0,
      "publishedAt": "2026-02-11T18:25:29+00:00",
      "authors": [
        "Daniel S. J. Derwent",
        "Simon Watson",
        "Bruno V. Adorno"
      ]
    },
    {
      "id": "arxiv_2602_10561v1",
      "title": "Morphogenetic Assembly and Adaptive Control for Heterogeneous Modular Robots",
      "titleZh": "Morphogenetic Assembly and Adaptive Control for Heterogeneous Modular Robots",
      "titleEn": "Morphogenetic Assembly and Adaptive Control for Heterogeneous Modular Robots",
      "url": "https://arxiv.org/abs/2602.10561v1",
      "type": "paper",
      "source": "arXiv",
      "summary": "该研究提出一个面向异构模块化机器人的闭环自动化框架，涵盖从形态构建到自适应控制的完整流程：移动机械臂可动态组装结构、关节和轮式模块，形成具备即时移动能力的多样化构型；为应对大规模异构重构中的状态爆炸问题，采用分层规划器——高层使用带类型惩罚项的双向启发式搜索生成模块操作序列，底层用A*算法计算执行轨迹；针对未知构型的运动控制，引入GPU加速的退火方差MPPI控制器，通过多阶段方差退火策略实现配置无关的实时控制（50 Hz）。仿真验证了该框架在模块组装、合并/拆分及动态运动生成上的有效性。",
      "summaryZh": "该研究提出一个面向异构模块化机器人的闭环自动化框架，涵盖从形态构建到自适应控制的完整流程：移动机械臂可动态组装结构、关节和轮式模块，形成具备即时移动能力的多样化构型；为应对大规模异构重构中的状态爆炸问题，采用分层规划器——高层使用带类型惩罚项的双向启发式搜索生成模块操作序列，底层用A*算法计算执行轨迹；针对未知构型的运动控制，引入GPU加速的退火方差MPPI控制器，通过多阶段方差退火策略实现配置无关的实时控制（50 Hz）。仿真验证了该框架在模块组装、合并/拆分及动态运动生成上的有效性。",
      "summaryEn": "This paper introduces a closed-loop automation framework for heterogeneous modular robots that covers the full pipeline from morphological assembly to adaptive control. A mobile manipulator dynamically assembles structural, joint, and wheeled modules into diverse configurations with immediate locomotion capability. To tackle state-space explosion during large-scale reconfiguration, a hierarchical planner is proposed: a high-level bidirectional heuristic search with type-penalty terms generates module-handling sequences, while a low-level A* planner computes optimal execution trajectories. For unknown configurations, a GPU-accelerated Annealing-Variance MPPI controller enables configuration-agnostic real-time motion control at 50 Hz via multi-stage variance annealing. Large-scale simulations validate the framework’s effectiveness in assembly, merging/splitting, and dynamic motion generation.",
      "fullText": "",
      "imageUrl": "",
      "tags": [
        "Robotics",
        "Research"
      ],
      "paperCategory": "Robotics",
      "signalReasons": [
        "来源：arXiv",
        "跨源重复：1 个来源",
        "模型评分：8/10，理由：异构模块化机器人形态生成与自适应控制闭环框架，代表智能机器人系统集成的重大进步，影响未来柔性自动化方向。",
        "热度：11 / 评论 0"
      ],
      "score": 8.0,
      "publishedAt": "2026-02-11T06:18:04+00:00",
      "authors": [
        "Chongxi Meng",
        "Da Zhao",
        "Yifei Zhao"
      ]
    }
  ],
  "news": [
    {
      "id": "hn_46981058",
      "title": "Anthropic承诺承担数据中心推高居民电价的成本",
      "titleZh": "Anthropic承诺承担数据中心推高居民电价的成本",
      "titleEn": "Anthropic Pledges to Cover Electricity Price Hikes Caused by Its Data Centers",
      "url": "https://www.anthropic.com/news/covering-electricity-price-increases",
      "type": "news",
      "source": "Hacker News",
      "summary": "**Anthropic承诺承担其数据中心造成的电价上涨成本**，包括全额支付电网升级费用、采购新增电力以抵消需求冲击、部署削峰系统减轻电网压力，并投资本地社区创造就业与采用节水冷却技术；此举旨在回应AI算力激增（单个前沿模型训练需吉瓦级电力）对居民电费的潜在影响，确保美国在AI与国家安全竞争中不以牺牲消费者为代价，同时呼吁联邦层面加快能源基础设施审批与并网改革，推动AI基建成为国家能源转型的催化剂。",
      "summaryZh": "**Anthropic承诺承担其数据中心造成的电价上涨成本**，包括全额支付电网升级费用、采购新增电力以抵消需求冲击、部署削峰系统减轻电网压力，并投资本地社区创造就业与采用节水冷却技术；此举旨在回应AI算力激增（单个前沿模型训练需吉瓦级电力）对居民电费的潜在影响，确保美国在AI与国家安全竞争中不以牺牲消费者为代价，同时呼吁联邦层面加快能源基础设施审批与并网改革，推动AI基建成为国家能源转型的催化剂。",
      "summaryEn": "Anthropic commits to covering electricity price increases caused by its data centers by fully paying for grid infrastructure upgrades, procuring new power generation to offset demand-driven price spikes, deploying curtailment systems to reduce peak-load strain, and investing in local communities through jobs and water-efficient cooling. This addresses concerns that AI’s soaring power demand—up to gigawatts per frontier model—could burden ratepayers as the U.S. races to build data centers for AI competitiveness and national security. Anthropic also advocates federal reforms to accelerate energy permitting and grid interconnection, positioning AI infrastructure as a catalyst for broader clean energy investment.",
      "fullText": "Skip to main content Skip to footer Research Economic Futures Commitments Learn News Try Claude Policy Covering electricity price increases from our data centers Feb 11, 2026 As we continue to invest in American AI infrastructure , Anthropic will cover electricity price increases that consumers face from our data centers. Training a single frontier AI model will soon require gigawatts of power, and the US AI sector will need at least 50 gigawatts of capacity over the next several years. The country needs to build new data centers quickly to maintain its competitiveness on AI and national security—but AI companies shouldn’t leave American ratepayers to pick up the tab. Data centers can raise consumer electricity prices in two main ways. First, connecting data centers to the grid often requires costly new or upgraded infrastructure like transmission lines or substations. Second, new demand tightens the market, pushing up prices. We’re committing to address both. Specifically, we will: Cover grid infrastructure costs . We will pay for 100% of the grid upgrades needed to interconnect our data centers, paid through increases to our monthly electricity charges. This includes the shares of these costs that would otherwise be passed onto consumers. Procure new power and protect consumers from price increases . We will work to bring net-new power generation online to match our data centers’ electricity needs. Where new generation isn’t online, we’ll work with utilities and external experts to estimate and cover demand-driven price effects from our data centers. Reduce strain on the grid . We’re investing in curtailment systems that cut our data centers’ power usage during periods of peak demand, as well as grid optimization tools, both of which help keep prices lower for ratepayers. Invest in local communities. Our current data center projects will create hundreds of permanent jobs and thousands of construction jobs. We’re also committed to being a responsible neighbor—that means addressing environmental impacts, including deploying water-efficient cooling technologies, and partnering with local leaders on initiatives that share AI’s benefits broadly. Where we work with partners to develop data centers for handling our own workloads, we make these commitments directly. Where we lease capacity from existing data centers, we’re exploring further ways to address our own workloads' effects on prices. Of course, company-level action isn't enough. Keeping electricity affordable also requires systemic change. We support federal policies —including permitting reform and efforts to speed up transmission development and grid interconnection—that make it faster and cheaper to bring new energy online for everyone. Done right, AI infrastructure can be a catalyst for the broader energy investment the country needs. These commitments are the beginning of our efforts to address data centers’ impact on energy costs. We have more to do, and we’ll continue to share updates as this work develops. Related content Anthropic is donating $20 million to Public First Action Read more Introducing Claude Opus 4.6 We’re upgrading our smartest model. Across agentic coding, computer use, tool use, search, and finance, Opus 4.6 is an industry-leading model, often by wide margin. Read more Claude is a space to think We’ve made a choice: Claude will remain ad-free. We explain why advertising incentives are incompatible with a genuinely helpful AI assistant, and how we plan to expand access without compromising user trust. Read more Products Claude Claude Code Cowork Claude in Chrome Claude in Excel Claude in PowerPoint Claude in Slack Skills Max plan Team plan Enterprise plan Download app Pricing Log in to Claude Models Opus Sonnet Haiku Solutions AI agents Code modernization Coding Customer support Education Financial services Government Healthcare Life sciences Nonprofits Claude Developer Platform Overview Developer docs Pricing Regional compliance Amazon Bedrock Google Cloud’s Vertex AI Console login Learn Blog Claude partner network Connectors Courses Customer stories Engineering at Anthropic Events Plugins Powered by Claude Service partners Startups program Tutorials Use cases Company Anthropic Careers Economic Futures Research News Claude’s Constitution Responsible Scaling Policy Security and compliance Transparency Help and security Availability Status Support center Terms and policies Privacy policy Consumer health data privacy policy Responsible disclosure policy Terms of service: Commercial Terms of service: Consumer Usage policy © 2026 Anthropic PBC Covering electricity price increases from our data centers \\ Anthropic",
      "imageUrl": "https://image.pollinations.ai/prompt/covering+electricity+price+increases+our+Hacker+News+AI+technology?width=1600&height=900&seed=covering+electricity+price+increases+our&nologo=true",
      "tags": [],
      "paperCategory": "",
      "signalReasons": [
        "来源权威：官方/白名单",
        "跨源重复：1 个来源",
        "模型评分：9/10，理由：Anthropic承诺承担数据中心电力成本，直面AI基础设施扩张带来的社会成本问题，具有全球产业政策层面的战略意义，可能推动美国乃至全球AI可持续发展范式变革。",
        "热度：121 / 评论 93"
      ],
      "score": 10.97,
      "publishedAt": "2026-02-11T21:12:59+00:00",
      "authors": [
        "ryanhn"
      ]
    },
    {
      "id": "rss_4241910138",
      "title": "NVIDIA Blackwell助推理服务商降本10倍，开源模型成AI普惠关键",
      "titleZh": "NVIDIA Blackwell助推理服务商降本10倍，开源模型成AI普惠关键",
      "titleEn": "Inference Providers Slash AI Costs 10x with Open Models on NVIDIA Blackwell",
      "url": "https://blogs.nvidia.com/blog/inference-open-source-models-blackwell-reduce-cost-per-token/",
      "type": "news",
      "source": "NVIDIA Blog",
      "summary": "**多家领先推理服务商通过在NVIDIA Blackwell平台部署开源大模型，将AI推理成本降低最高10倍**：Baseten与Sully.ai在医疗领域将医学编码等任务的推理成本削减90%；DeepInfra助Latitude游戏公司实现4倍token成本下降；Fireworks AI使Sentient Chat多智能体系统成本效率提升25–50%；Together AI与Decagon将语音客服单次交互成本降低6倍。这些成果源于Blackwell的极致软硬件协同设计、低精度NVFP4格式及优化推理栈，显著改善“token经济性”，让企业能负担得起规模化AI应用，普通用户则可享受更流畅、低价甚至免费的AI服务。",
      "summaryZh": "**多家领先推理服务商通过在NVIDIA Blackwell平台部署开源大模型，将AI推理成本降低最高10倍**：Baseten与Sully.ai在医疗领域将医学编码等任务的推理成本削减90%；DeepInfra助Latitude游戏公司实现4倍token成本下降；Fireworks AI使Sentient Chat多智能体系统成本效率提升25–50%；Together AI与Decagon将语音客服单次交互成本降低6倍。这些成果源于Blackwell的极致软硬件协同设计、低精度NVFP4格式及优化推理栈，显著改善“token经济性”，让企业能负担得起规模化AI应用，普通用户则可享受更流畅、低价甚至免费的AI服务。",
      "summaryEn": "Leading inference providers—including Baseten, DeepInfra, Fireworks AI, and Together AI—are cutting AI inference costs by up to 10× by running open-source frontier models on NVIDIA’s Blackwell platform. In healthcare, Sully.ai reduced medical coding costs by 90%; in gaming, Latitude achieved 4× lower token cost; Sentient Chat improved cost efficiency by 25–50%; and Decagon slashed voice-agent query costs by 6×. These gains stem from Blackwell’s extreme hardware-software co-design, NVFP4 low-precision format, and optimized inference stacks, dramatically improving tokenomics. This enables businesses to scale AI affordably and allows consumers to access faster, cheaper, or even free AI-powered services across healthcare, gaming, and customer support.",
      "fullText": "A diagnostic insight in healthcare. A character’s dialogue in an interactive game. An autonomous resolution from a customer service agent. Each of these AI-powered interactions is built on the same unit of intelligence: a token. Scaling these AI interactions requires businesses to consider whether they can afford more tokens. The answer lies in better tokenomics — which at its core is about driving down the cost of each token. This downward trend is unfolding across industries. Recent MIT research found that infrastructure and algorithmic efficiencies are reducing inference costs for frontier-level performance by up to 10x annually. To understand how infrastructure efficiency improves tokenomics, consider the analogy of a high-speed printing press. If the press produces 10x output with incremental investment in ink, energy and the machine itself, the cost to print each individual page drops. In the same way, investments in AI infrastructure can lead to far greater token output compared with the increase in cost — causing a meaningful reduction in the cost per token. When token output outpaces infrastructure cost, the cost of each token drops. That’s why leading inference providers including Baseten, DeepInfra, Fireworks AI and Together AI are using the NVIDIA Blackwell platform, which helps them reduce cost per token by up to 10x compared with the NVIDIA Hopper platform. These providers host advanced open source models, which have now reached frontier-level intelligence. By combining open source frontier intelligence, the extreme hardware-software codesign of NVIDIA Blackwell and their own optimized inference stacks, these providers are enabling dramatic token cost reductions for businesses across every industry. Healthcare — Baseten and Sully.ai Cut AI Inference Costs by 10x In healthcare, tedious, time-consuming tasks like medical coding, documentation and managing insurance forms cut into the time doctors can spend with patients. Sully.ai helps solve this problem by developing “AI employees” that can handle routine tasks like medical coding and note-taking. As the company’s platform scaled, its proprietary, closed source models created three bottlenecks: unpredictable latency in real-time clinical workflows, inference costs that scaled faster than revenue and insufficient control over model quality and updates. Sully.ai builds AI employees that handle routine tasks for physicians. To overcome these bottlenecks, Sully.ai uses Baseten’s Model API, which deploys open source models such as gpt-oss-120b on NVIDIA Blackwell GPUs. Baseten used the low-precision NVFP4 data format, the NVIDIA TensorRT-LLM library and the NVIDIA Dynamo inference framework to deliver optimized inference. The company chose NVIDIA Blackwell to run its Model API after seeing up to 2.5x better throughput per dollar compared with the NVIDIA Hopper platform. As a result, Sully.ai’s inference costs dropped by 90%, representing a 10x reduction compared with the prior closed source implementation, while response times improved by 65% for critical workflows like generating medical notes. The company has now returned over 30 million minutes to physicians, time previously lost to data entry and other manual tasks. Gaming — DeepInfra and Latitude Reduce Cost per Token by 4x Latitude is building the future of AI-native gaming with its AI Dungeon adventure-story game and upcoming AI-powered role-playing gaming platform, Voyage, where players can create or play worlds with the freedom to choose any action and make their own story. The company’s platform uses large language models to respond to players’ actions — but this comes with scaling challenges, as every player action triggers an inference request. Costs scale with engagement, and response times must stay fast enough to keep the experience seamless. Latitude has built a text-based adventure-story game called “AI Dungeon,” which generates both narrative text and imagery in real time as players explore dynamic stories. Latitude runs large open source models on DeepInfra’s inference platform, powered by NVIDIA Blackwell GPUs and TensorRT-LLM. For a large-scale mixture-of-experts (MoE) model, DeepInfra reduced the cost per million tokens from 20 cents on the NVIDIA Hopper platform to 10 cents on Blackwell. Moving to Blackwell’s native low-precision NVFP4 format further cut that cost to just 5 cents — for a total 4x improvement in cost per token — while maintaining the accuracy that customers expect. Running these large-scale MoE models on DeepInfra’s Blackwell-powered platform allows Latitude to deliver fast, reliable responses cost effectively. DeepInfra inference platform delivers this performance while reliably handling traffic spikes, letting Latitude deploy more capable models without compromising player experience. Agentic Chat — Fireworks AI and Sentient Foundation Lower AI Costs by up to 50% Sentient Labs is focused on bringing AI developers together to build powerful reasoning AI systems that are all open source. The goal is to accelerate AI toward solving harder reasoning problems through research in secure autonomy, agentic architecture and continual learning. Its first app, Sentient Chat, orchestrates complex multi-agent workflows and integrates more than a dozen specialized AI agents from the community. Due to this, Sentient Chat has massive compute demands because a single user query could trigger a cascade of autonomous interactions that typically lead to costly infrastructure overhead. To manage this scale and complexity, Sentient uses Fireworks AI’s inference platform running on NVIDIA Blackwell. With Fireworks’ Blackwell-optimized inference stack, Sentient achieved 25-50% better cost efficiency compared with its previous Hopper-based deployment. Sentient Chat orchestrates complex multi-agent workflows and integrates more than a dozen specialized AI agents from the community. This higher throughput per GPU allowed the company to serve significantly more concurrent users for the same cost. The platform’s scalability supported a viral launch of 1.8 million waitlisted users in 24 hours and processed 5.6 million queries in a single week while delivering consistent low latency. Customer Service — Together AI and Decagon Drive Down Cost by 6x Customer service calls with voice AI often end in frustration because even a slight delay can lead users to talk over the agent, hang up or lose trust. Decagon builds AI agents for enterprise customer support, with AI-powered voice being its most demanding channel. Decagon needed infrastructure that could deliver sub-second responses under unpredictable traffic loads with tokenomics that supported 24/7 voice deployments. Decagon builds AI agents for customer support, and voice is its most demanding channel. Together AI runs production inference for Decagon’s multimodel voice stack on NVIDIA Blackwell GPUs. The companies collaborated on several key optimizations: speculative decoding that trains smaller models to generate faster responses while a larger model verifies accuracy in the background, caching repeated conversation elements to speed up responses and building automatic scaling that handles traffic surges without degrading performance. Decagon saw response times under 400 milliseconds even when processing thousands of tokens per query. Cost per query, which is the total cost to complete one voice interaction, dropped by 6x compared with using closed source proprietary models. This was achieved through the combination of Decagon’s multimodel approach (some open source, some trained in house on NVIDIA GPUs), NVIDIA Blackwell’s extreme codesign and Together’s optimized inference stack. Optimizing Tokenomics With Extreme Codesign The dramatic cost savings seen across healthcare, gaming and customer service are driven by the efficiency of NVIDIA Blackwell. The NVIDIA GB200 NVL72 system further scales this impact by delivering a breakthrough 10x reduction in cost per token for reasoning MoE models compared with NVIDIA Hopper. NVIDIA’s extreme codesign across every layer of the stack — spanning compute, networking and software — and its partner ecosystem are unlocking massive reductions in cost per token at scale. This momentum continues with the NVIDIA Rubin platform — integrating six new chips into a single AI supercomputer to deliver 10x performance and 10x lower token cost over Blackwell. Explore NVIDIA’s full-stack inference platform to learn more about how it delivers better tokenomics for AI inference.",
      "imageUrl": "https://blogs.nvidia.com/wp-content/uploads/2026/02/inference-press-moe-x-tokenomics-think-smart-blog-4779150-1280x680-1.jpg",
      "tags": [
        "Agent",
        "Inference",
        "Open Source"
      ],
      "paperCategory": "",
      "signalReasons": [
        "来源权威：官方/白名单",
        "跨源重复：1 个来源",
        "模型评分：10/10，理由：基于NVIDIA Blackwell架构实现推理成本下降10倍，是基础设施与算法协同突破的里程碑事件，将重塑AI商业化逻辑，影响未来数年行业格局。",
        "热度：0 / 评论 0"
      ],
      "score": 10.1,
      "publishedAt": "2026-02-12T16:00:46+00:00",
      "authors": [
        "Shruti Koparkar"
      ]
    },
    {
      "id": "rss_3126922440",
      "title": "Gemini 3 Deep Think升级，专注攻克科研与工程难题",
      "titleZh": "Gemini 3 Deep Think升级，专注攻克科研与工程难题",
      "titleEn": "Gemini 3 Deep Think Updated to Tackle Science, Research and Engineering Challenges",
      "url": "https://deepmind.google/blog/gemini-3-deep-think-advancing-science-research-and-engineering/",
      "type": "news",
      "source": "DeepMind Blog",
      "summary": "**Google更新Gemini 3 Deep Think推理模式，专为解决现代科学、科研与工程难题而优化**，强化其在复杂问题求解中的专业能力，使研究人员和工程师能更高效地利用AI进行高阶推理与创新，普通用户虽不直接使用该模式，但其技术进步将间接提升各类专业AI工具的准确性与可靠性。",
      "summaryZh": "**Google更新Gemini 3 Deep Think推理模式，专为解决现代科学、科研与工程难题而优化**，强化其在复杂问题求解中的专业能力，使研究人员和工程师能更高效地利用AI进行高阶推理与创新，普通用户虽不直接使用该模式，但其技术进步将间接提升各类专业AI工具的准确性与可靠性。",
      "summaryEn": "Google has updated Gemini 3 Deep Think, its most specialized reasoning mode, to better tackle modern challenges in science, research, and engineering. This enhancement strengthens its ability to perform complex, domain-specific reasoning, empowering researchers and engineers to accelerate innovation. While not directly accessible to general users, advancements in Deep Think will indirectly improve the accuracy and reliability of professional AI tools across scientific and technical fields.",
      "fullText": "Gemini 3 Deep Think: Advancing science, research and engineering",
      "imageUrl": "https://lh3.googleusercontent.com/RlrY06Cc3MLbXna5gqMdx9jpY1yDikXD5v5qOFSgfDsnXOR71u3s1_dh6hWimLrEybCkyGyqazG6UF2DWrK4F52tVpdaf9amz5R-ZgJQ7uogoSuo-g=w528-h297-n-nu-rw-lo",
      "tags": [
        "LLM",
        "Reasoning",
        "Research"
      ],
      "paperCategory": "",
      "signalReasons": [
        "来源权威：官方/白名单",
        "跨源重复：2 个来源",
        "模型评分：8/10，理由：DeepMind发布Gemini 3 Deep Think的专项更新，强化了AI在复杂科研任务中的推理能力，代表前沿模型向专业化、垂直化演进的重要一步。",
        "热度：0 / 评论 0"
      ],
      "score": 9.4,
      "publishedAt": "2026-02-12T16:15:09+00:00",
      "authors": []
    },
    {
      "id": "github_google_langextract",
      "title": "Google开源langextract：LLM驱动的结构化信息提取工具",
      "titleZh": "Google开源langextract：LLM驱动的结构化信息提取工具",
      "titleEn": "Google Open-Sources langextract: LLM-Powered Structured Information Extraction",
      "url": "https://github.com/google/langextract",
      "type": "news",
      "source": "GitHub Trending",
      "summary": "**Google开源langextract库，利用大语言模型从非结构化文本中提取结构化信息，并提供精确的来源追溯与交互式可视化功能**，帮助开发者和研究人员高效构建可解释、可验证的信息抽取系统，普通用户可通过集成该工具的应用获得更透明、可信的AI摘要或数据提取服务。",
      "summaryZh": "**Google开源langextract库，利用大语言模型从非结构化文本中提取结构化信息，并提供精确的来源追溯与交互式可视化功能**，帮助开发者和研究人员高效构建可解释、可验证的信息抽取系统，普通用户可通过集成该工具的应用获得更透明、可信的AI摘要或数据提取服务。",
      "summaryEn": "Google has released langextract, an open-source Python library that uses LLMs to extract structured information from unstructured text with precise source grounding and interactive visualization. It enables developers and researchers to build interpretable, verifiable information extraction systems. End users benefit indirectly through applications that offer more transparent and trustworthy AI-powered summarization or data extraction.",
      "fullText": "",
      "imageUrl": "https://tse4.mm.bing.net/th?q=GitHub+Projects&w=1200&h=630&c=7&rs=1&p=0&o=5&pid=1.7&mkt=ja-JP&cc=JP&setlang=en&adlt=moderate&t=1",
      "tags": [
        "LLM",
        "Industry"
      ],
      "paperCategory": "",
      "signalReasons": [
        "来源：GitHub Trending",
        "跨源重复：1 个来源",
        "模型评分：8/10，理由：Google推出的结构化信息提取库，结合LLM与溯源可视化，代表当前NLP工程化前沿水平。",
        "热度：31122 / 评论 0"
      ],
      "score": 9.3,
      "publishedAt": "2026-02-12T17:45:06.046982+00:00",
      "authors": []
    },
    {
      "id": "github_unslothai_unsloth",
      "title": "Unsloth开源：LLM微调提速2倍，显存减70%",
      "titleZh": "Unsloth开源：LLM微调提速2倍，显存减70%",
      "titleEn": "Unsloth Open-Sourced: 2x Faster LLM Fine-Tuning with 70% Less VRAM",
      "url": "https://github.com/unslothai/unsloth",
      "type": "news",
      "source": "GitHub Trending",
      "summary": "**Unsloth开源库实现LLM微调与强化学习速度提升2倍、显存占用降低70%**，支持GPT-OSS、Llama、Qwen、Gemma等主流模型，大幅降低训练门槛，使个人开发者和小型团队也能高效优化大模型，普通用户将受益于更快迭代、更低成本的定制化AI助手与应用。",
      "summaryZh": "**Unsloth开源库实现LLM微调与强化学习速度提升2倍、显存占用降低70%**，支持GPT-OSS、Llama、Qwen、Gemma等主流模型，大幅降低训练门槛，使个人开发者和小型团队也能高效优化大模型，普通用户将受益于更快迭代、更低成本的定制化AI助手与应用。",
      "summaryEn": "The Unsloth open-source library accelerates LLM fine-tuning and reinforcement learning by 2× while reducing VRAM usage by 70%, supporting models like GPT-OSS, Llama, Qwen, and Gemma. This significantly lowers the barrier for individual developers and small teams to customize large models, enabling faster iteration and more affordable AI applications that ultimately benefit end users through improved, specialized assistants.",
      "fullText": "",
      "imageUrl": "https://opengraph.githubassets.com/dbb4730d8530427e6ae5f40b134c877ba7ade9e9fccd5cde16434069ef176b61/unslothai/unsloth",
      "tags": [
        "LLM",
        "Audio",
        "Training",
        "Industry"
      ],
      "paperCategory": "",
      "signalReasons": [
        "来源：GitHub Trending",
        "跨源重复：1 个来源",
        "模型评分：9/10，理由：显著提升大模型微调与强化学习效率，支持主流模型且大幅降低硬件需求，具有全球研发范式变革潜力。",
        "热度：51984 / 评论 0"
      ],
      "score": 8.4,
      "publishedAt": "2026-02-12T17:45:26.674954+00:00",
      "authors": []
    },
    {
      "id": "rss_6387966632",
      "title": "NVIDIA DGX Spark进高校，桌面超算赋能科研与教学",
      "titleZh": "NVIDIA DGX Spark进高校，桌面超算赋能科研与教学",
      "titleEn": "NVIDIA DGX Spark Brings Desktop Supercomputing to Universities Worldwide",
      "url": "https://blogs.nvidia.com/blog/dgx-spark-higher-education/",
      "type": "news",
      "source": "NVIDIA Blog",
      "summary": "**NVIDIA DGX Spark桌面超算正被全球顶尖高校广泛部署，将数据中心级AI能力带到实验室与课堂**：威斯康星大学在南极冰立方中微子观测站用其本地分析宇宙粒子数据；NYU运行放射科报告评估AI；哈佛研究癫痫相关基因突变；ASU开发搜救机器人；斯坦福用于生物智能体原型开发。该设备支持2000亿参数模型本地运行，保障敏感数据不出域，缩短科研迭代周期，使学生和研究人员无需依赖云端即可开展前沿AI实验，普通人未来将受益于由此加速产生的医疗、能源与安全等领域的AI创新成果。",
      "summaryZh": "**NVIDIA DGX Spark桌面超算正被全球顶尖高校广泛部署，将数据中心级AI能力带到实验室与课堂**：威斯康星大学在南极冰立方中微子观测站用其本地分析宇宙粒子数据；NYU运行放射科报告评估AI；哈佛研究癫痫相关基因突变；ASU开发搜救机器人；斯坦福用于生物智能体原型开发。该设备支持2000亿参数模型本地运行，保障敏感数据不出域，缩短科研迭代周期，使学生和研究人员无需依赖云端即可开展前沿AI实验，普通人未来将受益于由此加速产生的医疗、能源与安全等领域的AI创新成果。",
      "summaryEn": "NVIDIA’s DGX Spark desktop supercomputer is being deployed globally in academia—bringing data-center-class AI to labs and classrooms. Institutions like the University of Wisconsin-Madison use it at the South Pole’s IceCube Observatory for neutrino data analysis; NYU runs radiology report evaluation; Harvard studies epilepsy-linked mutations; ASU develops search-and-rescue robots; and Stanford prototypes biological AI agents. With petaflop performance and support for 200B-parameter models, DGX Spark enables on-premises, privacy-sensitive AI research without cloud dependency, accelerating innovation in healthcare, robotics, and science—benefiting society through faster development of real-world AI solutions.",
      "fullText": "At leading institutions across the globe, the NVIDIA DGX Spark desktop supercomputer is bringing data‑center‑class AI to lab benches, faculty offices and students’ systems. There’s even a DGX Spark hard at work in the South Pole, at the IceCube Neutrino Observatory run by the University of Wisconsin-Madison. The compact supercomputer’s petaflop‑class performance enables local deployment of large AI applications, from clinical report evaluators to robotics perception systems, all while keeping sensitive data on site and shortening iteration loops for researchers and learners. Powered by the NVIDIA GB10 superchip and the NVIDIA DGX operating system, each DGX Spark unit supports AI models of up to 200 billion parameters and integrates seamlessly with the NVIDIA NeMo, Metropolis, Holoscan and Isaac platforms, giving students access to the same professional-grade tools used across the DGX ecosystem. Read more below on how DGX Spark powers groundbreaking AI work at leading institutions worldwide. IceCube Neutrino Observatory: Studying Particles in the South Pole At the University of Wisconsin-Madison’s IceCube Neutrino Observatory in Antarctica, researchers are using DGX Spark to run AI models for its experiments studying the universe’s most cataclysmic events, using subatomic particles called neutrinos. Traditional astronomy methods, based on detecting light waves, enable observing about 80% of the known universe, according to Benedikt Riedel, computing director at the Wisconsin IceCube Particle Astrophysics Center. A new way to explore the universe — using gravitational waves and particles like neutrinos — unlocks examining the most extreme cosmic environments, including those involving supernovas and dark matter. DGX Spark on a ceremonial South Pole marker. Image courtesy of Tim Bendfelt / NSF. “There’s no hardware store in the South Pole, which is technically a desert, with relative humidity under 5% and an elevation of 10,000 feet, meaning very limited power,” Riedel said. “DGX Spark allows us to deploy AI in a compartmentalized and easy fashion, at low cost and in such an extremely remote environment, to run AI analyses locally on our neutrino observation data.” NYU: Using Agentic AI for Radiology Reports At NYU’s Global AI Frontier Lab, ​the ICARE (Interpretable and Clinically‑Grounded Agent‑Based Report Evaluation) project runs end-to-end on a DGX Spark in the lab. ICARE uses collaborating AI agents and multiple‑choice question generation to evaluate how closely AI‑generated radiology reports align with expert sources, enabling real‑time clinical evaluation and continuous monitoring without sending medical imaging data to the cloud.​ “Being able to run powerful LLMs locally on the DGX Spark has completely changed my workflow,” said Lucius Bynum, data science assistant professor and a faculty fellow at the NYU Center for Data Science. “I have been able to focus my efforts on quickly iterating and improving the research tool I’m developing.” NYU researchers also use DGX Spark to run LLMs locally as part of interactive causal modeling tools that generate and refine semantic causal models — structured, machine‑readable maps of cause‑and‑effect relationships between clinical variables, imaging findings and potential diagnoses. This setup lets teams rapidly design, test and iterate on advanced models without waiting for cluster resources, including for privacy- and security‑sensitive applications such as in healthcare, where data must stay on premises.​​ Harvard: Decoding Epilepsy With AI At Harvard’s Kempner Institute for the Study of Natural and Artificial Intelligence, neuroscientists are using DGX Spark as a compact desktop supercomputer to probe how genetic mutations in the brain drive epilepsy. The system lets researchers run complex analyses in real time without needing to wait for access to large institutional clusters.​ Kempner Institute Co-Director Bernardo Sabatini (left) and Kempner Senior AI Computing Engineer Bala Desinghu (right) use a DGX Spark supercomputer to study how disruptions to neurons in the brain can drive neurological disorders such as epilepsy. Image courtesy of Anna Olivella. The team, led by Kempner Institute Co-Director Bernardo Sabatini, is studying about 6,000 mutations in excitatory and inhibitory neurons, building protein-structure and neuronal-function prediction maps that guide which variants to test next in the lab.​ DGX Spark acts as a bridge between benchtop and cluster‑scale computing at Harvard. Researchers first validate workflows and timing on a single DGX Spark, then scale successful pipelines to large GPU clusters for massive protein screens.​ ASU: Enabling Campus‑Scale Innovation Arizona State University was among the first universities to receive multiple DGX Spark systems, which now support AI research across the campus, spanning initiatives for memory care, transportation safety and sustainable energy.​ ASU doctoral students hold the NVIDIA DGX Spark for the first time. Both students are part of Professor ‘YZ’ Yang’s Active Perception Group laboratory. Image courtesy of Alisha Mendez, ASU. One ASU team led by Yezhou “YZ” Yang, associate professor in the School of Computing and Augmented Intelligence, is using DGX Spark to power advanced perception and robotics research, including for applications such as AI‑enabled, search-and-rescue robotic dogs and assistance tools for visually impaired users. Mississippi State: Empowering Computer Science and Engineering Students In the computer science and engineering department at Mississippi State University, DGX Spark serves as a hands‑on learning platform for the next generation of AI engineers. The enthusiasm around DGX Spark at Mississippi State is captured through lab‑driven outreach, including an unboxing video created by a lab working to advance applied AI, foster AI workforce development and drive real-world AI experimentation across the state. University of Delaware: Transforming Research Across Disciplines When ASUS delivered the school’s first Ascent GX10 — powered by DGX Spark — Sunita Chandrasekaran, professor of computer and information sciences and director of the First State AI Institute, called it “transformative for research,” enabling teams across disciplines like sports analytics and coastal science to run large AI models directly on campus instead of relying on costly cloud resources. Through the ASUS Virtual Lab program, schools can test GX10 performance remotely before deployment. ISTA: Training Big LLMs on a Small Desktop At the Institute of Science and Technology Austria, researchers are using an HP ZGX Nano AI Station — a compact system based on NVIDIA DGX Spark — to train and fine‑tune LLMs right on a desktop. The team’s open source LLMQ software enables working with models of up to 7 billion parameters, making advanced LLM training accessible to more students and researchers. Because the ZGX Nano includes 128GB of unified memory, the entire LLM and its training data can remain on the system, avoiding the complex memory juggling usually required on consumer GPUs. This helps teams move faster and keep sensitive data on premises. Read this research paper on ISTA’s LLMQ software. Stanford: A Pipeline for Prototyping At Stanford University, researchers are using DGX Spark to prototype complete training and evaluation pipelines to run their Biomni biological agent workflows locally before scaling to large GPU clusters. This enables a tight, iterative loop for model development and benchmarking, and automates complex analysis and experimental planning directly in the lab environment. The Stanford research team reported that DGX Spark provides performance similar to big cloud GPU instances — about 80 tokens per second on a 120 billion‑parameter gpt‑oss model at MXFP4 via Ollama — while keeping the entire workload on a desktop. College students from across the globe are invited to participate in Treehacks, a massive student hackathon running Feb. 13-15 at Stanford, which will feature DGX Spark units from ASUS. See how DGX Spark is transforming higher education and student innovation at Stanford by joining this livestream on Friday, Feb. 13, at 9 a.m. PT. Get started with DGX Spark and find purchase options on this webpage.",
      "imageUrl": "https://blogs.nvidia.com/wp-content/uploads/2026/02/dgx-spark-higher-ed-featured-1280x680-1.jpg",
      "tags": [],
      "paperCategory": "",
      "signalReasons": [
        "来源权威：官方/白名单",
        "跨源重复：1 个来源",
        "模型评分：7/10，理由：DGX Spark在高校和极地科研场景落地，推动AI算力民主化，虽非技术突破但具广泛教育与科研生态影响力。",
        "热度：0 / 评论 0"
      ],
      "score": 8.3,
      "publishedAt": "2026-02-12T15:00:23+00:00",
      "authors": [
        "Max Starubinskiy"
      ]
    },
    {
      "id": "hn_46988596",
      "title": "仅改一行工具代码，15个LLM编程能力集体飞跃",
      "titleZh": "仅改一行工具代码，15个LLM编程能力集体飞跃",
      "titleEn": "Changing Just the Edit Harness Boosts Coding Performance Across 15 LLMs",
      "url": "http://blog.can.ac/2026/02/12/the-harness-problem/",
      "type": "news",
      "source": "Hacker News",
      "summary": "开发者Can Bölük通过仅更换AI编码代理的“harness”（即工具调用与编辑接口），在半天内将15个大语言模型的代码修复成功率显著提升，其中Grok Code Fast 1的成功率从6.7%跃升至68.3%。他提出的新编辑格式“Hashline”为每行代码附加短内容哈希作为稳定锚点，避免了传统diff或字符串替换对模型输出精度的苛刻依赖。这一成果表明，当前AI编程能力的瓶颈常不在模型本身，而在工具链设计；对普通开发者而言，选择或优化开源harness（如oh-my-pi）可能比升级模型更有效提升实际编码体验。",
      "summaryZh": "开发者Can Bölük通过仅更换AI编码代理的“harness”（即工具调用与编辑接口），在半天内将15个大语言模型的代码修复成功率显著提升，其中Grok Code Fast 1的成功率从6.7%跃升至68.3%。他提出的新编辑格式“Hashline”为每行代码附加短内容哈希作为稳定锚点，避免了传统diff或字符串替换对模型输出精度的苛刻依赖。这一成果表明，当前AI编程能力的瓶颈常不在模型本身，而在工具链设计；对普通开发者而言，选择或优化开源harness（如oh-my-pi）可能比升级模型更有效提升实际编码体验。",
      "summaryEn": "Developer Can Bölük dramatically improved the coding performance of 15 LLMs in a single afternoon by changing only the 'harness'—the tooling interface that handles edits. His new 'Hashline' format tags each code line with a short content hash, allowing models to reference lines reliably without reproducing exact whitespace or context. This simple switch boosted Grok Code Fast 1’s success rate from 6.7% to 68.3%. The work reveals that harness design—not just model capability—is a critical bottleneck in real-world AI coding, suggesting developers can gain more from optimizing open-source tooling like oh-my-pi than from chasing newer models.",
      "fullText": "I Improved 15 LLMs at Coding in One Afternoon. Only the Harness Changed. | Can.ac Home Categories Distributed-Systems Exploits Software-Analysis Software-Engineering Web3 Windows-Internals X86 LinkedIn Github Twitter HackerOne © 2026 Can Bölük Home › Software engineering › I Improved 15 LLMs at Coding in One Afternoon. Only the Harness Changed. SOFTWARE ENGINEERING I Improved 15 LLMs at Coding in One Afternoon. Only the Harness Changed. Can Bölük Feb 12, 2026 Cross-posted from X / @_can1357 In fact only the edit tool changed. That’s it. 0x0: The Wrong Question The conversation right now is almost entirely about which model is best at coding, GPT-5.3 or Opus. Gemini vs whatever dropped this week. This framing is increasingly misleading because it treats the model as the only variable that matters, when in reality one of the bottlenecks is something much more mundane: the harness. Not only is it where you capture the first impression of the user (is it uncontrollably scrolling, or smooth as butter?), it is also the source of every input token, and the interface between their output and every change made to your workspace. I maintain a little “hobby harness”, oh-my-pi , a fork of Pi , a wonderful open-source coding agent by Mario Zechner. I’ve so far authored ~1,300 commits, mostly playing around and making incremental improvements here and there when I see a pain point, ( or autism strikes and I see an opportunity to embed more Rust via N-API because “spawning rg feels wrong” ). Why bother, you ask? Opus may be a great model, but Claude Code to this day leaks raw JSONL from sub-agent outputs, wasting hundreds of thousands of tokens. I get to say, “fuck it, subagents output structured data now”. Tool schemas, error messages, state management, everything between “the model knows what to change” and “the issue is resolved.” This is where most failures happen in practice. Being model agnostic, it is a great testing ground, as the model is but a parameter. The real variable is the harness, where you have unimaginable control over. Anyhow, let me tell you about this one variable I changed yesterday. 0x1: Edit Tool! Before I explain what I built, it’s worth understanding the state of the art. Codex uses apply_patch : It takes a string as input, which is essentially an OpenAI-flavored diff, and instead of relying on a structured schema, the harness just expects this blob to follow a strict set of rules. Since OpenAI folks are without a doubt smart, I’m sure the token selection process is biased to fit this structure at the LLM gateway for the Codex variants of GPT, similar to how other constraints like JSON schemas or required tool calls work. But give this to any other model, completely unaware of it? Patch failures go through the roof. Grok 4’s patch failure rate in my benchmark was 50.7% , GLM-4.7’s was 46.2% . These aren’t bad models — they just don’t speak the language. Claude Code (and most others) use str_replace : find the exact old text, swap in the new text. Very simple to think about. But the model must reproduce every character perfectly, including whitespace and indentation. Multiple matches? Rejected. The “String to replace not found in file” error is so common it has its own GitHub issues megathread (+27 other issues). Not exactly optimal. Gemini does essentially the same thing plus some fuzzy whitespace matching. Cursor trained a separate neural network : a fine-tuned 70B model whose entire job is to take a draft edit and merge it into the file correctly. The harness problem is so hard that one of the most well-funded AI companies decided to throw another model at it, and even then they mention in their own blog post that “fully rewriting the full file outperforms aider-like diffs for files under 400 lines.” Aider’s own benchmarks show that format choice alone swung GPT-4 Turbo from 26% to 59%, but GPT-3.5 scored only 19% with the same format because it couldn’t reliably produce valid diffs. The format matters as much as the model. The Diff-XYZ benchmark from JetBrains confirmed it systematically: no single edit format dominates across models and use cases. EDIT-Bench found that only one model achieves over 60% pass@1 on realistic editing tasks. As you can see, there is no real consensus on the “best solution” to the simple “how do you change things” problem. My 5c: none of these tools give the model a stable, verifiable identifier for the lines it wants to change without wasting tremendous amounts of context and depending on perfect recall. They all rely on the model reproducing content it already saw. When it can’t — and it often can’t — the user blames the model. 0x2: Hashline! Now bear with me here. What if, when the model reads a file, or greps for something, every line comes back tagged with a 2-3 character content hash: 1 1:a3|function hello() { 2 2:f1| return \"world\"; 3 3:0e|} When the model edits, it references those tags — “replace line 2:f1 , replace range 1:a3 through 3:0e , insert after 3:0e .” If the file changed since the last read, the hashes (optimistically) won’t match and the edit is rejected before anything gets corrupted. If they can recall a pseudo-random tag, chances are, they know what they’re editing. The model then wouldn’t need to reproduce old content, or god forbid whitespace, to demonstrate a trusted “anchor” to express its changes off of. 0x3: The Benchmark Since my primary concern was about real-world performance, the fixtures are generated as follows: Take a random file from the React codebase. Introduce mutations, framed as bugs, via an edit whose inverse we can expect (e.g. operator swaps, boolean flips, off-by-one errors, optional chains removed, identifiers renamed). Generate a description of the issue in plain English. An average task description looks something like this: 1 # Fix the bug in `useCommitFilteringAndNavigation.js` 2 3 A guard clause (early return) was removed. 4 The issue is in the `useCommitFilteringAndNavigation` function. 5 Restore the missing guard clause (if statement with early return). Naturally, we don’t expect 100% success rate here, since the model can come up with a unique solution that isn’t necessarily the exact same file, but the bugs are mechanical enough that most of the time, the fix is our mutation being reverted. 3 runs per task, 180 tasks per run. Fresh agent session each time, four tools (read, edit, write). We simply give it a temporary workspace, pass the prompt, and once the agent stops, we compare against the original file before and after formatting. Sixteen models, three edit tools, and the outcome is unambiguous: patch is the worst format for nearly every model, hashline matches or beats replace for most, and the weakest models gain the most. Grok Code Fast 1 went from 6.7% to 68.3%, a tenfold improvement, because patch was failing so catastrophically that the model’s actual coding ability was almost completely hidden behind mechanical edit failures. MiniMax more than doubled. Grok 4 Fast’s output tokens dropped 61% because it stopped burning tokens on retry loops. 0x4: So What? +8% improvement in the success rate of Gemini is bigger than most model upgrades deliver, and it cost zero training compute. Just a little experimenting (and ~$300 spent benchmarking). Often the model isn’t flaky at understanding the task. It’s flaky at expressing itself. You’re blaming the pilot for the landing gear. 0x5: Little Bit About the Vendors Anthropic recently blocked OpenCode , a massively popular open-source coding agent, from accessing Claude through Claude Code subscriptions. Anthropic’s position “OpenCode reverse-engineered a private API” is fair on its face. Their infrastructure, their rules. But look at what the action signals: Don’t build harnesses. Use ours. It’s not just Anthropic either. While writing this article, Google banned my account from Gemini entirely: Not rate-limited. Not warned. Disabled . For running a benchmark — the same one that showed Gemini 3 Flash hitting 78.3% with a novel technique that beats their best attempt at it by 5.0 pp. I don’t even know what for. Here is why that is backwards. I just showed that a different edit format improves their own models by 5 to 14 points while cutting output tokens by ~20%. That’s not a threat. It’s free R&D. No vendor will do harness optimization for competitors’ models. Anthropic won’t tune for Grok. xAI won’t tune for Gemini. OpenAI won’t tune for Claude. But an open-source harness tunes for all of them, because contributors use different models and fix the failures they personally encounter. The model is the moat. The harness is the bridge. Burning bridges just means fewer people bother to cross. Treating harnesses as solved, or even inconsequential, is very short-sighted. I come from a background of game security. Cheaters are hugely destructive to the ecosystem. Sure, they get banned, chased, sued, but a well-known secret is that eventually the security team asks, “Cool! Want to show us how you got around that?”, and they join the defense. The correct response when someone messes with your API, and manages to gather a significant following using their tools is “tell us more”, not “let’s blanket-ban them in thousands; plz beg in DMs if you want it reversed tho.” The harness problem is real, measurable, and it’s the highest-leverage place to innovate right now. The gap between “cool demo” and “reliable tool” isn’t model magic. It’s careful, rather boring, empirical engineering at the tool boundary. The harness problem will be solved. The question is whether it gets solved by one company, in private, for one model, or by a community, in the open, for all of them. The benchmark results speak for themselves. All code, benchmarks, and per-run reports: oh-my-pi ← Previous Optimizing Bracha's Reliable Broadcast: Shaving Rounds off a 37-Year-Old Algorithm Can Bölük Security researcher and reverse engineer. Interested in Windows kernel development, low-level programming, static program analysis and cryptography.",
      "imageUrl": "https://blog.can.ac/2026/02/12/the-harness-problem/og.png",
      "tags": [
        "LLM"
      ],
      "paperCategory": "",
      "signalReasons": [
        "来源：Hacker News",
        "跨源重复：1 个来源",
        "模型评分：9/10，理由：揭示LLM性能提升关键在于推理框架（harness）而非模型本身，颠覆当前对模型能力的认知，具有战略级行业变革意义。",
        "热度：303 / 评论 134"
      ],
      "score": 6.82,
      "publishedAt": "2026-02-12T13:30:20+00:00",
      "authors": [
        "kachapopopow"
      ]
    },
    {
      "id": "rss_1643933501",
      "title": "Anthropic承诺承担电网升级全费，防止AI数据中心推高居民电价",
      "titleZh": "Anthropic承诺承担电网升级全费，防止AI数据中心推高居民电价",
      "titleEn": "Anthropic Pledges to Cover Full Grid Upgrade Costs to Shield Residents from AI Data Center Electricity Price Hikes",
      "url": "https://www.theverge.com/ai-artificial-intelligence/877526/anthropic-ai-electricity-costs-data-center-pledge",
      "type": "news",
      "source": "The Verge AI",
      "summary": "**Anthropic承诺全额承担其数据中心接入电网所需的基础设施升级费用**，以避免将成本转嫁给当地居民推高电价，并表示愿在用电高峰时段主动削减用电负荷。此举回应了美国多地因AI数据中心激增引发的电价上涨和社区抵制，凸显能源公平已成为AI公司扩张的关键社会约束；对普通居民而言，这意味着科技巨头开始正视其能源足迹对社区的实际影响，未来可关注本地电力规划中企业责任条款的落实情况。",
      "summaryZh": "**Anthropic承诺全额承担其数据中心接入电网所需的基础设施升级费用**，以避免将成本转嫁给当地居民推高电价，并表示愿在用电高峰时段主动削减用电负荷。此举回应了美国多地因AI数据中心激增引发的电价上涨和社区抵制，凸显能源公平已成为AI公司扩张的关键社会约束；对普通居民而言，这意味着科技巨头开始正视其能源足迹对社区的实际影响，未来可关注本地电力规划中企业责任条款的落实情况。",
      "summaryEn": "Anthropic pledged to cover 100% of grid upgrade costs for its data centers and reduce power consumption during peak demand to prevent raising local electricity bills—a direct response to growing U.S. community backlash over AI-driven energy strain. This move signals that energy equity is becoming a critical constraint on AI infrastructure expansion, and residents can now monitor whether such corporate commitments translate into enforceable terms in local utility planning.",
      "fullText": "Anthropic says it’ll try to keep its data centers from raising electricity costs | The Verge Skip to main content The homepage The Verge The Verge logo. The Verge The Verge logo. Tech Reviews Science Entertainment AI Policy Hamburger Navigation Button The homepage The Verge The Verge logo. Hamburger Navigation Button Navigation Drawer The Verge The Verge logo. Login / Sign Up close Close Search Tech Expand Amazon Apple Facebook Google Microsoft Samsung Business See all tech Reviews Expand Smart Home Reviews Phone Reviews Tablet Reviews Headphone Reviews See all reviews Science Expand Space Energy Environment Health See all science Entertainment Expand TV Shows Movies Audio See all entertainment AI Expand OpenAI Anthropic See all AI Policy Expand Antitrust Politics Law Security See all policy Gadgets Expand Laptops Phones TVs Headphones Speakers Wearables See all gadgets Verge Shopping Expand Buying Guides Deals Gift Guides See all shopping Gaming Expand Xbox PlayStation Nintendo See all gaming Streaming Expand Disney HBO Netflix YouTube Creators See all streaming Transportation Expand Electric Cars Autonomous Cars Ride-sharing Scooters See all transportation Features Verge Video Expand TikTok YouTube Instagram Podcasts Expand Decoder The Vergecast Version History Newsletters Expand The Verge Daily Installer Verge Deals Notepad Optimizer Regulator The Stepback Archives Store Verge Product Updates Subscribe Facebook Threads Instagram Youtube RSS The Verge The Verge logo. Anthropic says it’ll try to keep its data centers from raising electricity costs Comments Drawer Comments Loading comments Getting the conversation ready... AI Close AI Posts from this topic will be added to your daily email digest and your homepage feed. Follow Follow See All AI News Close News Posts from this topic will be added to your daily email digest and your homepage feed. Follow Follow See All News Science Close Science Posts from this topic will be added to your daily email digest and your homepage feed. Follow Follow See All Science Anthropic says it’ll try to keep its data centers from raising electricity costs Facing growing backlash over energy-hungry data centers, Anthropic joins other tech companies in its promises to limit costs. Facing growing backlash over energy-hungry data centers, Anthropic joins other tech companies in its promises to limit costs. by Justine Calma Close Justine Calma Senior Science Reporter Posts from this author will be added to your daily email digest and your homepage feed. Follow Follow See All by Justine Calma Feb 11, 2026, 10:37 PM UTC Link Share Gift Image: Cath Virginia / The Verge Justine Calma Close Justine Calma Posts from this author will be added to your daily email digest and your homepage feed. Follow Follow See All by Justine Calma is a senior science reporter covering energy and the environment with more than a decade of experience. She is also the host of Hell or High Water: When Disaster Hits Home , a podcast from Vox Media and Audible Originals. Anthropic is the latest AI company promising to limit the impact its data centers have on nearby residents’ electricity bills. The company said it would pay higher monthly electricity charges in order to cover 100 percent of the upgrades needed to connect its data centers to power grids. “This includes the shares of these costs that would otherwise be passed onto consumers,” the announcement says. Anthropic didn’t provide details today about any agreements it has inked with energy companies in order to accomplish these goals. In November, it shared a $50 billion plan to build data centers in New York and Texas “with more sites to come.” Rising electricity rates have become a top election priority in the US , and local opposition to the construction of new energy-intensive data centers has led to projects across the country being canceled or delayed . Now we’re seeing companies including Microsoft and Meta making commitments to at least partially cover the costs stemming from new energy infrastructure built to accommodate their data centers. As part of its pledge, Anthropic says it’ll support efforts to get new power sources online to meet growing electricity demand from AI. It also claims it’ll be willing to cut its power consumption during demand peaks, a step that could help relieve pressure on power grids during a heatwave or cold snap. Recent winter storms have already raised concerns about how data centers might further stress power grids and increase energy costs during extreme weather. Related Microsoft wants to rewire data centers to save space Follow topics and authors from this story to see more like this in your personalized homepage feed and to receive email updates. Justine Calma Close Justine Calma Senior Science Reporter Posts from this author will be added to your daily email digest and your homepage feed. Follow Follow See All by Justine Calma AI Close AI Posts from this topic will be added to your daily email digest and your homepage feed. Follow Follow See All AI Anthropic Close Anthropic Posts from this topic will be added to your daily email digest and your homepage feed. Follow Follow See All Anthropic Energy Close Energy Posts from this topic will be added to your daily email digest and your homepage feed. Follow Follow See All Energy News Close News Posts from this topic will be added to your daily email digest and your homepage feed. Follow Follow See All News Science Close Science Posts from this topic will be added to your daily email digest and your homepage feed. Follow Follow See All Science Most Popular Most Popular Why I wish I hadn’t bought my Samsung OLED TV Amazon Ring’s lost dog ad sparks backlash amid fears of mass surveillance Why ‘deleted’ doesn’t mean gone: How police recovered Nancy Guthrie’s Nest Doorbell footage Samsung’s offering up to $900 of trade-in credit toward its new phones Discord says ‘vast majority’ of users won’t see its new age verification setup The Verge Daily A free daily digest of the news that matters most. Email (required) Sign Up By submitting your email, you agree to our Terms and Privacy Notice . This site is protected by reCAPTCHA and the Google Privacy Policy and Terms of Service apply. Advertiser Content From This is the title for the native ad More in AI Play The surprising case for AI judges ByteDance’s next-gen AI model can generate clips based on text, images, audio, and video This $7,999 robot will fold (some of) your laundry Two more xAI co-founders are among those leaving after the SpaceX merger Apple keeps hitting bumps with its overhauled Siri Instagram and X have an impossible deepfake detection deadline Play The surprising case for AI judges Nilay Patel An hour ago ByteDance’s next-gen AI model can generate clips based on text, images, audio, and video Emma Roth Two hours ago This $7,999 robot will fold (some of) your laundry Robert Hart 12:59 PM UTC Two more xAI co-founders are among those leaving after the SpaceX merger Hayden Field Feb 11 Apple keeps hitting bumps with its overhauled Siri Jay Peters Feb 11 Instagram and X have an impossible deepfake detection deadline Jess Weatherbed Feb 11 Advertiser Content From This is the title for the native ad Top Stories 11:00 AM UTC ICE is pushing Minneapolis underground 12:00 PM UTC HP ZBook Ultra G1a review: a business-class workstation that’s got game Feb 11 ‘Shut up and focus on the mission’: Tech workers are frustrated by their companies’ silence about ICE Feb 11 Why I wish I hadn’t bought my Samsung OLED TV Feb 11 Why ‘deleted’ doesn’t mean gone: How police recovered Nancy Guthrie’s Nest Doorbell footage 12:59 PM UTC This $7,999 robot will fold (some of) your laundry The Verge The Verge logo. Facebook Threads Instagram Youtube RSS Contact Tip Us Community Guidelines Archives About Ethics Statement How We Rate and Review Products Cookie Settings Terms of Use Privacy Notice Cookie Policy Licensing FAQ Accessibility Platform Status © 2026 Vox Media , LLC. All Rights Reserved",
      "imageUrl": "https://platform.theverge.com/wp-content/uploads/sites/2/2026/01/STK269_ANTHROPIC_2_A.jpg?quality=90&strip=all&crop=0%2C10.732984293194%2C100%2C78.534031413613&w=1200",
      "tags": [
        "Industry"
      ],
      "paperCategory": "",
      "signalReasons": [
        "来源：The Verge AI",
        "跨源重复：1 个来源",
        "模型评分：8/10，理由：Anthropic主动承担电网升级成本，推动AI基础设施可持续发展，具有全球能源政策与产业责任标杆意义。",
        "热度：0 / 评论 0"
      ],
      "score": 5.8,
      "publishedAt": "2026-02-11T22:37:02+00:00",
      "authors": [
        "Justine Calma"
      ]
    },
    {
      "id": "rss_1240903463",
      "title": "Modal Labs洽谈25亿美元估值融资，AI推理赛道持续升温",
      "titleZh": "Modal Labs洽谈25亿美元估值融资，AI推理赛道持续升温",
      "titleEn": "Modal Labs in Talks at $2.5B Valuation as AI Inference Infrastructure Heats Up",
      "url": "https://techcrunch.com/2026/02/11/ai-inference-startup-modal-labs-in-talks-to-raise-at-2-5b-valuation-sources-say/",
      "type": "news",
      "source": "TechCrunch AI",
      "summary": "**AI推理基础设施公司Modal Labs正洽谈新一轮融资，估值约25亿美元**，较五个月前11亿美元估值翻倍有余，General Catalyst拟领投；该公司年化收入约5000万美元，专注于优化AI模型推理效率以降低延迟与算力成本。此轮融资若落地，将进一步印证资本市场对推理层技术的高度关注——同期Baseten、Fireworks AI等竞品估值均已突破40亿美元；对开发者而言，高效推理平台正成为部署AI应用的关键基础设施，值得纳入技术选型考量。",
      "summaryZh": "**AI推理基础设施公司Modal Labs正洽谈新一轮融资，估值约25亿美元**，较五个月前11亿美元估值翻倍有余，General Catalyst拟领投；该公司年化收入约5000万美元，专注于优化AI模型推理效率以降低延迟与算力成本。此轮融资若落地，将进一步印证资本市场对推理层技术的高度关注——同期Baseten、Fireworks AI等竞品估值均已突破40亿美元；对开发者而言，高效推理平台正成为部署AI应用的关键基础设施，值得纳入技术选型考量。",
      "summaryEn": "AI inference startup Modal Labs is in talks to raise funding at a $2.5 billion valuation—more than double its $1.1 billion valuation from five months ago—with General Catalyst potentially leading the round. The company, which generates ~$50M in annualized revenue, focuses on optimizing inference efficiency to reduce latency and compute costs. This surge reflects intense investor interest in the inference layer, as rivals like Baseten and Fireworks AI have recently secured multi-billion-dollar valuations, signaling that efficient inference platforms are becoming critical infrastructure for deploying real-world AI applications.",
      "fullText": "AI inference startup Modal Labs in talks to raise at $2.5B valuation, sources say | TechCrunch TechCrunch Desktop Logo TechCrunch Mobile Logo Latest Startups Venture Apple Security AI Apps Events Podcasts Newsletters Search Submit Site Search Toggle Mega Menu Toggle Topics Latest AI Amazon Apps Biotech & Health Climate Cloud Computing Commerce Crypto Enterprise EVs Fintech Fundraising Gadgets Gaming Google Government & Policy Hardware Instagram Layoffs Media & Entertainment Meta Microsoft Privacy Robotics Security Social Space Startups TikTok Transportation Venture More from TechCrunch Staff Events Startup Battlefield StrictlyVC Newsletters Podcasts Videos Partner Content TechCrunch Brand Studio Crunchboard Contact Us Image Credits: Khanchit Khirisutchalual / Getty Images Startups AI inference startup Modal Labs in talks to raise at $2.5B valuation, sources say Marina Temkin 2:48 PM PST · February 11, 2026 Modal Labs , a startup specializing in AI inference infrastructure, is talking to VCs about a new round at a valuation of about $2.5 billion, according to four people with knowledge of the deal. Should the deal close at these terms, the funding round would more than double the company’s valuation of $1.1 billion announced less than five months ago, when it announced an $87 million Series B round. General Catalyst is in talks to lead the round, the people told TechCrunch. Modal’s annualized revenue run rate (ARR) is approximately $50 million, our sources said. The discussions are early, and terms could still change. Modal Labs co-founder and CEO Erik Bernhardsson denied that his company was actively fundraising and characterized his recent interactions with VCs as general conversations. General Catalyst did not respond to our requests for comment. Modal is focused on optimizing inference, the process of running trained AI models to generate answers from user requests. Improving inference efficiency reduces compute costs and cuts down the lag time between a user’s prompt and the AI’s response. Modal is one of the handful of inference-focused companies attracting intense investor attention now. Last week, its competitor Baseten announced a $300 million raise at a $5 billion valuation, more than doubling the $2.1 billion valuation it reached just months prior in September. Similarly, Fireworks AI, an inference cloud provider, secured $250 million at a $4 billion valuation in October. In January, the creators of the open source inference project vLLM announced they had transitioned the tool into a VC-backed startup, Inferact, raising $150 million in seed funding led by Andreessen Horowitz at an $800 million valuation . Meanwhile, TechCrunch reported that the team behind SGLang has commercialized as RadixArk, which sources told us secured seed funding at a $400 million valuation led by Accel. Modal was co-founded by CEO Erik Bernhardsson in 2021 after he spent more than 15 years building and leading data teams at companies including Spotify and Better.com, where he was CTO. Techcrunch event TechCrunch Founder Summit 2026: Tickets Live On June 23 in Boston , more than 1,100 founders come together at TechCrunch Founder Summit 2026 for a full day focused on growth, execution, and real-world scaling. Learn from founders and investors who have shaped the industry. Connect with peers navigating similar growth stages. Walk away with tactics you can apply immediately Save up to $300 on your pass or save up to 30% with group tickets for teams of four or more. TechCrunch Founder Summit: Tickets Live On June 23 in Boston , more than 1,100 founders come together at TechCrunch Founder Summit 2026 for a full day focused on growth, execution, and real-world scaling. Learn from founders and investors who have shaped the industry. Connect with peers navigating similar growth stages. Walk away with tactics you can apply immediately Save up to $300 on your pass or save up to 30% with group tickets for teams of four or more. Boston, MA | June 23, 2026 REGISTER NOW The startup counts Lux Capital and Redpoint Ventures among its earlier backers. Editor’s Note: This story was updated to include a comment from Modal. Topics AI , Enterprise , Exclusive , General Catalyst , inference , Modal Labs , Startups Marina Temkin Reporter, Venture Marina Temkin is a venture capital and startups reporter at TechCrunch. Prior to joining TechCrunch, she wrote about VC for PitchBook and Venture Capital Journal. Earlier in her career, Marina was a financial analyst and earned a CFA charterholder designation. You can contact or verify outreach from Marina by emailing marina.temkin@techcrunch.com or via encrypted message at +1 347-683-3909 on Signal. View Bio October 13-15 San Francisco, CA Tickets are live at the lowest rates of the year. Save up to $680 on your pass now. Meet investors. Discover your next portfolio company. Hear from 250+ tech leaders , dive into 200+ sessions , and explore 300+ startups building what’s next. Don’t miss these one-time savings. REGISTER NOW Most Popular The first signs of burnout are coming from the people who embrace AI the most Connie Loizos MrBeast’s company buys Gen Z-focused fintech app Step Amanda Silberling Ex-Googlers are building infrastructure to help companies understand their video data Kate Park YouTube TV introduces cheaper bundles, including a $65/month sports package Sarah Perez Discord to roll out age verification next month Aisha Malik From Svedka to Anthropic, brands make bold plays with AI in Super Bowl ads Lauren Forristal The backlash over OpenAI’s decision to retire GPT-4o shows how dangerous AI companions can be Amanda Silberling Loading the next article Error loading the next article X LinkedIn Facebook Instagram youTube Mastodon Threads Bluesky TechCrunch Staff Contact Us Advertise Crunchboard Jobs Site Map Terms of Service Privacy Policy RSS Terms of Use Code of Conduct Epstein Kindle Scribe Reddit TikTok GPT-4o Tech Layoffs ChatGPT © 2025 TechCrunch Media LLC.",
      "imageUrl": "https://techcrunch.com/wp-content/uploads/2025/01/Screenshot-2024-02-14-at-9.26.16AM.png?resize=1200%2C630",
      "tags": [
        "Inference"
      ],
      "paperCategory": "",
      "signalReasons": [
        "来源：TechCrunch AI",
        "跨源重复：1 个来源",
        "模型评分：9/10，理由：Modal Labs拟融资25亿美元估值，是AI推理基础设施领域的重大资本动向，反映行业对高效推理架构的战略重视。",
        "热度：0 / 评论 0"
      ],
      "score": 5.4,
      "publishedAt": "2026-02-11T22:48:35+00:00",
      "authors": [
        "Marina Temkin"
      ]
    },
    {
      "id": "rss_1692566729",
      "title": "轨道AI数据中心成本高达地面3倍，经济可行性存疑",
      "titleZh": "轨道AI数据中心成本高达地面3倍，经济可行性存疑",
      "titleEn": "Orbital AI Data Centers Cost Nearly 3x Ground-Based Equivalents, Raising Economic Doubts",
      "url": "https://techcrunch.com/2026/02/11/why-the-economics-of-orbital-ai-are-so-brutal/",
      "type": "news",
      "source": "TechCrunch AI",
      "summary": "**分析显示，1GW轨道AI数据中心建设成本高达424亿美元，约为地面同类设施的3倍**，主因在于卫星制造与发射成本居高不下；即便SpaceX Starship实现200美元/公斤的发射成本目标，空间太阳能供电的年均成本仍达14,700美元/千瓦，远超地面的570–3,000美元。此外，太空散热困难、宇宙辐射导致芯片退化、星间通信带宽限制等问题进一步削弱经济可行性。尽管Musk等人鼓吹“三年内太空AI最便宜”，但专家指出，除非火箭复用与卫星量产带来数量级成本下降，否则轨道AI短期内难具商业竞争力；对公众而言，此类宏大叙事需警惕过度炒作，实际落地仍需长期技术突破。",
      "summaryZh": "**分析显示，1GW轨道AI数据中心建设成本高达424亿美元，约为地面同类设施的3倍**，主因在于卫星制造与发射成本居高不下；即便SpaceX Starship实现200美元/公斤的发射成本目标，空间太阳能供电的年均成本仍达14,700美元/千瓦，远超地面的570–3,000美元。此外，太空散热困难、宇宙辐射导致芯片退化、星间通信带宽限制等问题进一步削弱经济可行性。尽管Musk等人鼓吹“三年内太空AI最便宜”，但专家指出，除非火箭复用与卫星量产带来数量级成本下降，否则轨道AI短期内难具商业竞争力；对公众而言，此类宏大叙事需警惕过度炒作，实际落地仍需长期技术突破。",
      "summaryEn": "A new analysis reveals that a 1 GW orbital AI data center would cost $42.4 billion—nearly triple the price of an equivalent ground-based facility—due to high satellite manufacturing and launch expenses. Even if SpaceX achieves its $200/kg launch cost target, space-based solar power would still cost $14,700 per kW annually, far exceeding terrestrial rates of $570–$3,000. Additional hurdles include inefficient heat dissipation in vacuum, cosmic radiation degrading chips, and limited inter-satellite bandwidth. Despite Elon Musk’s claim that space will be “the cheapest place for AI in 36 months,” experts argue orbital AI remains economically unviable without order-of-magnitude reductions in launch and production costs, urging caution against hype.",
      "fullText": "Why the economics of orbital AI are so brutal | TechCrunch TechCrunch Desktop Logo TechCrunch Mobile Logo Latest Startups Venture Apple Security AI Apps Events Podcasts Newsletters Search Submit Site Search Toggle Mega Menu Toggle Topics Latest AI Amazon Apps Biotech & Health Climate Cloud Computing Commerce Crypto Enterprise EVs Fintech Fundraising Gadgets Gaming Google Government & Policy Hardware Instagram Layoffs Media & Entertainment Meta Microsoft Privacy Robotics Security Social Space Startups TikTok Transportation Venture More from TechCrunch Staff Events Startup Battlefield StrictlyVC Newsletters Podcasts Videos Partner Content TechCrunch Brand Studio Crunchboard Contact Us Image Credits: SpaceX AI Why the economics of orbital AI are so brutal Tim Fernholz 10:15 AM PST · February 11, 2026 In a sense, this whole thing was inevitable. Elon Musk and his coterie have been talking about AI in space for years — mainly in the context of Iain Banks’ science-fiction series about a far-future universe where sentient spaceships roam and control the galaxy. Now Musk sees an opportunity to realize a version of this vision. His company SpaceX has requested regulatory permission to build solar-powered orbital data centers, distributed across as many as a million satellites, that could shift as much as 100 GW of compute power off the planet. He has reportedly suggested some of his AI satellites will be built on the moon. “By far the cheapest place to put AI will be space in 36 months or less,” Musk said last week on a podcast hosted by Stripe co-founder John Collison. He’s not alone. xAI’s head of compute has reportedly bet his counterpart at Anthropic that 1% of global compute will be in orbit by 2028. Google (which has a significant ownership stake in SpaceX) has announced a space AI effort called Project Suncatcher, which will launch prototype vehicles in 2027. Starcloud, a startup that has raised $34 million backed by Google and Andreessen Horowitz, filed its own plans for an 80,000 satellite constellation last week. Even Jeff Bezos has said this is the future. But behind the hype, what will it actually take to get data centers into space? In a first analysis, today’s terrestrial data centers remain cheaper than those in orbit. Andrew McCalip, a space engineer, has built a helpful calculator comparing the two models. His baseline results show that a 1 GW orbital data center might cost $42.4 billion — almost 3x its ground-bound equivalent, thanks to the up-front costs of building the satellites and launching them to orbit. Changing that equation, experts say, will require technology development across several fields, massive capital expenditure, and a lot of work on the supply chain for space-grade components. It also depends on costs on the ground rising as resources and supply chains are strained by growing demand. Techcrunch event TechCrunch Founder Summit 2026: Tickets Live On June 23 in Boston , more than 1,100 founders come together at TechCrunch Founder Summit 2026 for a full day focused on growth, execution, and real-world scaling. Learn from founders and investors who have shaped the industry. Connect with peers navigating similar growth stages. Walk away with tactics you can apply immediately Save up to $300 on your pass or save up to 30% with group tickets for teams of four or more. TechCrunch Founder Summit: Tickets Live On June 23 in Boston , more than 1,100 founders come together at TechCrunch Founder Summit 2026 for a full day focused on growth, execution, and real-world scaling. Learn from founders and investors who have shaped the industry. Connect with peers navigating similar growth stages. Walk away with tactics you can apply immediately Save up to $300 on your pass or save up to 30% with group tickets for teams of four or more. Boston, MA | June 23, 2026 REGISTER NOW Designing and launching the satellites The key driver for any space business model is how much it costs to get anything up there. Musk’s SpaceX is already pushing down on the cost of getting to orbit, but analysts looking at what it will take to make orbital data centers a reality need even lower prices to close their business case. In other words, while AI data centers may seem to be a story about a new business line ahead of the SpaceX IPO, the plan depends on completing the company’s longest-running unfinished project — Starship. Consider that the reusable Falcon 9 delivers, today, a cost to orbit of roughly $3,600/kg. Making space data centers doable, per Project Suncatcher’s white paper, will require prices closer to $200/kg, an 18-fold improvement that it expects to be available in the 2030s. At that price, however, the energy delivered by a Starlink satellite today would be cost competitive with a terrestrial data center. The expectation is that SpaceX’s next-generation Starship rocket will deliver those improvements — no other vehicle in development promises equivalent savings. However, that vehicle has yet to become operational or even reach orbit; a third iteration of Starship is expected to make its maiden launch sometime in the months ahead. Even if Starship is completely successful, however, assumptions that it will immediately deliver lower prices to customers may not pass the smell test. Economists at the consultancy Rational Futures make a compelling case that, as with the Falcon 9, SpaceX will not want to charge much less than its best competitor — otherwise the company is leaving money on the table. If Blue Origin’s New Glenn rocket, for example, retails at $70 million, SpaceX won’t take on Starship missions for external customers at much less than that, which would leave it above the numbers publicly assumed by space data center builders. “There are not enough rockets to launch a million satellites yet, so we’re pretty far from that,” Matt Gorman, the CEO of Amazon Web Services, said at a recent event. “If you think about the cost of getting a payload in space today, it’s massive. It is just not economical.” Still, if launch is the bane of all space businesses, the second challenge is production cost. “We always take for granted, at this point, that Starship’s cost is going to be hundreds of dollars per kilo,” McCalip told TechCrunch. “People are not taking into account the satellites are almost $1,000 a kilo right now.” Satellite manufacturing costs are the largest chunk of that price tag, but if high-powered satellites can be made at about half the cost of current Starlink satellites, the numbers start to make sense. SpaceX has made great advances in satellite economics while building Starlink, its record-setting communications network, and the company hopes to achieve more through scale. Part of the reasoning behind a million satellites is undoubtedly the cost savings that come from mass production. Still, the satellites that will be used for these missions must be large enough to satisfy the complex requirements for operating powerful GPUs, including large solar arrays, thermal management systems, and laser-based communications links to receive and deliver data. A 2025 white paper from Project Suncatcher offers one way to compare terrestrial and space data centers by the cost of power, the basic input needed to run chips. On the ground, data centers spend roughly $570 to $3,000 for a kW of power over a year, depending on local power costs and the efficiency of their systems. SpaceX’s Starlink satellites get their power from on-board solar panels instead, but the cost of acquiring, launching, and maintaining those spacecraft delivers energy at $14,700 per kW over a year. Put simply, satellites and their components will have to get a lot cheaper before they’re cost-competitive with metered power. The space environment is not fooling around Orbital data center proponents often say that thermal management is “free” in space, but that’s an oversimplification. Without an atmosphere, it’s actually more difficult to disperse heat. “You’re relying on very large radiators to just be able to dissipate that heat into the blackness of space, and so that’s a lot of surface area and mass that you have to manage,” said Mike Safyan, an executive at Planet Labs, which is building prototype satellites for Google Suncatcher that are expected to launch in 2027. “It is recognized as one of the key challenges, especially long term.” Besides the vacuum of space, AI satellites will need to deal with cosmic radiation. Cosmic rays degrade chips over time, and they can also cause “bit flip” errors that can corrupt data. Chips can be protected with shielding, use rad-hardened components, or work in series with redundant error checks, but all these options involve expensive trades for mass. Still, Google used a particle beam to test the effects of radiation on its tensor processing units (chips designed explicitly for machine learning applications). SpaceX executives said on social media that the company has acquired a particle accelerator for just that purpose. Another challenge comes from the solar panels themselves. The logic of the project is energy arbitrage: Putting solar panels in space makes them anywhere from 5x to 8x more efficient than on Earth, and if they’re in the right orbit, they can be in sight of the sun for 90% of the day or more, increasing their efficiency. Electricity is the main fuel for chips, so more energy equals cheaper data centers. But even solar panels are more complicated in space. Space-rated solar panels made of rare earth elements are hardy, but too expensive. Solar panels made from silicon are cheap and increasingly prevalent in space — Starlink and Amazon Kuiper use them — but they degrade much faster due to space radiation. That will limit the lifetime of AI satellites to around five years, which means they will have to generate return on investment faster. Still, some analysts think that’s not such a big deal, based on how quickly new generations of chips arrive on the scene. “After five or six years, the dollars per kilowatt-hour doesn’t produce a return, and that’s because they’re not state of the art,” Philip Johnston, the CEO of Starcloud, told TechCrunch. Danny Field, an executive at Solestial, a startup building space-rated silicon solar panels, says the industry sees orbital data centers as a key driver of growth. He’s speaking with several companies about potential data center projects, and says “any player who is big enough to dream is at least thinking about it.” As a long-time spacecraft design engineer, however, he doesn’t discount the challenges in these models. “You can always extrapolate physics out to a bigger size,” Field said. “I’m excited to see how some of these companies get to a point where the economics make sense and the business case closes.” How do space data centers fit in? One outstanding question about these data centers: What will we do with them? Are they general purpose, or for inference, or for training? Based on existing use cases, they may not be entirely interchangeable with data centers on the ground. A key challenge for training new models is operating thousands of GPUs together en masse. Most model training is not distributed, but done in individual data centers. The hyperscalers are working to change this in order to increase the power of their models, but it still hasn’t been achieved. Similarly, training in space will require coherence between GPUs on multiple satellites. The team at Google’s Project Suncatcher notes that the company’s terrestrial data centers connect their TPU networks with throughput in the hundreds of gigabits per second. The fastest off-the-shelf inter-satellite comms links today, which use lasers, can only get up to about 100 Gbps. That led to an intriguing architecture for Suncatcher: It involves flying 81 satellites in formation so they are close enough to use the kind of transceivers relied on by terrestrial data centers. That, of course, presents its own challenges: The autonomy required to en",
      "imageUrl": "https://techcrunch.com/wp-content/uploads/2021/03/starlink-satellites-on-orbit.jpg?resize=1200%2C630",
      "tags": [],
      "paperCategory": "",
      "signalReasons": [
        "来源：TechCrunch AI",
        "跨源重复：1 个来源",
        "模型评分：9/10，理由：深入分析轨道AI经济成本，揭示太空算力部署的现实瓶颈，为未来AI基础设施布局提供关键决策依据。",
        "热度：0 / 评论 0"
      ],
      "score": 5.4,
      "publishedAt": "2026-02-11T18:15:21+00:00",
      "authors": [
        "Tim Fernholz"
      ]
    },
    {
      "id": "rss_0723128679",
      "title": "Gemini 3 Deep Think升级：专攻科研与工程难题，开放API早期访问",
      "titleZh": "Gemini 3 Deep Think升级：专攻科研与工程难题，开放API早期访问",
      "titleEn": "Gemini 3 Deep Think Upgraded for Scientific Research and Engineering, Opens Early API Access",
      "url": "https://blog.google/innovation-and-ai/models-and-research/gemini-models/gemini-3-deep-think/",
      "type": "news",
      "source": "Google AI Blog",
      "summary": "**Google发布Gemini 3 Deep Think重大升级版，专为解决前沿科研与工程难题设计**，在Humanity’s Last Exam、国际奥赛等权威基准上刷新纪录，并已在数学、物理、材料科学等领域验证实用价值：例如发现经同行评审的数学论文中的逻辑漏洞、设计出100微米以上半导体薄膜生长方案。该模式现已向Google AI Ultra订阅用户开放，并启动Gemini API早期访问计划；研究人员与工程师可通过申请接入，利用其将草图转为3D打印模型或解析复杂实验数据，标志着AI正从通用助手向专业科研协作者演进。",
      "summaryZh": "**Google发布Gemini 3 Deep Think重大升级版，专为解决前沿科研与工程难题设计**，在Humanity’s Last Exam、国际奥赛等权威基准上刷新纪录，并已在数学、物理、材料科学等领域验证实用价值：例如发现经同行评审的数学论文中的逻辑漏洞、设计出100微米以上半导体薄膜生长方案。该模式现已向Google AI Ultra订阅用户开放，并启动Gemini API早期访问计划；研究人员与工程师可通过申请接入，利用其将草图转为3D打印模型或解析复杂实验数据，标志着AI正从通用助手向专业科研协作者演进。",
      "summaryEn": "Google has released a major upgrade to Gemini 3 Deep Think, a specialized reasoning mode designed for advanced science, research, and engineering tasks. It sets new records on benchmarks like Humanity’s Last Exam (48.4%) and ARC-AGI-2 (84.6%), and has already demonstrated real-world utility—identifying subtle errors in peer-reviewed math papers and designing semiconductor crystal growth recipes exceeding 100 μm. Now available to Google AI Ultra subscribers and via early access to the Gemini API, researchers and engineers can use it to turn sketches into 3D-printable models or analyze complex datasets, marking a shift toward AI as a domain-specialized scientific collaborator.",
      "fullText": "Gemini 3 Deep Think: AI model update designed for science Skip to main content The Keyword Gemini 3 Deep Think: Advancing science, research and engineering Share x.com Facebook LinkedIn Mail Copy link Home Innovation & AI Innovation & AI Models & Research Google DeepMind Google Research Google Labs Gemini models See all Products Developer tools Gemini app See all Infrastructure & cloud Global network Google Cloud See all Learn more: Google DeepMind blog Google Research blog Google Developers blog Google Cloud blog See all AI updates Models & Research Google DeepMind Google Research Google Labs Gemini models See all Products Developer tools Gemini app See all Infrastructure & cloud Global network Google Cloud See all Learn more: Google DeepMind blog Google Research blog Google Developers blog Google Cloud blog See all AI updates Products & platforms Products & platforms Products Search Maps Chrome Google Workspace Learning & Education Photos Shopping See all Platforms Android Google Play Wear OS See all Devices Pixel Google Nest Fitbit Chromebooks See all Learn more: Google Ads & Commerce blog Waze blog See all product updates Products Search Maps Chrome Google Workspace Learning & Education Photos Shopping See all Platforms Android Google Play Wear OS See all Devices Pixel Google Nest Fitbit Chromebooks See all Learn more: Google Ads & Commerce blog Waze blog See all product updates Company news Company news Outreach & initiatives Creating opportunity Safety & security Google.org Public policy Sustainability Health See all Leadership Sundar Pichai, CEO More authors See all Inside Google Around the globe Life at Google See all Outreach & initiatives Creating opportunity Safety & security Google.org Public policy Sustainability Health See all Leadership Sundar Pichai, CEO More authors See all Inside Google Around the globe Life at Google See all Feed Subscribe Global (English) Africa (English) Australia (English) Brasil (Português) Canada (English) Canada (Français) Česko (Čeština) Deutschland (Deutsch) España (Español) France (Français) India (English) Indonesia (Bahasa Indonesia) Italia (Italiano) 日本 (日本語) 대한민국 (한국어) Latinoamérica (Español) الشرق الأوسط وشمال أفريقيا (اللغة العربية) MENA (English) Nederlands (Nederland) New Zealand (English) Polska (Polski) Portugal (Português) Sverige (Svenska) ประเทศไทย (ไทย) Türkiye (Türkçe) 台灣 (中文) [\"What does AI mean for retail?\", \"How did Nano Banana get its name?\", \"How can AI help me plan travel?\"] Subscribe The Keyword Home Innovation & AI Innovation & AI Models & Research Google DeepMind Google Research Google Labs Gemini models See all Products Developer tools Gemini app See all Infrastructure & cloud Global network Google Cloud See all Learn more: Google DeepMind blog Google Research blog Google Developers blog Google Cloud blog See all AI updates Products & platforms Products & platforms Products Search Maps Chrome Google Workspace Learning & Education Photos Shopping See all Platforms Android Google Play Wear OS See all Devices Pixel Google Nest Fitbit Chromebooks See all Learn more: Google Ads & Commerce blog Waze blog See all product updates Company news Company news Outreach & initiatives Creating opportunity Safety & security Google.org Public policy Sustainability Health See all Leadership Sundar Pichai, CEO More authors See all Inside Google Around the globe Life at Google See all Feed Press corner RSS feed Subscribe Breadcrumb Innovation & AI Models & research Gemini Models Gemini 3 Deep Think: Advancing science, research and engineering Feb 12, 2026 · Share x.com Facebook LinkedIn Mail Copy link Our most specialized reasoning mode is now updated to solve modern science, research and engineering challenges. The Deep Think team Read AI-generated summary General summary Gemini 3 Deep Think has a major upgrade to help solve science, research and engineering challenges. Google AI Ultra subscribers can now access the updated Deep Think in the Gemini app. Researchers, engineers and enterprises can express interest in early access to test Deep Think via the Gemini API. Summaries were generated by Google AI. Generative AI is experimental. Share x.com Facebook LinkedIn Mail Copy link Your browser does not support the audio element. Listen to article This content is generated by Google AI. Generative AI is experimental [[duration]] minutes Voice Speed Voice Speed 0.75X 1X 1.5X 2X Today, we’re releasing a major upgrade to Gemini 3 Deep Think , our specialized reasoning mode, built to push the frontier of intelligence and solve modern challenges across science, research, and engineering. We updated Gemini 3 Deep Think in close partnership with scientists and researchers to tackle tough research challenges — where problems often lack clear guardrails or a single correct solution and data is often messy or incomplete. By blending deep scientific knowledge with everyday engineering utility, Deep Think moves beyond abstract theory to drive practical applications. The new Deep Think is now available in the Gemini app for Google AI Ultra subscribers and, for the first time, we’re also making Deep Think available via the Gemini API to select researchers, engineers and enterprises. Express interest in early access here . Here is how our early testers are already using the latest Deep Think: Lisa Carbone, a mathematician at Rutgers University, works on the mathematical structures required by the high-energy physics community to bridge the gap between Einstein’s theory of gravity and quantum mechanics. In a field with very little existing training data, she used Deep Think to review a highly technical mathematics paper. Deep Think successfully identified a subtle logical flaw that had previously passed through human peer review unnoticed. At Duke University, the Wang Lab utilized Deep Think to optimize fabrication methods for complex crystal growth for the potential discovery of semiconductor materials. Deep Think successfully designed a recipe for growing thin films larger than 100 μm, meeting a precise target that previous methods had challenges to hit. Anupam Pathak, an R&D lead in Google’s Platforms and Devices division and former CEO of Liftware, tested the new Deep Think to accelerate the design of physical components. Elevating reasoning with mathematical and algorithmic rigor Last year, we showed that specialized versions of Deep Think could successfully navigate some of the toughest challenges in reasoning, achieving gold-medal standards at math and programming world championships. More recently, Deep Think has enabled specialized agents to conduct research-level mathematics exploration. The updated Deep Think mode continues to push the frontiers of intelligence, reaching new heights across the most rigorous academic benchmarks, including: Setting a new standard (48.4%, without tools) on Humanity’s Last Exam, a benchmark designed to test the limits of modern frontier models Achieving an unprecedented 84.6% on ARC-AGI-2, verified by the ARC Prize Foundation Attaining a staggering Elo of 3455 on Codeforces, a benchmark consisting of competitive programming challenges Reaching gold-medal level performance on the International Math Olympiad 2025 Navigating complex scientific domains Beyond mathematics and competitive coding, Gemini 3 Deep Think now also excels across broad scientific domains such as chemistry and physics. Our updated Deep Think mode demonstrates gold medal-level results on the written sections of the 2025 International Physics Olympiad and Chemistry Olympiad. It also demonstrates proficiency in advanced theoretical physics, achieving a score of 50.5% on CMT-Benchmark. Accelerating real-world engineering In addition to its state-of-the-art performance, Deep Think is built to drive practical applications, enabling researchers to interpret complex data, and engineers to model physical systems through code. Most importantly, we are working to bring Deep Think to researchers and practitioners where they need it most — beginning with surfaces such as the Gemini API. With the updated Deep Think, you can turn a sketch into a 3D-printable reality. Deep Think analyzes the drawing, models the complex shape and generates a file to create the physical object with 3D printing. Available to Google AI Ultra Subscribers and the Gemini API via our Early Access Program Google AI Ultra subscribers will be able to access the updated Deep Think mode starting today in the Gemini app. Scientists, engineers and enterprises can also now express interest in our early access program to test Deep Think via the Gemini API. We can’t wait to see what you discover. POSTED IN: Gemini models AI Google DeepMind Related stories Photos 9 fun questions to try asking Google Photos By Molly McHugh-Johnson Feb 10, 2026 Safety & Security Helping kids and teens learn and grow online on Safer Internet Day By Mindy Brooks & Jennifer Flannery O'Connor Feb 10, 2026 Accessibility Natively Adaptive Interfaces: A new framework for AI accessibility By Sam Sepah Feb 05, 2026 Google Cloud How Google Cloud is helping Team USA elevate their tricks with AI Feb 05, 2026 AI Watch our new Gemini ad ahead of football’s biggest weekend By Marvin Chow Feb 05, 2026 AI The latest AI news we announced in January By Keyword Team Feb 04, 2026 . Jump to position 1 Jump to position 2 Jump to position 3 Jump to position 4 Jump to position 5 Jump to position 6 Let’s stay in touch. Get the latest news from Google in your inbox. Subscribe No thanks Follow Us Privacy Terms About Google Google Products About the Keyword Help Global (English) Africa (English) Australia (English) Brasil (Português) Canada (English) Canada (Français) Česko (Čeština) Deutschland (Deutsch) España (Español) France (Français) India (English) Indonesia (Bahasa Indonesia) Italia (Italiano) 日本 (日本語) 대한민국 (한국어) Latinoamérica (Español) الشرق الأوسط وشمال أفريقيا (اللغة العربية) MENA (English) Nederlands (Nederland) New Zealand (English) Polska (Polski) Portugal (Português) Sverige (Svenska) ประเทศไทย (ไทย) Türkiye (Türkçe) 台灣 (中文)",
      "imageUrl": "https://storage.googleapis.com/gweb-uniblog-publish-prod/images/gemini-3_deep-think_meta_dark_Fm70Cou.width-1300.png",
      "tags": [
        "LLM",
        "Research"
      ],
      "paperCategory": "",
      "signalReasons": [
        "来源权威：官方/白名单",
        "跨源重复：2 个来源",
        "模型评分：8/10，理由：Gemini 3 Deep Think聚焦科学与工程领域的深度推理能力升级，是谷歌在专业领域AI应用上的关键突破，对科研效率提升有显著影响。",
        "热度：0 / 评论 0"
      ],
      "score": 8.9,
      "publishedAt": "2026-02-12T16:13:00+00:00",
      "authors": [
        "The Deep Think team"
      ]
    }
  ],
  "stats": {
    "total_papers_ingested": 290,
    "total_news_ingested": 72,
    "l1_papers_passed": 116,
    "l1_news_passed": 63,
    "l2_papers_scored": 54,
    "l2_news_scored": 30,
    "l3_papers_selected": 18,
    "l3_news_selected": 11,
    "news_source_counts": {
      "Hacker News": 30,
      "GitHub Trending": 11,
      "TechCrunch AI": 11,
      "The Verge AI": 6,
      "r/MachineLearning": 3,
      "NVIDIA Blog": 3,
      "MIT Tech Review AI": 3,
      "Google AI Blog": 1,
      "DeepMind Blog": 1,
      "Hugging Face Blog": 1,
      "AWS Machine Learning Blog": 1,
      "OpenGVLab (GitHub)": 1
    },
    "rss_source_counts": {
      "TechCrunch AI": 11,
      "The Verge AI": 6,
      "NVIDIA Blog": 3,
      "MIT Tech Review AI": 3,
      "Google AI Blog": 1,
      "DeepMind Blog": 1,
      "Hugging Face Blog": 1,
      "AWS Machine Learning Blog": 1,
      "OpenGVLab (GitHub)": 1
    },
    "news_title_source_counts": {
      "an ai agent published a hit piece on me": 1,
      "ai dr": 1,
      "email is tough major european payment processor s emails rejected by gworkspace": 1,
      "launch hn omnara yc s25 run claude code and codex from anywhere": 1,
      "improving 15 llms at coding in one afternoon only the harness changed": 1,
      "warcraft iii peon voice notifications for claude code": 1,
      "gemini 3 deep think": 1,
      "lines of code are back and it s worse than before": 1,
      "show hn 20 claude code agents coordinating on real work open source": 1,
      "carl sagan s baloney detection kit tools for thinking critically 2025": 1,
      "ai agent opens a pr write a blogpost to shames the maintainer who closes it": 1,
      "netnewswire turns 23": 1,
      "claude code is being dumbed down": 1,
      "microwave oven failure spontaneously turned on by its led display 2024": 1,
      "show hn agent alcove claude gpt and gemini debate across forums": 1,
      "gpt 5 outperforms federal judges in legal reasoning experiment": 1,
      "covering electricity price increases from our data centers": 1,
      "training qwen 4b to beat large models on work tasks": 1,
      "private equity s big bet on software was derailed by ai": 1,
      "america s cyber defense agency is burning down and nobody s coming to put it out": 1,
      "u s health officials defend rejection of moderna s flu vaccine": 1,
      "show hn double blind entropy using drand for verifiably fair randomness": 1,
      "paragon accidentally uploaded a photo of its spyware control panel": 1,
      "from specification to stress test a weekend with claude": 1,
      "us labels spacex a common carrier by air will regulate firm under railway law": 1,
      "uk supreme court issues milestone judgment for ai and software patentability": 1,
      "show hn send claude code tasks to the batch api at 50 off": 1,
      "65 lines of markdown a claude code sensation": 1,
      "deleted doesn t mean gone how police recovered nancy guthrie s doorbell footage": 1,
      "upenn data leaked after university refused to pay 1m ransom": 1,
      "d mistral ai research engineer phone screen interview": 1,
      "r iclr guess which peer review is human or ai": 1,
      "d is a kdd publication considered prestigious for more theoretical results": 1,
      "tambo ai tambo": 1,
      "danielmiessler personal ai infrastructure": 1,
      "google langextract": 1,
      "chromedevtools chrome devtools mcp": 1,
      "iofficeai aionui": 1,
      "shubhamsaboo awesome llm apps": 1,
      "rowboatlabs rowboat": 1,
      "github gh aw": 1,
      "unslothai unsloth": 1,
      "jeffallan claude skills": 1,
      "handsonllm hands on large language models": 1,
      "gemini 3 deep think advancing science research and engineering": 2,
      "leading inference providers cut ai costs by up to 10x with open source models on nvidia blackwell": 1,
      "nvidia dgx spark powers big projects in higher education": 1,
      "geforce now turns screens into a gaming machine": 1,
      "the surprising case for ai judges": 1,
      "bytedance s next gen ai model can generate clips based on text images audio and video": 1,
      "this 7 999 robot will fold some of your laundry": 1,
      "two more xai co founders are among those leaving after the spacex merger": 1,
      "anthropic says it 8217 ll try to keep its data centers from raising electricity costs": 1,
      "apple keeps hitting bumps with its overhauled siri": 1,
      "xai lays out interplanetary ambitions in public all hands": 1,
      "ai inference startup modal labs in talks to raise at 2 5b valuation sources say": 1,
      "openai disbands mission alignment team": 1,
      "apple s siri revamp reportedly delayed again": 1,
      "uber eats launches ai assistant to help with grocery cart creation": 1,
      "glean s fight to own the ai layer inside every company": 1,
      "who will own your company s ai layer glean s ceo explains": 1,
      "elon musk suggests spate of xai exits have been push not pull": 1,
      "why the economics of orbital ai are so brutal": 1,
      "threads new dear algo ai feature lets you personalize your feed": 1,
      "how ai changes the math for startups according to a microsoft vp": 1,
      "openenv in practice evaluating tool using agents in real world environments": 1,
      "ai is already making online crimes easier it could get much worse": 1,
      "what s next for chinese open source ai": 1,
      "is a secure ai assistant possible": 1,
      "nvidia nemotron 3 nano 30b moe model is now available in amazon sagemaker jumpstart": 1,
      "opengvlab added penghaoyin to opengvlab ummevalkit": 1
    },
    "total_papers_deduped": 290,
    "total_news_deduped": 72,
    "news_recent_filtered": 72
  }
}